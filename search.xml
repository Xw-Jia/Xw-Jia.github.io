<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>5.1_Neural_Networks_Learning</title>
      <link href="/2020/02/05/5-1-neural-networks-learning/"/>
      <url>/2020/02/05/5-1-neural-networks-learning/</url>
      
        <content type="html"><![CDATA[<p><img src="https://upload-images.jianshu.io/upload_images/1194012-03b47f040418215a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="5-1-Neural-Networks-Learning"><a href="#5-1-Neural-Networks-Learning" class="headerlink" title="5.1_Neural_Networks_Learning"></a>5.1_Neural_Networks_Learning</h1><h2 id="一-Cost-Function-and-Backpropagation"><a href="#一-Cost-Function-and-Backpropagation" class="headerlink" title="一. Cost Function and Backpropagation"></a>一. Cost Function and Backpropagation</h2><h3 id="1-Cost-Function"><a href="#1-Cost-Function" class="headerlink" title="1. Cost Function"></a>1. Cost Function</h3><p><img src="https://img.halfrost.com/Blog/ArticleImage/72_3.png" alt=""></p><p>假设训练集中有 m 个训练样本，$\begin{Bmatrix} (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \cdots ,(x^{(m)},y^{(m)}) \end{Bmatrix}$，L 表示神经网络的总层数 Layer，用 $S_{l}$ 表示第 L 层的单元数(神经元的数量)，但是不包括第 L 层的偏差单元(常数项)。令 K 为输出层的单元数目，即 最后一层的单元数。</p><p><strong>符号约定</strong>：</p><p>$z_i^{(j)}$ =  第 $j$ 层的第 $i$ 个节点（神经元）的“计算值”<br>$a_i^{(j)}$ = 第 $j$ 层的第 $i$ 个节点（神经元）的“激活值”;</p> $\Theta^{(l)}_{i,j}$  = 映射第 $l$ 层到第 $l+1$ 层的权值矩阵的第 $i$ 行第 $j$ 列的分量<p>$L$ = 神经网络总层数（包括输入层、隐层和输出层）<br>$s_l$ = 第 $l$ 层节点（神经元）个数，不包括偏移量节点。<br>$K$ = 输出节点个数<br>$h_{\theta}(x)_k$ = 第 $k$ 个预测输出结果<br>$x^{(i)}$ = 第 $i$ 个样本特征向量<br>$x^{(i)}_k$ = 第 $i$ 个样本的第 $k$ 个特征值<br>$y^{(i)}$ = 第 $i$ 个样本实际结果向量<br>$y^{(i)}_k$ = 第 $i$ 个样本结果向量的第 $k$ 个分量   </p><p>之前讨论的逻辑回归中代价函数如下：</p>$$\begin{align*}\rm{CostFunction} = \rm{F}({\theta}) &amp;= -\frac{1}{m}\left [ \sum_{i=1}^{m} y^{(i)}logh_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)})) \right ] +\frac{\lambda}{2m} \sum_{j=1}^{n}\theta_{j}^{2}  \\\end{align*}$$<p>扩展到神经网络中：</p>$$\begin{align*}\rm{CostFunction} = \rm{F}({\Theta}) &amp;= -\frac{1}{m}\left [ \sum_{i=1}^{m} \sum_{k=1}^{K} y^{(i)}_{k} log(h_{\Theta}(x^{(i)}))_{k} + (1-y^{(i)}_{k})log(1-(h_{\Theta}(x^{(i)}))_{k}) \right ] +\frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{S_{l}}\sum_{j=1}^{S_{l} +1}(\Theta_{j,i}^{(l)})^{2}  \\h_{\Theta}(x) &amp;\in \mathbb{R}^{K} \;\;\;\;\;\;\;\;\; (h_{\Theta}(x))_{i} = i^{th} \;\;output \\\end{align*}$$<p>$h_{\Theta}(x)$ 是一个 K 维向量，$ i $ 表示选择输出神经网络输出向量中的第 i 个元素。</p><p>神经网络的代价函数相比逻辑回归的代价函数，前一项的求和过程中多了一个 $ \sum_{k=1}^{K} $ ,由于 K 代表了最后一层的单元数，所以这里就是累加了 k 个输出层的代价函数。</p><p>后一项是正则化项，神经网络的正则化项看起来特别复杂，其实就是对 $ (\Theta_{j,i}^{(l)})^{2} $ 项对所有的 i，j，l的值求和。正如在逻辑回归中的一样，这里要除去那些对应于偏差值的项，因为我们不对它们进行求和，即不对 $ (\Theta_{j,0}^{(l)})^{2} ;;;;(i=0) $ 项求和。</p><h3 id="2-Backpropagation-Algorithm-反向传播算法"><a href="#2-Backpropagation-Algorithm-反向传播算法" class="headerlink" title="2. Backpropagation Algorithm 反向传播算法"></a>2. Backpropagation Algorithm 反向传播算法</h3><p>令 $ \delta_{j}^{(l)} $ 表示第 $l$ 层第 $j$ 个结点的误差。</p><p>反向传播从最后一层开始往前推：</p>$$\begin{align*}\delta_{j}^{(L)} &amp;= a_{j}^{(L)} - y_{j} \\&amp;=(h_{\theta}(x))_{j} - y_{j} \\\end{align*}$$<p>往前计算几步：</p>$$\begin{align*}\delta^{(3)} &amp;= (\Theta^{(3)})^{T}\delta^{(4)} . * g^{'}(z^{(3)}) \\\delta^{(2)} &amp;= (\Theta^{(2)})^{T}\delta^{(3)} . * g^{'}(z^{(2)}) \\\end{align*}$$<p>逻辑函数（Sigmoid函数）求导：</p>$$\begin{align*}\sigma(x)'&amp;=\left(\frac{1}{1+e^{-x}}\right)'=\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\frac{-1'-(e^{-x})'}{(1+e^{-x})^2}=\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\frac{e^{-x}}{(1+e^{-x})^2} \newline &amp;=\left(\frac{1}{1+e^{-x}}\right)\left(\frac{e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{+1-1 + e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{1 + e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}}\right)\\&amp;=\sigma(x)(1 - \sigma(x))\\\end{align*}$$<p>可以算出 $g^{‘}(z^{(3)}) = a^{(3)} . * (1-a^{(3)})$ ， $g^{‘}(z^{(2)}) = a^{(2)} . * (1-a^{(2)})$。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/72_4.png" alt=""></p><p>于是可以给出反向传播的算法步骤：</p><p>首先有一个训练集 $\begin{Bmatrix} (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \cdots ,(x^{(m)},y^{(m)}) \end{Bmatrix}$，初始值对每一个 $(l,i,j)$ 都设置 $\Delta^{(l)}_{i,j} := 0$ ，即初始矩阵是全零矩阵。</p><p>针对 $1-m$ 训练集开始以下步骤的训练：</p><h3 id="1-前向传播"><a href="#1-前向传播" class="headerlink" title="(1) 前向传播"></a>(1) 前向传播</h3><p>设置 $ a^{(1)} := x^{(t)} $，并按照前向传播的方法，计算出每一层的激励 $a^{(l)}$ 。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/72_5.png" alt=""></p><h3 id="2-计算误差"><a href="#2-计算误差" class="headerlink" title="(2) 计算误差"></a>(2) 计算误差</h3><p>利用 $y^{(t)}$，计算 $\delta^{(L)} = a^{(L)} - y^{t}$</p><p>其中 $L$ 是我们的总层数，$a^{(L)}$ 是最后一层激活单元输出的向量。所以我们最后一层的“误差值”仅仅是我们在最后一层的实际结果和 y 中的正确输出的差异。为了获得最后一层之前的图层的增量值，我们可以使用下面步骤中的方程，让我们从右向左前进：</p><h3 id="3-反向传播"><a href="#3-反向传播" class="headerlink" title="(3) 反向传播"></a>(3) 反向传播</h3><p>通过 $\delta^{(l)} = ((\Theta^{(l)})^{(T)}\delta^{(l+1)}).* a^{(l)} .*(1-a^{(l)})$，计算 $\delta^{(L-1)},\delta^{(L-2)},\cdots,\delta^{(2)}$ 计算出每一层神经节点的误差。</p><h3 id="4-计算偏导数"><a href="#4-计算偏导数" class="headerlink" title="(4) 计算偏导数"></a>(4) 计算偏导数</h3><p>最后利用 $\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_{j}^{(l)}\delta_{i}^{(l+1)}$，或者矢量表示为 $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^{T}$。</p>$$\frac{\partial }{\partial \Theta_{i,j}^{(l)} }F(\Theta) = D_{i,j}^{(l)} := \left\{\begin{matrix}\frac{1}{m} \left( \Delta_{i,j}^{(l)} + \lambda\Theta_{i,j}^{(l)}  \right) \;\;\;\;\;\;\;\; j\neq 0\\ \frac{1}{m}\Delta_{i,j}^{(l)} \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; j = 0\end{matrix}\right.$$<h3 id="5-更新矩阵"><a href="#5-更新矩阵" class="headerlink" title="(5) 更新矩阵"></a>(5) 更新矩阵</h3><p>更新各层的权值矩阵 $\Theta^{(l)}$ ，其中 $\alpha$  为学习率：</p><p>$$\Theta^{(l)} = \Theta^{(l)} - \alpha D^{(l)}$$</p><hr><h2 id="二-推导"><a href="#二-推导" class="headerlink" title="二. 推导"></a>二. 推导</h2><h3 id="1-目标"><a href="#1-目标" class="headerlink" title="1. 目标"></a>1. 目标</h3><p>求 $\min_\Theta F(\Theta)$</p><h3 id="2-思路"><a href="#2-思路" class="headerlink" title="2. 思路"></a>2. 思路</h3><p>类似梯度下降法，给定一个初值后，计算出所有节点的计算值和激活值，然后根据代价函数的变化不断调整参数值（权值），最终不断逼近最优结果，使代价函数值最小。</p><h3 id="3-推导过程"><a href="#3-推导过程" class="headerlink" title="3. 推导过程"></a>3. 推导过程</h3><p>为了实现上述思路，我们必须首先计算代价函数的偏导数：</p><p>$$\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}F(\Theta)$$</p><p>这个偏导并不好求，为了方便推导，我们假设只有一个样本（$m=1$，可忽略代价函数中的外部求和），并舍弃正规化部分，然后分为两种情况来求。</p><h3 id="情况1-隐藏层-→-输出层"><a href="#情况1-隐藏层-→-输出层" class="headerlink" title="情况1 隐藏层 → 输出层"></a>情况1 隐藏层 → 输出层</h3><p>我们知道：</p>$$\begin{align*}h_\Theta(x) &amp;= a^{(j+1)} = g(z^{(j+1)}) \\z^{(j)} &amp;= \Theta^{(j-1)}a^{(j-1)} \\\end{align*}$$<p>另外，输出层即第$L$层。</p><p>所以：</p>$$\dfrac{\partial}{\partial \Theta_{i,j}^{(L)}}F(\Theta)= \dfrac{\partial F(\Theta)}{\partial h_{\Theta}(x)_i} \dfrac{\partial h_{\Theta}(x)_i}{\partial z_i^{(L)}} \dfrac{\partial z_i^{(L)}}{\partial  \Theta_{i,j}^{(L)}}= \dfrac{\partial F(\Theta)}{\partial a_i^{(L)}} \dfrac{\partial a_i^{(L)}}{\partial z_i^{(L)}} \dfrac{\partial z_i^{(L)}}{\partial \Theta_{i,j}^{(L)}}$$<p>其中：</p>$$\begin{align*}\dfrac{\partial F(\Theta)}{\partial a_i^{(L)}} &amp;= \dfrac{a_i^{(L)} - y_i}{(1 - a_i^{(L)})a_i^{(L)}} \\\dfrac{\partial a_i^{(L)}}{\partial z_i^{(L)}} &amp;= \dfrac{\partial g(z_i^{(L)})}{\partial z_i^{(L)}} = \dfrac{e^{z_i^{(L)}}}{(e^{z_i^{(L)}}+1)^2} = a_i^{(L)} (1 - a_i^{(L)}) \\\dfrac{\partial z_i^{(L)}}{\partial \Theta_{i,j}^{(L)}} &amp;= \dfrac{\partial ( \sum_{k=0}^{s_{(L-1)}}\; \Theta_{i,k}^{(L)} a_k^{(L-1)})}{\partial  \Theta_{i,j}^{(L)}} = a_j^{(L-1)} \\\end{align*}$$<p>综上：</p>$$\begin{split}\dfrac{\partial}{\partial \Theta_{i,j}^{(L)}}F(\Theta)=&amp; \dfrac{\partial F(\Theta)}{\partial a_i^{(L)}} \dfrac{\partial a_i^{(L)}}{\partial z_i^{(L)}} \dfrac{\partial z_i^{(L)}}{\partial \Theta_{i,j}^{(L)}} \newline  =&amp; \dfrac{a_i^{(L)} - y_i}{(1 - a_i^{(L)})a_i^{(L)}} a_i^{(L)} (1 - a_i^{(L)}) a_j^{(L-1)} \newline  =&amp; (a_i^{(L)} - y_i)a_j^{(L-1)}\end{split}$$<h3 id="情况2-隐藏层-输入层-→-隐藏层"><a href="#情况2-隐藏层-输入层-→-隐藏层" class="headerlink" title="情况2 隐藏层 / 输入层 → 隐藏层"></a>情况2 隐藏层 / 输入层 → 隐藏层</h3><p>因为 $a^{(1)}=x$，所以可以将输入层和隐藏层同样对待。</p>$$\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}F(\Theta)=\dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} \dfrac{\partial a_i^{(l)}}{\partial z_i^{(l)}} \dfrac{\partial z_i^{(l)}}{\partial \Theta_{i,j}^{(l)}}\ (l = 1, 2, ..., L-1)$$<p>其中后两部分偏导很容易根据前面所得类推出来：</p>$$\begin{align*}\dfrac{\partial a_i^{(l)}}{\partial z_i^{(l)}} &amp;= \dfrac{e^{z_i^{(l)}}}{(e^{z_i^{(l)}}+1)^2} = a_i^{(l)} (1 - a_i^{(l)}) \\\dfrac{\partial z_i^{(l)}}{\partial \Theta_{i,j}^{(l)}} &amp;= a_j^{(l-1)} \\\end{align*}$$<p>第一部分偏导是不好求解的，或者说是没法直接求解的，我们可以得到一个递推式：</p>$$\dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} = \sum_{k=1}^{s_{(l+1)}} \Bigg[\dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \dfrac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}}\Bigg]$$<blockquote><p>因为该层的激活值与下一层各节点都有关，链式法则求导时需一一求导，所以有上式中的求和。</p></blockquote><p>递推式中第一部分是递推项，后两部分同样易求：</p>$$\begin{align*}\dfrac{\partial a_k^{(l+1)}}{\partial z_{k}^{(l+1)}} &amp;= \dfrac{e^{z_{k}^{(l+1)}}}{(e^{z_{k}^{(l+1)}}+1)^2} = a_k^{(l+1)} (1 - a_k^{(l+1)}) \\\dfrac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}} &amp;= \dfrac{\partial ( \sum_{j=0}^{s_l} \Theta_{k,j}^{(l+1)} a_j^{(l)})}{\partial a_i^{(l)}} = \Theta_{k,i}^{(l+1)} \\\end{align*}$$<p>所以，递推式为：</p>$$\begin{split}\dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} =&amp; \sum_{k=1}^{s_{(l+1)}} \Bigg[\dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \dfrac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}}\Bigg] \newline  =&amp; \sum_{k=1}^{s_{(l+1)}} \Bigg[ \dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \Theta_{k,i}^{(l+1)} \Bigg] \newline  =&amp; \sum_{k=1}^{s_{(l+1)}} \Bigg[ \dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} a_k^{(l+1)} (1 - a_k^{(l+1)}) \Theta_{k,i}^{(l+1)} \Bigg]\end{split}$$<p>为了简化表达式，定义第 $l$ 层第 $i$ 个节点的误差：</p>$$\begin{split}\delta^{(l)}_i =&amp; \dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} \dfrac{\partial a_i^{(l)}}{\partial z_i^{(l)}} \newline  =&amp; \dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} a_i^{(l)} (1 - a_i^{(l)})  \newline  =&amp; \sum_{k=1}^{s_{(l+1)}} \Bigg[ \dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \Theta_{k,i}^{(l+1)} \Bigg] a_i^{(l)} (1 - a_i^{(l)}) \newline  =&amp; \sum_{k=1}^{s_{(l+1)}} \Big[\delta^{(l+1)}_k \Theta_{k,i}^{(l+1)} \Big] a_i^{(l)} (1 - a_i^{(l)})\end{split}$$<p>可知，<strong>情况1</strong>的误差为：</p>$$\begin{split}\delta^{(L)}_i =&amp; \dfrac{\partial F(\Theta)}{\partial a_i^{(L)}} \dfrac{\partial a_i^{(L)}}{\partial z_i^{(L)}} \newline  =&amp; \dfrac{a_i^{(L)} - y_i}{(1 - a_i^{(L)})a_i^{(L)}} a_i^{(L)} (1 - a_i^{(L)}) \newline  =&amp; a_i^{(L)} - y_i\end{split}$$<p>最终的代价函数的偏导为：</p>$$\begin{split}\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}F(\Theta) =&amp; \dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} \dfrac{\partial a_i^{(l)}}{\partial z_i^{(l)}} \dfrac{\partial z_i^{(l)}}{\partial \Theta_{i,j}^{(l)}} \newline  =&amp; \delta^{(l)}_i \dfrac{\partial z_i^{(l)}}{\partial \Theta_{i,j}^{(l)}} \newline  =&amp; \delta^{(l)}_i a_j^{(l-1)} \end{split}$$<p>我们发现，引入误差 $\delta^{(l)}_i$ 后，这个公式可以通用于<strong>情况1</strong>和<strong>情况2</strong>。</p><p>可以看出，当前层的代价函数偏导，需要依赖于后一层的计算结果。这也是为什么这个算法的名称叫做“反向传播算法”。</p><h3 id="4-总结算法公式"><a href="#4-总结算法公式" class="headerlink" title="4. 总结算法公式"></a>4. 总结算法公式</h3><ul><li>输出层误差</li></ul><p>$$\delta^{(L)}_i = a_i^{(L)} - y_i$$</p><ul><li><p>隐藏层误差（反向传播计算）</p>$$\delta^{(l)}_i = \sum_{k=1}^{s_{(l+1)}} \Big[\delta^{(l+1)}_k \Theta_{k,i}^{(l+1)} \Big] a_i^{(l)} (1 - a_i^{(l)})$$</li><li><p>代价函数偏导计算（通用）</p></li></ul><p>$$\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}F(\Theta) = \delta^{(l)}_i a_j^{(l-1)}$$</p><hr><h2 id="三-Backpropagation-Algorithm-反向传播算法过程"><a href="#三-Backpropagation-Algorithm-反向传播算法过程" class="headerlink" title="三. Backpropagation Algorithm 反向传播算法过程"></a>三. Backpropagation Algorithm 反向传播算法过程</h2><p><img src="https://img.halfrost.com/Blog/ArticleImage/72_2_.png" alt=""></p><p>有了上述推导，我们描述一下算法具体的操作流程：</p><ul><li>输入：输入样本数据，初始化权值参数（建议随机生成较小的数）。</li><li>前馈：计算各层（$l=2, 3, …, L$）各节点的计算值（$z^{(l)}=\Theta^{(l-1)}a^{(l-1)}$）和激活值（$a^{(l)}=g(z^{(l)})$）。</li><li>输出层误差：计算输出层误差<script type="math/tex">\delta^{(L)}</script>（公式见前文）。</li><li>反向传播误差：计算各层（$l=L-1, L-2, …, 2$）的误差 $\delta^{(l)}$（公式见前文）。</li><li>输出：得到代价函数的梯度 $\nabla F(\Theta)$（参考前文偏导计算公式）。</li></ul><p>反向传播算法帮助我们得到了代价函数的梯度，我们就可以借助梯度下降法训练神经网络了。</p><p>$$\Theta := \Theta - \alpha  \nabla F(\Theta)$$</p><p>$\alpha $ 为学习速率。</p><hr><h2 id="四-Backpropagation-Algorithm-implementation-算法实现"><a href="#四-Backpropagation-Algorithm-implementation-算法实现" class="headerlink" title="四. Backpropagation Algorithm implementation 算法实现"></a>四. Backpropagation Algorithm implementation 算法实现</h2><p>以3层神经网络（输入层、隐层、输出层各一）为例。</p><ul><li>X 为大小为样本数∗特征数的样本特征矩阵</li><li>Y 为大小为样本数∗输出节点数的样本类别（结果）矩阵</li><li>Theta1 为输入层→隐层的权值矩阵</li><li>Theta2 为隐藏层→输出层的权值矩阵</li><li>m 为样本数</li><li>K 为输出层节点数</li><li>H 为隐藏层节点数</li><li>sigmoid 函数即逻辑函数（S型函数，Sigmoid函数）</li><li>sigmoidGradient 函数即 Sigmoid 函数的导函数</li><li>代码实现中，考虑了正规化，避免出现过拟合问题。</li></ul><h3 id="1-前馈阶段"><a href="#1-前馈阶段" class="headerlink" title="1. 前馈阶段"></a>1. 前馈阶段</h3><p>逐层计算各节点值和激活值。</p><pre class=" language-c"><code class="language-c">a1 <span class="token operator">=</span> X<span class="token punctuation">;</span>z2 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token function">ones</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> a1<span class="token punctuation">]</span> <span class="token operator">*</span> Theta1'<span class="token punctuation">;</span>a2 <span class="token operator">=</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z2<span class="token punctuation">)</span><span class="token punctuation">;</span>z3 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token function">ones</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> a2<span class="token punctuation">]</span> <span class="token operator">*</span> Theta2'<span class="token punctuation">;</span>a3 <span class="token operator">=</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z3<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h3 id="2-代价函数"><a href="#2-代价函数" class="headerlink" title="2. 代价函数"></a>2. 代价函数</h3><p>正规化部分需注意代价函数不惩罚偏移参数，即 $\Theta_{i,0}$（代码表示为 $Theta(:,1)$）。</p><pre class=" language-c"><code class="language-c">F <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> m <span class="token operator">*</span> <span class="token function">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token function">log</span><span class="token punctuation">(</span>a3<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">*</span> Y <span class="token operator">-</span> <span class="token function">log</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token punctuation">.</span><span class="token operator">-</span> a3<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> Y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> # 代价部分 lambda <span class="token operator">/</span> <span class="token number">2</span> <span class="token operator">/</span> m <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token function">Theta1</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span>end<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">^</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token function">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token function">Theta2</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span>end<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">^</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  # 正规化部分，lambda为正规参数，需除去偏移参数Theta<span class="token operator">*</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><h3 id="3-反向传播-1"><a href="#3-反向传播-1" class="headerlink" title="3. 反向传播"></a>3. 反向传播</h3><p>输出层误差和 $\Theta^{(2)}$ 梯度计算，反向传播计算隐层误差和 $\Theta^{(1)}$ 梯度。</p><p>仍需注意正规化时排除偏移参数，另外注意为激活值补一个偏移量 $1$。</p><pre class=" language-c"><code class="language-c">function g <span class="token operator">=</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span>    g <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token punctuation">.</span><span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">+</span> <span class="token function">exp</span><span class="token punctuation">(</span><span class="token operator">-</span>z<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>endfunction g <span class="token operator">=</span> <span class="token function">sigmoidGradient</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span>    g <span class="token operator">=</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>enddelta3 <span class="token operator">=</span> a3 <span class="token operator">-</span> Y<span class="token punctuation">;</span>Theta2_grad <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> m <span class="token operator">*</span> delta3' <span class="token operator">*</span> <span class="token punctuation">[</span><span class="token function">ones</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> a2<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  lambda <span class="token operator">/</span> m <span class="token operator">*</span> <span class="token punctuation">[</span><span class="token function">zeros</span><span class="token punctuation">(</span>K<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">Theta2</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span>end<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">;</span> # 正规化部分delta2 <span class="token operator">=</span> <span class="token punctuation">(</span>delta3 <span class="token operator">*</span> Theta2 <span class="token punctuation">.</span><span class="token operator">*</span> <span class="token function">sigmoidGradient</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token function">ones</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> z2<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>delta2 <span class="token operator">=</span> <span class="token function">delta2</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span>end<span class="token punctuation">)</span><span class="token punctuation">;</span> # 反向计算多一个偏移参数误差，除去Theta1_grad <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> m <span class="token operator">*</span>  delta2' <span class="token operator">*</span> <span class="token punctuation">[</span><span class="token function">ones</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> a1<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  lambda <span class="token operator">/</span> m <span class="token operator">*</span> <span class="token punctuation">[</span><span class="token function">zeros</span><span class="token punctuation">(</span>H<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">Theta1</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span>end<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">;</span> # 正规化部分</code></pre><hr><p>推荐阅读：</p><p>[Principles of training multi-layer neural network using backpropagation</p><p>](<a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="noopener">http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html</a>)</p><p><a href="https://www.zhihu.com/question/27239198" target="_blank" rel="noopener">如何直观地解释 back propagation 算法？</a></p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Learning.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Learning.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4.1_Neural_Networks_Representation</title>
      <link href="/2020/02/03/4-1-neural-networks-representation/"/>
      <url>/2020/02/03/4-1-neural-networks-representation/</url>
      
        <content type="html"><![CDATA[<p><img src="https://img.halfrost.com/Blog/ArticleImage/71_1.png" alt=""></p><h1 id="4-1-Neural-Networks-Representation"><a href="#4-1-Neural-Networks-Representation" class="headerlink" title="4.1_Neural_Networks_Representation"></a>4.1_Neural_Networks_Representation</h1><h2 id="一-Motivations"><a href="#一-Motivations" class="headerlink" title="一. Motivations"></a>一. Motivations</h2><p>假如我们用之前的逻辑回归解决以下分类问题：</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_1_1.png" alt=""></p><p>我们需要构造一个有很多项的非线性的逻辑回归函数。当只有两个特征量的时候，这还算比较简单的，但是假如我们有100个特征量呢？我们只考虑二阶项的话，其二阶项的个数大约是 $\frac{n^2}{2}$ 。假如我们要包含所有的二阶项的话这样看起来不是一个好办法，因为项数实在太多运算量也很多，而且最后结果往往容易造成过拟合。当然我们只是考虑了二阶项，考虑二阶项以上的就更多了。</p><p>当初始特征个数 n 增大时，这些高阶多项式项数将以几何级数上升，特征空间也会随之急剧膨胀 。所以当特征个数 n比较大的时候，用这个方法建立分类器并不是一个好的做法。</p><p>而对于大多数的机器学习问题， n  一般是比较大的。</p><p>对一个拥有很多特征的复杂数据集进行线性回归是代价很高的。比如我们对 50 * 50 像素的黑白图分类，我们就拥有了 2500 个特征。如果我们还要包含所有二次特征，复杂度为 $O(n^{2}/2)$，也就是说一共要有 $2500^{2}/2=3125000$ 个特征。这样计算的代价是高昂的。</p><p>人工神经网络是对具有很多特征的复杂问题进行机器学习的一种方法。</p><hr><h2 id="二-Neural-Networks"><a href="#二-Neural-Networks" class="headerlink" title="二. Neural Networks"></a>二. Neural Networks</h2><p>人工神经网络是对生物神经网络的一种简化的模拟。那么，我们先从生物中的神经元入手，进而了解神经网络的工作方式。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_6.svg" alt=""></p><p>用一个简单的模型来模拟神经元的工作，我们将神经元模拟成一个逻辑单元：</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_1_2.png" alt=""></p><p>$x_{1},x_{2},x_{3}$ 可以将其看成输入神经树突，黄色的圆圈则可以看成中心处理器细胞核， $h_\theta(x)$ 则可看成输出神经轴突。因为这里是逻辑单元，所以我们的输出函数为： $h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$ 。一般我们把这称为一个有 s 型函数（逻辑函数）作为激励的人工神经元。</p><p>那么神经网络其实就是这些神经元组合在一起的集合，如下图：</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_2.png" alt=""></p><p>左边第一层 Layer1 被称为<strong>输入层</strong>。在输入层我们输入我们的特征项 $x_{1},x_{2},x_{3}$ 。</p><p>右边最后一层被称为<strong>输出层</strong>。输出函数为： $h_\Theta(x)$ 。</p><p>中间这层被称为<strong>隐藏层</strong>。</p><p>我们现在要计算当前神经元的值，在当前神经元所在层的前一层，有很多个突触前神经元（当前神经元也是相对于他们的突触后神经元）。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_7.png" alt=""></p><p>对于前一层的每一个突触前神经元，都有一个输出值，作为当前神经元的输入值，经过轴突传递到当前神经元。当然，如果是第一层神经元，则直接从输入样本数据中接受刺激（对应图中的 $x_{i}$）。</p><p>轴突具有权值（对应图中的权值 weights 列：$w_{ij}$），对每一个输出值加权求和，得到该神经元的输入值。这个加权求和对应图中的transfer function（转移函数），但这个函数的名称并不明确，有人把它称作激活函数（activation function），不同的人可能有不同的叫法，这里仅供参考。</p><p>得到了该神经元的值，就要判定该神经元是否激活兴奋。这对应于图中的activation function（激活函数），但也有人将这个函数叫做输出函数（output function），而把前面说的那一部分叫做激活函数（activation function），并把这两部分合称为转移函数（transfer function）。</p><p>有几种函数可以作为激活函数：</p><ul><li>阶跃函数。这是最简单直接的形式，也是人工神经网络定义时一般采用的。</li><li>逻辑函数。就是S型函数（Sigmoid函数），具有可无限微分的优势。</li><li>斜坡函数</li><li>高斯函数</li><li>…</li></ul><p>可以注意到图中的threshold（阈值），$\theta_{j}$，即激活阈值。也就是说，仅当神经元的值大于这个阈值时，该神经元激活兴奋，输出1；否则无法激活，输出0。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_3.png" alt=""></p><p>其中隐藏层中的元素我们用 $a_i^{(j)}$ 表示。上标 j 表示的是第几层（有时候我们并不只有简单一层），下标 i 表示第几个，<strong>第j层的第i个节点（神经元）的“激活值”</strong>。</p><p>上面的神经网络可以简单的表示为：</p><p>$$\begin{bmatrix} x_{0}\ x_{1}\ x_{2}\ x_{3} \end{bmatrix} \rightarrow \begin{bmatrix} a_{1}^{(2)}\ a_{2}^{(2)}\ a_{3}^{(2)} \end{bmatrix} \rightarrow h_{\theta}(x) $$</p><p>左边输入层多增加了一个偏置单元(偏置神经元)，$x_{0}$</p><p>用 $\Theta^{(j)}$ 表示特征量前的参数，是一个有权重的矩阵控制着一层参数的大小，<strong>映射第j层到第j+1层的权值矩阵</strong>。</p><p>上述的神经网络可用数学表达，如下：</p>$$\begin{align*}a_{1}^{(2)} &amp;= g(\Theta_{10}^{(1)}x_{0}+\Theta_{11}^{(1)}x_{1}+\Theta_{12}^{(1)}x_{2}+\Theta_{13}^{(1)}x_{3}) \\a_{2}^{(2)} &amp;= g(\Theta_{20}^{(1)}x_{0}+\Theta_{21}^{(1)}x_{1}+\Theta_{22}^{(1)}x_{2}+\Theta_{23}^{(1)}x_{3}) \\a_{3}^{(2)} &amp;= g(\Theta_{30}^{(1)}x_{0}+\Theta_{31}^{(1)}x_{1}+\Theta_{32}^{(1)}x_{2}+\Theta_{33}^{(1)}x_{3}) \\h_{\Theta}(x) &amp;= a_{1}^{(3)} = g(\Theta_{10}^{(2)}a_{0}^{(2)}+\Theta_{11}^{(2)}a_{1}^{(2)}+\Theta_{12}^{(2)}a_{2}^{(2)}+\Theta_{13}^{(2)}a_{3}^{(2)}) \\\end{align*}$$<p>$\Theta$ 矩阵也被称作为模型的权重。这里的 $g(x)$ 都是 sigmoid 激活函数，即 $g(x) = \frac{1}{1+e^{-x}}$</p><p>对上面的神经网络数学表达方式进行向量化推导，令：</p>$$\begin{align*}z_{1}^{(2)} &amp;= \Theta_{10}^{(1)}x_{0}+\Theta_{11}^{(1)}x_{1}+\Theta_{12}^{(1)}x_{2}+\Theta_{13}^{(1)}x_{3} \\z_{2}^{(2)} &amp;= \Theta_{20}^{(1)}x_{0}+\Theta_{21}^{(1)}x_{1}+\Theta_{22}^{(1)}x_{2}+\Theta_{23}^{(1)}x_{3} \\z_{3}^{(2)} &amp;= \Theta_{30}^{(1)}x_{0}+\Theta_{31}^{(1)}x_{1}+\Theta_{32}^{(1)}x_{2}+\Theta_{33}^{(1)}x_{3} \\\vdots \\z_{k}^{(2)} &amp;= \Theta_{k,0}^{(1)}x_{0}+\Theta_{k,1}^{(1)}x_{1}+\Theta_{k,2}^{(1)}x_{2}+\Theta_{k,3}^{(1)}x_{3} \\\end{align*}$$<p>于是可以得到：</p>$$\begin{align*}a_{1}^{(2)} &amp;= g(z_{1}^{(2)}) \\a_{2}^{(2)} &amp;= g(z_{2}^{(2)}) \\a_{3}^{(2)} &amp;= g(z_{3}^{(2)}) \\\end{align*}$$<p>用向量即可表示为：</p>$$x = \begin{bmatrix}x_{0}\\ x_{1}\\ x_{2}\\ x_{3}\end{bmatrix},z^{(2)} = \begin{bmatrix}z_{1}^{(2)}\\ z_{2}^{(2)}\\ z_{3}^{(2)}\\ \end{bmatrix} = \Theta^{(1)}x$${% raw %}统一一下前后两层的输入输出关系，将 $x=a^{(1)}$，即可得到：{% raw %}$$\begin{align*}x &amp;= \begin{bmatrix}x_{0}\\ x_{1}\\ \vdots \\ x_{n}\end{bmatrix},z^{(j)} = \begin{bmatrix}z_{1}^{(j)}\\ z_{2}^{(j)}\\\vdots \\ z_{3}^{(j)}\\ \end{bmatrix}, \\ \Rightarrow  z^{(j)} &amp;=\Theta^{(j-1)}a^{(j-1)}\\\end{align*}$${% endraw %}<p>这里也可以得到一个结论：</p><p>假如一个网络里面在第 j  层有 $s_j$ 个单元，在第 j+1 层有 $s_{j+1}$ 个单元，那么 $\Theta^{(j)}$ 则控制着第 j 层到第 j+1 层的映射矩阵，矩阵的维度是： $s_{j+1} * (s_j + 1)$ 。(例如： j=1 , $s_j=1$， $s_{j+1}$=1 ，也就是说第一层只有一个单元，第二层也只有一个单元，那么 $\Theta^{(1)}$ 矩阵维度就是 1 * 2 ,因为要算上偏置单元)</p><p>因为我们通常有 $a_0^{(j)}=1$ ，所以：</p>{% raw %}$$\begin{align*}a^{(j)}&amp;=g(z^{(j)})\\z^{(j+1)}&amp;=\Theta^{(j)}a^{(j)}\\h_\Theta(x)&amp;=a^{(j+1)}=g(z^{(j+1)})\\\end{align*}$${% endraw %}<p>由这个关系其实可以看出，神经网络跟之前所学的逻辑回归根本区别在于，它是将上一层的输出当做下一层的输入，这个从输入层到隐藏层再到输出层一次计算激励的过程叫做 <strong>forward propagation（前向传播）</strong>。</p><hr><h2 id="三-Applications"><a href="#三-Applications" class="headerlink" title="三. Applications"></a>三. Applications</h2><h3 id="1-逻辑运算"><a href="#1-逻辑运算" class="headerlink" title="1. 逻辑运算"></a>1. 逻辑运算</h3><p>利用神经网络进行 逻辑与运算</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_4.png" alt=""></p><p>利用神经网络进行 逻辑非运算</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_1_3.png" alt=""></p><p>但是单一一层无法完成异或运算。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_1_4.png" alt=""></p><p>异或在几何上的问题其实是将红叉和蓝圈分开，但是我们的输出函数是： $h_\Theta(x)=g(\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2)$ ,这是线性的，那么在图上无论怎么画一条直线，也没有办法将两个不同的训练集分开。既然一条直线不行，那么神经网络增加一层。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_5.png" alt=""></p><p>如上图，将第二层第一个元素 $a_1^{(2)}$ 作为与运算的结果，第二个元素 $a_2^{(2)}$ 作为或非运算的结果， $a_1^{(2)}$ 和 $a_2^{(2)}$ 再作为输入，进行或运算，作为第三层输出的结果，最后得到的结果与输入的关系正是异或运算的关系。</p><h3 id="2-本质"><a href="#2-本质" class="headerlink" title="2. 本质"></a>2. 本质</h3><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_1_5.png" alt=""></p><p>神经网络正是这样解决比较复杂的函数，当层数很多的时候，我们有一个相对简单的输入量，通过加以权重和不同的运算送到第二层，而第三层在第二层作为输入的基础上再来进行一些更复杂的运算，一层一层下去解决问题。</p><hr><h2 id="四-Neural-Networks-Representation-测试"><a href="#四-Neural-Networks-Representation-测试" class="headerlink" title="四. Neural Networks: Representation 测试"></a>四. Neural Networks: Representation 测试</h2><h3 id="1-Question-1"><a href="#1-Question-1" class="headerlink" title="1. Question 1"></a>1. Question 1</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. Suppose you have a multi-class classification problem with three classes, trained with a 3 layer network. Let a(3)1=(hΘ(x))1 be the activation of the first output unit, and similarly a(3)2=(hΘ(x))2 and a(3)3=(hΘ(x))3. Then for any input x, it must be the case that a(3)1+a(3)2+a(3)3=1.</p><p>B. The activation values of the hidden units in a neural network, with the sigmoid activation function applied at every layer, are always in the range (0, 1).</p><p>C. A two layer (one input layer, one output layer; no hidden layer) neural network can represent the XOR function.</p><p>D. Any logical function over binary-valued (0 or 1) inputs x1 and x2 can be (approximately) represented using some neural network.</p><p>解答： B、D</p><p>B.S型函数作为判断函数运用到每一层，其范围是[0,1]，正确。<br>D.任何二进制输入的逻辑运算都可以神经网络解决，正确。<br>C.异或不可以用一层神经网络解决。<br>A.不一定，决策函数不是S型函数的话最后结果相加就不是1了。   </p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Representation.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Representation.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.2_Regularization</title>
      <link href="/2020/02/03/3-2-regularization/"/>
      <url>/2020/02/03/3-2-regularization/</url>
      
        <content type="html"><![CDATA[<p><img src="https://upload-images.jianshu.io/upload_images/1194012-756b3fa02ecf03db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="3-2-Regularization"><a href="#3-2-Regularization" class="headerlink" title="3.2_Regularization"></a>3.2_Regularization</h1><h2 id="一-Solving-the-Problem-of-Overfitting"><a href="#一-Solving-the-Problem-of-Overfitting" class="headerlink" title="一. Solving the Problem of Overfitting"></a>一. Solving the Problem of Overfitting</h2><p>考虑从 $x \in \mathbb{R}$ 预测 y 的问题。下面最左边的图显示了将 $y =\theta_{0}+\theta_{1}x$ 拟合到数据集的结果。我们看到这些数据并不是直线的，所以这个数据并不是很好。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/70_2.png" alt=""></p><p>相反，如果我们添加了一个额外的特征 x2，并且拟合 $y =\theta_{0}+\theta_{1}x+\theta_{2}x^{2}$，那么我们获得的数据稍微更适合,如上图。</p><p>但是并不是添加的多项式越多越好。但是，添加太多特征也是一个危险：最右边的数字是拟合五阶多项式 $y =\theta_{0}+\theta_{1}x+\theta_{2}x^{2}+\theta_{3}x^{3}+\theta_{4}x^{4}+\theta_{5}x^{5} $ 的结果。我们看到即使拟合曲线完美地传递了数据，我们也不会认为这是一个很好的预测，上图最右边的图就是过度拟合的例子。</p><p>上图最右边的图也称有<strong>高方差</strong>。如果我们拟合一个高阶多项式，有过度的特征，并且这个假设函数能拟合几乎所有的数据，这就面临可能的函数太过于庞大，变量太多的问题。我们没有足够的数据去约束它，来获得一个好的假设函数，这就是过度拟合。</p><p>欠拟合或高偏倚是当我们的假设函数h的形式很难与数据的趋势作图时。它通常是由一个功能太简单或功能太少造成的。另一方面，过度拟合或高度方差是由适合现有数据的假设函数引起的，但不能很好地预测新数据。它通常是由一个复杂的函数造成的，它会产生大量与数据无关的不必要的曲线和角度。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/70_3.png" alt=""></p><p>这个术语适用于线性和逻辑回归。解决过度配合问题有两个主要选项：</p><h3 id="1-减少特征的数量："><a href="#1-减少特征的数量：" class="headerlink" title="1. 减少特征的数量："></a>1. 减少特征的数量：</h3><ul><li>手动选择要保留的特征，哪些变量更为重要，哪些变量应该保留，哪些应该舍弃。 </li><li>使用模型选择算法（稍后在课程中学习），算法会自动选择哪些特征变量保留，哪些舍弃。</li></ul><p>缺点是舍弃了一些特征以后，也就舍弃了一些问题的关键信息。</p><h3 id="2-正则化"><a href="#2-正则化" class="headerlink" title="2. 正则化"></a>2. 正则化</h3><ul><li>保留所有的特征，但减少参数 $\theta_{j}$ 的大小或者减少量级。 </li><li>当有很多个特征的时候，并且每个特征都会对最终预测值产生影响，正则化可以保证运作良好。</li></ul><p>正则化目的是尽量去简化这个假设模型。因为这些参数都接近0的时候，越简单的模型也被证明越不容易出现过拟合的问题。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/70_4.png" alt=""></p><p>减少一些数量级的特征，加一些“惩罚”项(为了使代价函数最小，乘以 1000 就是惩罚)。</p><p>代价函数：</p>$$ \rm{CostFunction} = \rm{F}({\theta}) = \frac{1}{2m} \left [ \sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2 + \lambda \sum_{i = 1}^{m} \theta_{j}^{2} \right ]$$<p>$\lambda \sum_{i = 1}^{m} \theta_{j}^{2}$ 是正则化项，它缩小每个参数的值。 $\lambda$ 是正则化参数，$\lambda$ 控制两个不同目标之间的取舍，即更好的去拟合训练集的目标 和 将参数控制的更小的目标，从而保持假设模型的相对简单，避免出现过拟合的情况。</p><p>但是如果选择的 $\lambda $ 太大，可能会过多地消除特征，导致 $\theta$ 都约等于 0 了，最终预测函数变成了水平直线了。这就变成了欠拟合的例子了(偏见性太强，偏差过高)。</p><hr><h2 id="二-Regularized-Linear-Regression-线性回归正则化"><a href="#二-Regularized-Linear-Regression-线性回归正则化" class="headerlink" title="二. Regularized Linear Regression 线性回归正则化"></a>二. Regularized Linear Regression 线性回归正则化</h2><h3 id="1-Gradient-Descent-线性回归梯度下降正则化"><a href="#1-Gradient-Descent-线性回归梯度下降正则化" class="headerlink" title="1. Gradient Descent 线性回归梯度下降正则化"></a>1. Gradient Descent 线性回归梯度下降正则化</h3><p>$$\theta_{0} := \theta_{0} - \alpha \frac{1}{m} \sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}$$</p>$$\theta_{j} := \theta_{j} - \alpha \left [ \left ( \frac{1}{m} \sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}\right ) + \frac{\lambda}{m}\theta_{j} \right ]  \;\;\;\;\;\;\;\;j \in \begin{Bmatrix} 1,2,3,4, \cdots n\end{Bmatrix}$$<p>将上面的式子化简得：</p>$$\theta_{j} := \theta_{j}(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}   \;\;\;\;\;\;\;\;j \in \begin{Bmatrix} 1,2,3,4, \cdots n\end{Bmatrix}$$<p>在上面的式子中 $(1-\alpha \frac{\lambda}{m}) &lt; 1$ 恒小于 1，约等于 1(0.999) 。于是梯度下降的过程就是每次更新都把参数乘以 0.999，缩小一点点，然后再向最小点的方向移动一下。</p><h3 id="2-Normal-Equation-线性回归正规方程正则化"><a href="#2-Normal-Equation-线性回归正规方程正则化" class="headerlink" title="2. Normal Equation 线性回归正规方程正则化"></a>2. Normal Equation 线性回归正规方程正则化</h3><p>之前推导过的正规方程结论：</p><p>$$\Theta = (X^{T}X)^{-1}X^{T}Y$$</p><p>正则化以后，上述式子变成了：</p>$$\Theta = \left( X^{T}X +\lambda \begin{bmatrix}0 &amp;  &amp;  &amp;  &amp; \\  &amp; 1 &amp;  &amp;  &amp; \\  &amp;  &amp; 1 &amp;  &amp; \\  &amp;  &amp;  &amp; \ddots  &amp; \\  &amp;  &amp;  &amp;  &amp; 1\end{bmatrix} \right) ^{-1}X^{T}Y$$<p>在之前的讨论中，有一个<strong>前提条件是 $X^{T}X$ 是非奇异(非退化)矩阵， 即 $ \left | X^{T}X \right | \neq 0 $</strong></p><p>在上述正则化的式子里面，只要 $\lambda &gt; 0$，就不存在不可逆的问题了。因为 $\left( X^{T}X +\lambda \begin{bmatrix}0 &amp;  &amp;  &amp;  &amp; \\  &amp; 1 &amp;  &amp;  &amp; \\  &amp;  &amp; 1 &amp;  &amp; \\  &amp;  &amp;  &amp; \ddots  &amp; \\  &amp;  &amp;  &amp;  &amp; 1\end{bmatrix} \right)$ 这一项一定是可逆的，因为它一定不是奇异矩阵。所以<strong>正则化还能解决不可逆的情况</strong>。</p><hr><h2 id="三-Regularized-Logistic-Regression-逻辑回归正则化"><a href="#三-Regularized-Logistic-Regression-逻辑回归正则化" class="headerlink" title="三. Regularized Logistic Regression 逻辑回归正则化"></a>三. Regularized Logistic Regression 逻辑回归正则化</h2><p><img src="https://img.halfrost.com/Blog/ArticleImage/70_5.png" alt=""></p><p>之前讨论过的代价函数是：</p>$$\begin{align*}\rm{CostFunction} = \rm{F}({\theta}) &amp;= -\frac{1}{m}\left [ \sum_{i=1}^{m} y^{(i)}logh_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)})) \right ] \\\left( h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}} \right ) \end{align*}$$<p>正则化以后：</p>$$\begin{align*}\rm{CostFunction} = \rm{F}({\theta}) &amp;= -\frac{1}{m}\left [ \sum_{i=1}^{m} y^{(i)}logh_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)})) \right ] +\frac{\lambda}{2m} \sum_{j=1}^{n}\theta_{j}^{2}  \\\end{align*}$$<h3 id="1-Gradient-Descent-逻辑回归梯度下降正则化"><a href="#1-Gradient-Descent-逻辑回归梯度下降正则化" class="headerlink" title="1. Gradient Descent 逻辑回归梯度下降正则化"></a>1. Gradient Descent 逻辑回归梯度下降正则化</h3><p>式子等同于线性回归正则化</p>$$\begin{align*}\theta_{0} &amp;:= \theta_{0} - \alpha \frac{1}{m} \sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)} \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;j = 1 \\\theta_{j} &amp;:= \theta_{j}(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}   \;\;\;\;\;\;\;\;j \in \begin{Bmatrix} 1,2,3,4, \cdots n\end{Bmatrix} \\\end{align*}$$<p>虽然式子和线性回归的一模一样，不过这里的 $h_{\theta}(x)$ 代表的意义不同，逻辑回归中：</p><p>$$h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}}$$</p><hr><h2 id="四-Regularization-测试"><a href="#四-Regularization-测试" class="headerlink" title="四. Regularization 测试"></a>四. Regularization 测试</h2><h3 id="1-Question-1"><a href="#1-Question-1" class="headerlink" title="1. Question 1"></a>1. Question 1</h3><p>You are training a classification model with logistic regression. Which of the following statements are true? Check all that apply.</p><p>A. Introducing regularization to the model always results in equal or better performance on the training set.</p><p>B. Introducing regularization to the model always results in equal or better performance on examples not in the training set.</p><p>C. Adding many new features to the model makes it more likely to overfit the training set.</p><p>D. Adding a new feature to the model always results in equal or better performance on examples not in the training set.</p><p>解答： D  </p><p>A、B 正则化的引入是解决过拟合的问题，而过拟合正是过度拟合数据但无法泛化到新的数据样本中。<br>D 增加一些特征量可能导致拟合在训练集原本没有被拟合到的数据，正确，这就是过拟合。</p><h3 id="2-Question-2"><a href="#2-Question-2" class="headerlink" title="2. Question 2"></a>2. Question 2</h3><p>Suppose you ran logistic regression twice, once with λ=0, and once with λ=1. One of the times, you got</p><p>parameters $\theta = \begin{bmatrix}26.29\\ 65.41\end{bmatrix}$, and the other time you got $\theta = \begin{bmatrix}2.75\\ 1.32\end{bmatrix}$. However, you forgot which value of λ corresponds to which value of θ. Which one do you think corresponds to λ=1?</p><p>A. $\theta = \begin{bmatrix}26.29\\ 65.41\end{bmatrix}$   </p><p>B. $\theta = \begin{bmatrix}2.75\\ 1.32\end{bmatrix}$</p><p>解答： B</p><p>$\lambda = 1$表示正则化以后。正则化其实让我们的 $\theta_j$变小，所以选B。</p><h3 id="3-Question-3"><a href="#3-Question-3" class="headerlink" title="3. Question 3"></a>3. Question 3</h3><p>Which of the following statements about regularization are true? Check all that apply.</p><p>A. Using too large a value of λ can cause your hypothesis to overfit the data; this can be avoided by reducing λ.</p><p>B. Consider a classification problem. Adding regularization may cause your classifier to incorrectly classify some training examples (which it had correctly classified when not using regularization, i.e. when λ=0).</p><p>C. Because logistic regression outputs values 0≤hθ(x)≤1, its range of output values can only be “shrunk” slightly by regularization anyway, so regularization is generally not helpful for it.</p><p>D. Using a very large value of λ cannot hurt the performance of your hypothesis; the only reason we do not set λ to be too large is to avoid numerical problems.</p><p>解答： B</p><p>C 正则化对逻辑回归没用，错误。<br>A、D   $\lambda$过大会导致欠拟合。</p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Regularization.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Regularization.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.1_Logistic_Regression</title>
      <link href="/2020/02/03/3-1-logistic-regression/"/>
      <url>/2020/02/03/3-1-logistic-regression/</url>
      
        <content type="html"><![CDATA[<p><img src="https://img.halfrost.com/Blog/ArticleImage/69_6.png" alt=""></p><h1 id="3-1-Logistic-Regression"><a href="#3-1-Logistic-Regression" class="headerlink" title="3.1_Logistic_Regression"></a>3.1_Logistic_Regression</h1><h2 id="一-Classification-and-Representation"><a href="#一-Classification-and-Representation" class="headerlink" title="一. Classification and Representation"></a>一. Classification and Representation</h2><p>要尝试分类，一种方法是使用线性回归，并将所有大于0.5的预测值映射为1，将小于0.5的所有预测值映射为0.但是，此方法效果不佳，因为分类实际上不是线性函数。 分类问题就像回归问题一样，除了我们现在想要预测的值只有少数离散值。</p><p><strong>线性回归用来解决分类问题，通常不是一个好主意</strong>。</p><p>我们解决分类问题，忽略y是离散值，并使用我们的旧线性回归算法来尝试预测给定的x。但是，构建这种方法性能很差的示例很容易。直观地说，当知道$y\in \begin{Bmatrix}<br>0,1<br>\end{Bmatrix}$时，$h_{\theta}(x)$ 取大于1或小于0的值也是没有意义的。为了解决这个问题，让我们改变我们的假设 $h_{\theta}(x)$ 的形式以满足 $0\leqslant h_{\theta}(x)\leqslant 1$。这是通过将 $\theta^{T}x$ 插入 Logistic 函数来完成的：</p><p>$$g(x) = \frac{1}{1+e^{-x}}$$</p><p>上式称为 Sigmoid Function 或者 Logistic Function</p><p>令 $h_{\theta}(x) = g(\theta^{T}x)$,$z = \theta^{T}x$,则:</p><p>$$g(x) = \frac{1}{1+e^{-\theta^{T}x}}$$</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/69_8.png" alt=""></p><p>这里显示的函数$g(x)$将任何实数映射到（0,1）区间，使得它可用于将任意值函数转换为更适合分类的函数。</p><p><strong>决策边界不是训练集的属性，而是假设本身及其参数的属性</strong>。</p><hr><h2 id="二-Logistic-Regression-Model"><a href="#二-Logistic-Regression-Model" class="headerlink" title="二. Logistic Regression Model"></a>二. Logistic Regression Model</h2><h3 id="1-Cost-Function"><a href="#1-Cost-Function" class="headerlink" title="1. Cost Function"></a>1. Cost Function</h3><p>之前定义的代价函数：</p><p>$$ \rm{CostFunction} = \rm{F}({\theta}) = \frac{1}{m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2 $$</p><p>如果将 $$h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}} $$ 代入到上面的式子中，$\rm{CostFunction}$ 的函数图像会是一个非凸函数，会有很多个局部极值点。</p><p>于是我们重新寻找一个新的代价函数：</p><p>$$\rm{CostFunction} = \rm{F}({\theta}) = \frac{1}{m}\sum_{i = 1}^{m} \rm{Cost}(h_{\theta}(x^{(i)}),y^{(i)})$$</p>$$\rm{Cost}(h_{\theta}(x^{(i)}),y^{(i)}) = \left\{\begin{matrix}-log(h_{\theta}(x)) &amp;if \; y = 1 \\ -log(1-h_{\theta}(x)) &amp; if\; y = 0\end{matrix}\right.$$<p>需要说明的一点是，在我们的训练集中，甚至不在训练集中的样本，y 的值总是等于 0 或者 1 。</p><h3 id="2-Simplified-Cost-Function-and-Gradient-Descent"><a href="#2-Simplified-Cost-Function-and-Gradient-Descent" class="headerlink" title="2. Simplified Cost Function and Gradient Descent"></a>2. Simplified Cost Function and Gradient Descent</h3><p>于是进一步我们把代价函数写成一个式子：</p><p>$$\rm{Cost}(h_{\theta}(x),y) = - ylog(h_{\theta}(x)) - (1-y)log(1-h_{\theta}(x))$$</p><p>所以代价函数最终表示为：</p>$$\begin{align*}\rm{CostFunction} = \rm{F}({\theta}) &amp;= \frac{1}{m}\sum_{i = 1}^{m} \rm{Cost}(h_{\theta}(x^{(i)}),y^{(i)})\\&amp;= -\frac{1}{m}\left [ \sum_{i=1}^{m} y^{(i)}logh_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)})) \right ] \\\left( h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}} \right ) \end{align*}$$<p>向量化形式：</p>$$\begin{align*}h &amp;= g(X\theta)\\ \rm{CostFunction} = \rm{F}({\theta}) &amp;= \frac{1}{m} \left ( -\overrightarrow{y}^{T}log(h) - (1-\overrightarrow{y})^{T}log(1-h) \right ) \\ \end{align*}$$<p>为了把式子写成上面这样子是来自于统计学的极大似然估计法得来的，它是统计学里为不同的模型快速寻找参数的方法。它的性质之一是它是凸函数。</p><p>利用梯度下降的方法，得到代价函数的最小值：</p><p>$$ \theta_{j} := \theta_{j} - \alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_{j}$$</p><p>矢量化，即：</p><p>$$ \theta := \theta - \alpha \frac{1}{m} X^{T}(g(X\Theta)-\vec{y})$$</p><p><strong>这里需要注意的是</strong>，</p><p><strong>线性回归中，$h_{\theta}(x) = \theta^{T}x $</strong>,</p><p><strong>而 Logistic 回归中，$h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}}$</strong> 。</p><p>最后，特征缩放的方法同样适用于 Logistic 回归，让其梯度下降收敛更快。</p><hr><h3 id="3-求导过程"><a href="#3-求导过程" class="headerlink" title="3. 求导过程"></a>3. 求导过程</h3><p>逻辑函数</p><p>我们先来看看如何对逻辑函数（Sigmoid函数）求导：</p>$$\begin{align*}\sigma(x)'&amp;=\left(\frac{1}{1+e^{-x}}\right)'=\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\frac{-1'-(e^{-x})'}{(1+e^{-x})^2}=\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\frac{e^{-x}}{(1+e^{-x})^2} \newline &amp;=\left(\frac{1}{1+e^{-x}}\right)\left(\frac{e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{+1-1 + e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{1 + e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}}\right)\\&amp;=\sigma(x)(1 - \sigma(x))\\\end{align*}$$<p>代价函数</p><p>利用上面的结果，借助复合函数求导公式等，可得：</p>$$\begin{align*}\frac{\partial}{\partial \theta_j} J(\theta) &amp;= \frac{\partial}{\partial \theta_j} \frac{-1}{m}\sum_{i=1}^m \left [ y^{(i)} log (h_\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\theta(x^{(i)})) \right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} \frac{\partial}{\partial \theta_j} log (h_\theta(x^{(i)}))   + (1-y^{(i)}) \frac{\partial}{\partial \theta_j} log (1 - h_\theta(x^{(i)}))\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \frac{\partial}{\partial \theta_j} h_\theta(x^{(i)})}{h_\theta(x^{(i)})}   + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - h_\theta(x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \frac{\partial}{\partial \theta_j} \sigma(\theta^T x^{(i)})}{h_\theta(x^{(i)})}   + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - \sigma(\theta^T x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})}   + \frac{- (1-y^{(i)}) \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})}   - \frac{(1-y^{(i)}) h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} (1 - h_\theta(x^{(i)})) x^{(i)}_j - (1-y^{(i)}) h_\theta(x^{(i)}) x^{(i)}_j\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} (1 - h_\theta(x^{(i)})) - (1-y^{(i)}) h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} - y^{(i)} h_\theta(x^{(i)}) - h_\theta(x^{(i)}) + y^{(i)} h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [ y^{(i)} - h_\theta(x^{(i)}) \right ] x^{(i)}_j  \newline&amp;= \frac{1}{m}\sum_{i=1}^m \left [ h_\theta(x^{(i)}) - y^{(i)} \right ] x^{(i)}_j\end{align*}$$<p>向量化形式：</p><p>$$\nabla J(\theta) = \frac{1}{m} \cdot  X^T \cdot \left(g\left(X\cdot\theta\right) - \vec{y}\right)$$</p><hr><h3 id="4-Advanced-Optimization"><a href="#4-Advanced-Optimization" class="headerlink" title="4. Advanced Optimization"></a>4. Advanced Optimization</h3><p>除去梯度下降法，还有其他的优化方法，</p><p>conjugate gradient 共轭梯度法，<br>BFGS，<br>L_BFGS，  </p><p>上述3种算法在高等数值计算中。它们相比梯度下降，有以下一些优点：</p><ol><li>不需要手动选择学习率 $\alpha$ 。可以理解为它们有一个智能的内循环(线搜索算法)，它会自动尝试不同的学习速率 $\alpha$，并自动选择一个最好的学习速率 $\alpha$ 。甚至还可以为每次迭代选择不同的学习速率，那么就不需要自己选择了。</li><li>收敛速度远远快于梯度下降。</li></ol><p>缺点就是相比梯度下降而言，更加复杂。</p><p>举个例子：</p><pre class=" language-c"><code class="language-c">function <span class="token punctuation">[</span>jVal<span class="token punctuation">,</span> gradient<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">costFunction</span><span class="token punctuation">(</span>theta<span class="token punctuation">)</span>jVal <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token function">theta</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token operator">^</span><span class="token number">2</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token function">theta</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token operator">^</span><span class="token number">2</span><span class="token punctuation">;</span>gradient <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">gradient</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span><span class="token punctuation">(</span><span class="token function">theta</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">gradient</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span><span class="token punctuation">(</span><span class="token function">theta</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>调用高级函数 fminunc:</p><pre class=" language-c"><code class="language-c">options <span class="token operator">=</span> <span class="token function">optimset</span><span class="token punctuation">(</span><span class="token string">'GrabObj'</span><span class="token punctuation">,</span><span class="token string">'on'</span><span class="token punctuation">,</span><span class="token string">'MaxIter'</span><span class="token punctuation">,</span><span class="token string">'100'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>initialTheta <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">[</span>optTheta<span class="token punctuation">,</span> functionVal<span class="token punctuation">,</span> exitFlag<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span>@costFunction<span class="token punctuation">,</span> initialTheta<span class="token punctuation">,</span> options<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>最终结果:</p><pre class=" language-c"><code class="language-c">optTheta <span class="token operator">=</span>     <span class="token number">5.0000</span>    <span class="token number">5.0000</span>functionVal <span class="token operator">=</span> <span class="token number">1.5777e-030</span>exitFlag <span class="token operator">=</span> <span class="token number">1</span></code></pre><p>optTheta 表示的是最终求得的结果，functionVal 表示的是代价函数的最小值，这里是 0，是我们期望的。exitFlag 表示的是最终是否收敛，1表示收敛。</p><p>这里的 fminunc 是试图找到一个多变量函数的最小值，从一个估计的初试值开始，这通常被认为是无约束非线性优化问题。</p><p>另外一些例子：</p><pre class=" language-c"><code class="language-c">x <span class="token operator">=</span><span class="token function">fminunc</span><span class="token punctuation">(</span>fun<span class="token punctuation">,</span>x0<span class="token punctuation">)</span>                                   <span class="token operator">%</span>试图从x0附近开始找到函数的局部最小值，x0可以是标量，向量或矩阵x <span class="token operator">=</span><span class="token function">fminunc</span><span class="token punctuation">(</span>fun<span class="token punctuation">,</span>x0<span class="token punctuation">,</span>options<span class="token punctuation">)</span>                           <span class="token operator">%</span>根据结构体options中的设置来找到最小值，可用optimset来设置optionsx <span class="token operator">=</span><span class="token function">fminunc</span><span class="token punctuation">(</span>problem<span class="token punctuation">)</span>                                  <span class="token operator">%</span>为problem找到最小值<span class="token punctuation">,</span>而problem是在Input Arguments中定义的结构体<span class="token punctuation">[</span>x<span class="token punctuation">,</span>fval<span class="token punctuation">]</span><span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>                               <span class="token operator">%</span>返回目标函数fun在解x处的函数值<span class="token punctuation">[</span>x<span class="token punctuation">,</span>fval<span class="token punctuation">,</span>exitflag<span class="token punctuation">]</span><span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>                      <span class="token operator">%</span>返回一个描述退出条件的值exitflag<span class="token punctuation">[</span>x<span class="token punctuation">,</span>fval<span class="token punctuation">,</span>exitflag<span class="token punctuation">,</span>output<span class="token punctuation">]</span><span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>               <span class="token operator">%</span>返回一个叫output的结构体，它包含着优化的信息<span class="token punctuation">[</span>x<span class="token punctuation">,</span>fval<span class="token punctuation">,</span>exitflag<span class="token punctuation">,</span>output<span class="token punctuation">,</span>grad<span class="token punctuation">]</span><span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>          <span class="token operator">%</span>返回函数在解x处的梯度的值，存储在grad中<span class="token punctuation">[</span>x<span class="token punctuation">,</span>fval<span class="token punctuation">,</span>exitflag<span class="token punctuation">,</span>output<span class="token punctuation">,</span>grad<span class="token punctuation">,</span>hessian<span class="token punctuation">]</span><span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>  <span class="token operator">%</span>返回函数在解x处的Hessian矩阵的值，存储在hessian中</code></pre><hr><h2 id="三-Multiclass-Classification"><a href="#三-Multiclass-Classification" class="headerlink" title="三. Multiclass Classification"></a>三. Multiclass Classification</h2><p>这一章节我们来讨论一下如何利用逻辑回归来解决多类别分类问题。介绍一个一对多的分类算法。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/69_7.png" alt=""></p><p>现在，当我们有两个以上的类别时，我们将处理数据的分类。我们将扩展我们的定义，使得y = {0,1 … n}，而不是y = {0,1}。 由于y = {0,1 … n}，我们将问题分成n + 1（+1，因为索引从0开始）二元分类问题;在每一个中，我们都预测’y’是我们其中一个类的成员的概率。</p><p>最终在 n + 1 个分类器中分别输入 x ，然后取这 n + 1 个分类器概率的最大值,即是对应 $y=i$ 的概率值。</p><hr><h2 id="四-Logistic-Regression-测试"><a href="#四-Logistic-Regression-测试" class="headerlink" title="四. Logistic Regression 测试"></a>四. Logistic Regression 测试</h2><h3 id="1-Question-1"><a href="#1-Question-1" class="headerlink" title="1. Question 1"></a>1. Question 1</h3><p>Suppose that you have trained a logistic regression classifier, and it outputs on a new example x a prediction hθ(x) = 0.7. This means (check all that apply):</p><p>A. Our estimate for P(y=1|x;θ) is 0.7.<br>B. Our estimate for P(y=0|x;θ) is 0.3.<br>C. Our estimate for P(y=1|x;θ) is 0.3.<br>D. Our estimate for P(y=0|x;θ) is 0.7.  </p><p>解答： A、B  </p><h3 id="2-Question-2"><a href="#2-Question-2" class="headerlink" title="2. Question 2"></a>2. Question 2</h3><p>Suppose you have the following training set, and fit a logistic regression classifier hθ(x)=g(θ0+θ1x1+θ2x2).</p><p>Which of the following are true? Check all that apply.</p><p>A. Adding polynomial features (e.g., instead using hθ(x)=g(θ0+θ1x1+θ2x2+θ3x21+θ4x1x2+θ5x22) ) could increase how well we can fit the training data.  </p><p>B. At the optimal value of θ (e.g., found by fminunc), we will have J(θ)≥0.  </p><p>C. Adding polynomial features (e.g., instead using hθ(x)=g(θ0+θ1x1+θ2x2+θ3x21+θ4x1x2+θ5x22) ) would increase J(θ) because we are now summing over more terms.  </p><p>D. If we train gradient descent for enough iterations, for some examples x(i) in the training set it is possible to obtain hθ(x(i))&gt;1.  </p><p>解答： A、B</p><h3 id="3-Question-3"><a href="#3-Question-3" class="headerlink" title="3. Question 3"></a>3. Question 3</h3><p>For logistic regression, the gradient is given by ∂∂θjJ(θ)=1m∑mi=1(hθ(x(i))−y(i))x(i)j. Which of these is a correct gradient descent update for logistic regression with a learning rate of α? Check all that apply.</p><p>A. θj:=θj−α1m∑mi=1(hθ(x(i))−y(i))x(i) (simultaneously update for all j).  </p><p>B. θj:=θj−α1m∑mi=1(hθ(x(i))−y(i))x(i)j (simultaneously update for all j).  </p><p>C. θj:=θj−α1m∑mi=1(11+e−θTx(i)−y(i))x(i)j (simultaneously update for all j).  </p><p>D. θ:=θ−α1m∑mi=1(θTx−y(i))x(i).  </p><p>解答： A、D</p><p>线性回归与逻辑回归的区别</p><h3 id="4-Question-4"><a href="#4-Question-4" class="headerlink" title="4. Question 4"></a>4. Question 4</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. The cost function J(θ) for logistic regression trained with m≥1 examples is always greater than or equal to zero.  </p><p>B. Linear regression always works well for classification if you classify by using a threshold on the prediction made by linear regression.  </p><p>C. The one-vs-all technique allows you to use logistic regression for problems in which each y(i) comes from a fixed, discrete set of values.   </p><p>D. For logistic regression, sometimes gradient descent will converge to a local minimum (and fail to find the global minimum). This is the reason we prefer more advanced optimization algorithms such as fminunc (conjugate gradient/BFGS/L-BFGS/etc).  </p><p>解答： A、C</p><p>D由于使用代价函数为线性回归代价函数，会有很多局部最优值</p><h3 id="5-Question-5"><a href="#5-Question-5" class="headerlink" title="5. Question 5"></a>5. Question 5</h3><p>Suppose you train a logistic classifier hθ(x)=g(θ0+θ1x1+θ2x2). Suppose θ0=6,θ1=0,θ2=−1. Which of the following figures represents the decision boundary found by your classifier?</p><p>解答： C</p><p>6-x2&gt;=0 即X2&lt;6时为1</p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Logistic_Regression.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Logistic_Regression.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.2_Computing_Parameters_Analytically</title>
      <link href="/2020/01/27/2-2-computing-parameters-analytically/"/>
      <url>/2020/01/27/2-2-computing-parameters-analytically/</url>
      
        <content type="html"><![CDATA[<h1 id="2-2-Computing-Parameters-Analytically"><a href="#2-2-Computing-Parameters-Analytically" class="headerlink" title="2.2_Computing_Parameters_Analytically"></a>2.2_Computing_Parameters_Analytically</h1><h2 id="一-Normal-Equation"><a href="#一-Normal-Equation" class="headerlink" title="一. Normal Equation"></a>一. Normal Equation</h2><h3 id="1-正规方程"><a href="#1-正规方程" class="headerlink" title="1. 正规方程"></a>1. 正规方程</h3><p>正规方程法相对梯度下降法，它可以一步找到最小值。而且它也不需要进行特征值的缩放。</p><p>样本集是 $ m * n $ 的矩阵，每行样本表示为 $ \vec{x^{(i)}} $ ,第 i 行第 n 列分别表示为 $ x^{(i)}_{0} , x^{(i)}_{1} , x^{(i)}_{2} , x^{(i)}_{3} \cdots x^{(i)}_{n} $, m 行向量分别表示为 $ \vec{x^{(1)}} , \vec{x^{(2)}} , \vec{x^{(3)}} , \cdots \vec{x^{(m)}} $</p><p>令 </p>$$ \vec{x^{(i)}} = \begin{bmatrix} x^{(i)}_{0}\\ x^{(i)}_{1}\\ \vdots \\ x^{(i)}_{n}\\ \end{bmatrix} $$<p>$ \vec{x^{(i)}} $ 是这样一个 $(n+1)*1$ 维向量。每行都对应着 i 行 0-n 个变量。</p><p>再构造几个矩阵：</p>$$ X = \begin{bmatrix} (\vec{x^{(1)}})^{T}\\  \vdots \\  (\vec{x^{(m)}})^{T} \end{bmatrix} \;\;\;\;\Theta = \begin{bmatrix} \theta_{0}\\ \theta_{1}\\ \vdots \\ \theta_{n}\\ \end{bmatrix} \;\;\;\;Y = \begin{bmatrix} y^{(1)}\\ y^{(2)}\\ \vdots \\ y^{(m)}\\ \end{bmatrix} $$<p>X 是一个 $ m * (n+1)$ 的矩阵，$ \Theta $ 是一个 $ (n+1) * 1$ 的向量，Y 是一个 $ m * 1$的矩阵。</p><p>对比之前代价函数中，$$ \rm{CostFunction} = \rm{F}({\theta_{0}},{\theta_{1}}) = \frac{1}{2m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2 $$</p>  $$\begin{align*}X \cdot \Theta - Y = \begin{bmatrix}(\vec{x^{(1)}})^{T}\\ \vdots \\ (\vec{x^{(m)}})^{T}\end{bmatrix} \cdot \begin{bmatrix} \theta_{0}\\ \theta_{1}\\ \vdots \\ \theta_{n}\\ \end{bmatrix} - \begin{bmatrix} y^{(1)}\\ y^{(2)}\\ \vdots \\ y^{(m)}\\ \end{bmatrix} = \begin{bmatrix} h_{\theta}(x^{(1)})-y^{(1)}\\ h_{\theta}(x^{(2)})-y^{(2)}\\ \vdots \\ h_{\theta}(x^{(m)})-y^{(m)}\\ \end{bmatrix}\end{align*}$$<p>代入到之前代价函数中，</p>$$\begin{align*}\rm{CostFunction} = \rm{F}({\theta_{0}},{\theta_{1}}) &amp;= \frac{1}{2m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2\\&amp; = \frac{1}{2m} (X \cdot \Theta - Y)^{T}(X \cdot \Theta - Y)\\\end{align*}$$<hr><h3 id="2-矩阵的微分和矩阵的迹"><a href="#2-矩阵的微分和矩阵的迹" class="headerlink" title="2. 矩阵的微分和矩阵的迹"></a>2. 矩阵的微分和矩阵的迹</h3><p>接下来在进行推导之前，需要引入矩阵迹的概念，因为迹是求解一阶矩阵微分的工具。</p><p>矩阵迹的定义是 </p><p>$$ \rm{tr} A =  \sum_{i=1}^{n}A_{ii}$$ </p><p>简单的说就是左上角到右下角对角线上元素的和。</p><p>接下来有几个性质在下面推导过程中需要用到：</p><ol><li><p>$ \rm{tr};a = a $ ， a 是标量 ( $ a \in \mathbb{R} $)  </p></li><li><p>$ \rm{tr};AB = \rm{tr};BA $ 更近一步 $ \rm{tr};ABC = \rm{tr};CAB = \rm{tr};BCA $<br>证明：假设 A 是 $n * m$ 矩阵， B 是 $m * n$ 矩阵，则有<br>$$ \rm{tr};AB = \sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji} = \sum_{j=1}^{n} \sum_{i=1}^{m}B_{ji}A_{ij}= \rm{tr};BA $$<br>同理：$$ \rm{tr};ABC = \rm{tr};(AB)C = \rm{tr};C(AB) = \rm{tr};CAB$$<br>$$ \rm{tr};ABC = \rm{tr};A(BC) = \rm{tr};(BC)A = \rm{tr};BCA$$<br>连起来，即 $$ \rm{tr};ABC = \rm{tr};CAB = \rm{tr};BCA $$</p></li><li><p>$ \triangledown_{A}\rm{tr};AB = \triangledown_{A}\rm{tr};BA = B^{T}$<br>证明：按照矩阵梯度的定义：</p>   $$\triangledown_{X}f(X) = \begin{bmatrix}   \frac{\partial f(X) }{\partial x_{11}} &amp; \cdots &amp; \frac{\partial f(X) }{\partial x_{1n}}\\    \vdots &amp; \ddots  &amp; \vdots \\    \frac{\partial f(X) }{\partial x_{m1}} &amp; \cdots &amp; \frac{\partial f(X) }{\partial x_{mn}}   \end{bmatrix} = \frac{\partial f(X) }{\partial X}$$   <p>假设 A 是 $n * m$ 矩阵， B 是 $m * n$ 矩阵，则有</p>   $$\begin{align*}\triangledown_{A}\rm{tr}\;AB &amp;= \triangledown_{A} \sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji}  = \frac{\partial}{\partial A}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji})\\ &amp; = \begin{bmatrix}   \frac{\partial}{\partial A_{11}}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji}) &amp; \cdots &amp; \frac{\partial}{\partial A_{1m}}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji})\\    \vdots &amp; \ddots  &amp; \vdots \\    \frac{\partial}{\partial A_{n1}}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji}) &amp; \cdots &amp; \frac{\partial}{\partial A_{nm}}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji})   \end{bmatrix} \\ &amp; = \begin{bmatrix}   B_{11} &amp; \cdots &amp; B_{m1} \\    \vdots &amp; \ddots  &amp; \vdots \\    B_{1n} &amp; \cdots &amp; B_{mn}   \end{bmatrix} = B^{T}\\ \end{align*}$$   </li></ol>      $$\begin{align*}\triangledown_{A}\rm{tr}\;BA &amp;= \triangledown_{A} \sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji}  = \frac{\partial}{\partial A}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji})\\ &amp; = \begin{bmatrix}   \frac{\partial}{\partial A_{11}}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji}) &amp; \cdots &amp; \frac{\partial}{\partial A_{1m}}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji})\\    \vdots &amp; \ddots  &amp; \vdots \\    \frac{\partial}{\partial A_{n1}}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji}) &amp; \cdots &amp; \frac{\partial}{\partial A_{nm}}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji})   \end{bmatrix} \\ &amp; = \begin{bmatrix}   B_{11} &amp; \cdots &amp; B_{m1} \\    \vdots &amp; \ddots  &amp; \vdots \\    B_{1n} &amp; \cdots &amp; B_{mn}   \end{bmatrix} = B^{T}\\ \end{align*}$$   <p>   所以有 $ \triangledown_{A}\rm{tr};AB = \triangledown_{A}\rm{tr};BA = B^{T}$</p><ol start="4"><li><p>$\triangledown_{A^{T}}a = (\triangledown_{A}a)^{T};;;; (a \in \mathbb{R})$<br>证明：假设 A 是 $n * m$ 矩阵</p>   $$\begin{align*}\triangledown_{A^{T}}a &amp; = \begin{bmatrix}   \frac{\partial}{\partial A_{11}}a &amp; \cdots &amp; \frac{\partial}{\partial A_{1n}}a\\    \vdots &amp; \ddots  &amp; \vdots \\    \frac{\partial}{\partial A_{m1}}a &amp; \cdots &amp; \frac{\partial}{\partial A_{mn}}a   \end{bmatrix}  = (\begin{bmatrix}   \frac{\partial}{\partial A_{11}}a &amp; \cdots &amp; \frac{\partial}{\partial A_{1m}}a\\    \vdots &amp; \ddots  &amp; \vdots \\    \frac{\partial}{\partial A_{n1}}a &amp; \cdots &amp; \frac{\partial}{\partial A_{nm}}a   \end{bmatrix})^{T} \\ &amp; = (\triangledown_{A}a)^{T}\\ \end{align*}$$   </li><li><p>$\mathrm{d}(\rm{tr};A) = \rm{tr}(\mathrm{d}A)$<br>证明：<br>$$\mathrm{d}(\rm{tr};A) = \mathrm{d}(\sum_{i=1}^{n}a_{ii}) = \sum_{i=1}^{n}\mathrm{d}a_{ii} = \rm{tr}(\mathrm{d}A)$$<br>矩阵的迹的微分等于矩阵的微分的迹。</p></li><li><p>$\triangledown_{A}\rm{tr};ABA^{T}C = CAB + C^{T}AB^{T}$<br>证明：<br>根据实标量函数梯度的乘法法则：<br>若 f(A)、g(A)、h(A) 分别是矩阵 A 的实标量函数，则有   $$\begin{align*}\frac{\partial f(A)g(A)}{\partial A} &amp;= g(A)\frac{\partial f(A)}{\partial A} + f(A)\frac{\partial g(A)}{\partial A}\\ \frac{\partial f(A)g(A)h(A)}{\partial A} &amp;= g(A)h(A)\frac{\partial f(A)}{\partial A} + f(A)h(A)\frac{\partial g(A)}{\partial A}+ f(A)g(A)\frac{\partial h(A)}{\partial A}\\ \end{align*}$$<br>令 $f(A) = AB,g(A) = A^{T}C$，由性质5，矩阵的迹的微分等于矩阵的微分的迹，那么则有：</p>   $$\begin{align*} \triangledown_{A}\rm{tr}\;ABA^{T}C &amp; = \rm{tr}(\triangledown_{A}ABA^{T}C) = \rm{tr}(\triangledown_{A}f(A)g(A)) = \rm{tr}\triangledown_{A_{1}}(A_{1}BA^{T}C) + \rm{tr}\triangledown_{A_{2}}(ABA_{2}^{T}C)  \\ &amp; = (BA^{T}C)^{T} + \rm{tr}\triangledown_{A_{2}}(ABA_{2}^{T}C) = C^{T}AB^{T} + \triangledown_{A_{2}}\rm{tr}(ABA_{2}^{T}C)\\ &amp; = C^{T}AB^{T} + \triangledown_{A_{2}}\rm{tr}(A_{2}^{T}CAB) = C^{T}AB^{T} + (\triangledown_{{A_{2}}^{T}}\;\rm{tr}\;A_{2}^{T}CAB)^{T} \\ &amp; = C^{T}AB^{T} + ((CAB)^{T})^{T}  \\ &amp; = C^{T}AB^{T} + CAB  \\ \end{align*}$$   </li></ol><hr><h3 id="3-推导"><a href="#3-推导" class="headerlink" title="3. 推导"></a>3. 推导</h3><p>回到之前的代价函数中：<br>$$<br>\rm{CostFunction} = \rm{F}({\theta_{0}},{\theta_{1}}) = \frac{1}{2m} (X \cdot \Theta - Y)^{T}(X \cdot \Theta - Y)<br>$$<br>求导：</p>$$\begin{align*}\triangledown_{\theta}\rm{F}(\theta) &amp; = \frac{1}{2m} \triangledown_{\theta}(X \cdot \Theta - Y)^{T}(X \cdot \Theta - Y) = \frac{1}{2m}\triangledown_{\theta}(\Theta^{T}X^{T}-Y^{T})(X\Theta-Y)\\&amp; = \frac{1}{2m}\triangledown_{\theta}(\Theta^{T}X^{T}X\Theta-Y^{T}X\Theta-\Theta^{T}X^{T}Y+Y^{T}Y) \\ \end{align*}$$<p>上式中，对 $\Theta $矩阵求导，$ Y^{T}Y $ 与 $\Theta $ 无关，所以这一项为 0 。 $Y^{T}X\Theta$ 是标量，由性质4可以知道，$Y^{T}X\Theta = (Y^{T}X\Theta)^{T} = \Theta^{T}X^{T}Y$，因为 $\Theta^{T}X^{T}X\Theta , Y^{T}X\Theta $都是标量，所以它们的也等于它们的迹，（处理矩阵微分的问题常常引入矩阵的迹），于是有</p>$$\begin{align*}\triangledown_{\theta}\rm{F}(\theta) &amp; = \frac{1}{2m}\triangledown_{\theta}(\Theta^{T}X^{T}X\Theta-2Y^{T}X\Theta) \\ &amp; = \frac{1}{2m}\triangledown_{\theta}\rm{tr}\;(\Theta^{T}X^{T}X\Theta-2Y^{T}X\Theta) \\ &amp; = \frac{1}{2m}\triangledown_{\theta}\rm{tr}\;(\Theta\Theta^{T}X^{T}X-2Y^{T}X\Theta) \\ &amp; = \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr}\;\Theta\Theta^{T}X^{T}X -\triangledown_{\theta}\rm{tr}\;Y^{T}X\Theta) \\ &amp; = \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr}\;\Theta\Theta^{T}X^{T}X -(Y^{T}X)^{T}) = \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr}\;\Theta\Theta^{T}X^{T}X -X^{T}Y)\\ \end{align*}$$<p>上面第三步用的性质2矩阵迹的交换律，第五步用的性质3。</p><p>为了能进一步化简矩阵的微分，我们在矩阵的迹上面乘以一个单位矩阵，不影响结果。于是：</p>$$\begin{align*}\triangledown_{\theta}\rm{F}(\theta) &amp; = \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr}\;\Theta\Theta^{T}X^{T}X -X^{T}Y) \\ &amp;= \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr}\;\Theta I \Theta^{T}X^{T}X -X^{T}Y) \end{align*}$$<p>利用性质6 展开上面的式子，令 $ A = \Theta , B = I , C = X^{T}X $。</p>$$\begin{align*}\triangledown_{\theta}\rm{F}(\theta) &amp;= \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr}\;\Theta I \Theta^{T}X^{T}X -X^{T}Y) \\ &amp; = \frac{1}{m}(\frac{1}{2}(X^{T}X\Theta I + (X^{T}X)^{T}\Theta I^{T}) -X^{T}Y) \\ &amp; = \frac{1}{m}(\frac{1}{2}(X^{T}X\Theta I + (X^{T}X)^{T}\Theta I^{T}) -X^{T}Y) \\ &amp; = \frac{1}{m}(\frac{1}{2}(X^{T}X\Theta + X^{T}X\Theta) -X^{T}Y)  = \frac{1}{m}(X^{T}X\Theta -X^{T}Y) \\ \end{align*}$$<p>令 $\triangledown_{\theta}\rm{F}(\theta) = 0$，即 $X^{T}X\Theta -X^{T}Y = 0$, 于是 $ X^{T}X\Theta = X^{T}Y $ ，这里假设 $ X^{T}X$ 这个矩阵是可逆的，等号两边同时左乘$ X^{T}X$的逆矩阵，得到 $\Theta = (X^{T}X)^{-1}X^{T}Y$</p><p>最终结果也就推导出来了，$$\Theta = (X^{T}X)^{-1}X^{T}Y$$</p><p>但是这里有一个<strong>前提条件是 $X^{T}X$ 是非奇异(非退化)矩阵， 即 $ \left | X^{T}X \right | \neq 0 $</strong></p><hr><h3 id="4-梯度下降和正规方程法比较："><a href="#4-梯度下降和正规方程法比较：" class="headerlink" title="4. 梯度下降和正规方程法比较："></a>4. 梯度下降和正规方程法比较：</h3><p>优点：<br>梯度下降在超大数据集面前也能运行的很良好。<br>正规方程在超大数据集合面前性能会变得很差，因为需要计算 $(x^{T}x)^{-1}$,时间复杂度在 $O(n^{3})$ 这个级别。  </p><p>缺点：<br>梯度下降需要合理的选择学习速率 $\alpha$ , 需要很多次迭代的操作去选择合理的 $\alpha$，寻找最小值的时候也需要迭代很多次才能收敛。<br>正规方程的优势相比而言，不需要选择学习速率 $\alpha$，也不需要多次的迭代或者画图检测是否收敛。</p><hr><h2 id="二-Normal-Equation-Noninvertibility"><a href="#二-Normal-Equation-Noninvertibility" class="headerlink" title="二. Normal Equation Noninvertibility"></a>二. Normal Equation Noninvertibility</h2><p>上一章谈到了如何利用正规方程法求解 $\Theta $,但是在线性代数中存在这样一个问题，如果是奇异(退化)矩阵，是不存在逆矩阵的。也就是说用上面正规方程的公式是不一定能求解出正确结果的。</p><p>在 Octave 软件中，存在2个求解逆矩阵的函数，一个是 pinv 和 inv。pinv (pseudo-inverse)求解的是<strong>伪逆矩阵</strong>，inv 求解的是逆矩阵，所以用 pinv 求解问题，就算是 $ X^{T}X $ 不存在逆矩阵，也一样可以得到最后的结果。</p><p>导致$ X^{T}X $ 不存在逆矩阵有2种情况：</p><ol><li>多余的特征。特征之间呈倍数关系，线性依赖。</li><li>过多的特征。当 $ m \leqslant n $ 的时候，会导致过多的特征。解决办法是删除一些特征，或者进行正则化。</li></ol><p>所以解决$ X^{T}X $ 不存在逆矩阵的办法也就是对应上面2种情况：</p><ol><li>删掉多余的特征，线性相关的，倍数关系的。直到没有多余的特征</li><li>再删除一些不影响结果的特征，或者进行正则化。</li></ol><hr><h2 id="三-Linear-Regression-with-Multiple-Variables-测试"><a href="#三-Linear-Regression-with-Multiple-Variables-测试" class="headerlink" title="三. Linear Regression with Multiple Variables 测试"></a>三. Linear Regression with Multiple Variables 测试</h2><h3 id="1-Question-1"><a href="#1-Question-1" class="headerlink" title="1. Question 1"></a>1. Question 1</h3><p>Suppose m=4 students have taken some class, and the class had a midterm exam and a final exam. You have collected a dataset of their scores on the two exams, which is as follows:</p><p>midterm exam    (midterm exam)2    final exam<br>89    7921    96<br>72    5184    74<br>94    8836    87<br>69    4761    78<br>You’d like to use polynomial regression to predict a student’s final exam score from their midterm exam score. Concretely, suppose you want to fit a model of the form hθ(x)=θ0+θ1x1+θ2x2, where x1 is the midterm score and x2 is (midterm score)2. Further, you plan to use both feature scaling (dividing by the “max-min”, or range, of a feature) and mean normalization.</p><p>What is the normalized feature x(2)2? (Hint: midterm = 72, final = 74 is training example 2.) Please round off your answer to two decimal places and enter in the text box below.</p><p>解答：<br>标准化 $$x = \frac{x_{2}^{2}-\frac{(7921+5184+8836+4761)}{4}}{\max - \min } = \frac{5184 - 6675.5}{8836-4761} = -0.37$$</p><h3 id="2-Question-2"><a href="#2-Question-2" class="headerlink" title="2. Question 2"></a>2. Question 2</h3><p>You run gradient descent for 15 iterations</p><p>with α=0.3 and compute J(θ) after each</p><p>iteration. You find that the value of J(θ) increases over</p><p>time. Based on this, which of the following conclusions seems</p><p>most plausible?</p><p>A. Rather than use the current value of α, it’d be more promising to try a smaller value of α (say α=0.1).</p><p>B. α=0.3 is an effective choice of learning rate.</p><p>C. Rather than use the current value of α, it’d be more promising to try a larger value of α (say α=1.0).</p><p>解答：  A </p><p>下降太快所以a下降速率过大，a越大下降越快，a小下降慢，在本题中，代价函数快速收敛到最小值，代表此时a最合适。</p><h3 id="3-Question-3"><a href="#3-Question-3" class="headerlink" title="3. Question 3"></a>3. Question 3</h3><p>Suppose you have m=28 training examples with n=4 features (excluding the additional all-ones feature for the intercept term, which you should add). The normal equation is θ=(XTX)−1XTy. For the given values of m and n, what are the dimensions of θ, X, and y in this equation?</p><p>A. X is 28×4, y is 28×1, θ is 4×4</p><p>B. X is 28×5, y is 28×5, θ is 5×5</p><p>C. X is 28×5, y is 28×1, θ is 5×1</p><p>D. X is 28×4, y is 28×1, θ is4×1</p><p>解答：  C </p><p>这里需要注意的是，题目中说了额外添加一列全部为1的，所以列数是5 。</p><h3 id="4-Question-4"><a href="#4-Question-4" class="headerlink" title="4. Question 4"></a>4. Question 4</h3><p>Suppose you have a dataset with m=50 examples and n=15 features for each example. You want to use multivariate linear regression to fit the parameters θ to our data. Should you prefer gradient descent or the normal equation?</p><p>A. Gradient descent, since it will always converge to the optimal θ.</p><p>B. Gradient descent, since (XTX)−1 will be very slow to compute in the normal equation.</p><p>C. The normal equation, since it provides an efficient way to directly find the solution.</p><p>D. The normal equation, since gradient descent might be unable to find the optimal θ.</p><p>解答：  C </p><p>数据量少，选择正规方程法更加高效</p><h3 id="5-Question-5"><a href="#5-Question-5" class="headerlink" title="5. Question 5"></a>5. Question 5</h3><p>Which of the following are reasons for using feature scaling?</p><p>A. It prevents the matrix XTX (used in the normal equation) from being non-invertable (singular/degenerate).</p><p>B. It is necessary to prevent the normal equation from getting stuck in local optima.</p><p>C. It speeds up gradient descent by making it require fewer iterations to get to a good solution.</p><p>D. It speeds up gradient descent by making each iteration of gradient descent less expensive to compute.</p><p>解答：  C </p><p>normal equation 不需要 Feature Scaling，排除AB， 特征缩放减少迭代数量，加快梯度下降，然而不能防止梯度下降陷入局部最优。</p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Computing_Parameters_Analytically.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Computing_Parameters_Analytically.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.1_Multivariate_Linear_Regression</title>
      <link href="/2020/01/27/2-1-multivariate-linear-regression/"/>
      <url>/2020/01/27/2-1-multivariate-linear-regression/</url>
      
        <content type="html"><![CDATA[<h1 id="Multivariate-Linear-Regression"><a href="#Multivariate-Linear-Regression" class="headerlink" title="Multivariate Linear Regression"></a>Multivariate Linear Regression</h1><h2 id="一-Multiple-Features"><a href="#一-Multiple-Features" class="headerlink" title="一. Multiple Features"></a>一. Multiple Features</h2><p>具有多个变量的线性回归也被称为“多元线性回归”。</p><p>$x_{j}^{(i)}$: 训练集第 i 个向量中的第 j 个元素(第 i 行第 j 列)<br>$x^{(i)}$: 训练集第 i 个向量(第 i 行)<br>$ m $: 总共 m 行<br>$ n $: 总共 n 列  </p><p>适应这些多特征的假设函数的多变量形式如下：</p><p>$$ h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} + \cdots + \theta_{n}x_{n} $$</p><p>使用矩阵乘法的定义，我们的多变量假设函数可以简洁地表示为：</p><p>$$ h_{\theta}(x) = \begin{bmatrix}<br>\theta_{0} &amp; \theta_{1} &amp; \cdots  &amp; \theta_{n}<br>\end{bmatrix} \begin{bmatrix}<br>x_{0}\<br>x_{1}\<br> \vdots \<br>x_{n}<br>\end{bmatrix} = \theta^{T}x$$</p><p>其中 $ x_{0}^{(i)} = 1 (i\in 1,\cdots,m)$</p><hr><h2 id="二-Gradient-Descent-for-Multiple-Variables"><a href="#二-Gradient-Descent-for-Multiple-Variables" class="headerlink" title="二. Gradient Descent for Multiple Variables"></a>二. Gradient Descent for Multiple Variables</h2><p>多个变量的梯度下降，同时更新 n 个变量。</p><p>$$ \theta_{j} := \theta_{j} - \alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_{j}$$</p><p>其中 $ j \in [0,n]$</p><hr><h2 id="三-Gradient-Descent-in-Practice-I-Feature-Scaling"><a href="#三-Gradient-Descent-in-Practice-I-Feature-Scaling" class="headerlink" title="三. Gradient Descent in Practice I - Feature Scaling"></a>三. Gradient Descent in Practice I - Feature Scaling</h2><p>特征缩放包括将输入值除以输入变量的范围（即最大值减去最小值），导致新的范围仅为1。</p><p>均值归一化包括从输入变量的值中减去输入变量的平均值，从而导致输入变量的新平均值为零。</p><h3 id="1-Feature-Scaling"><a href="#1-Feature-Scaling" class="headerlink" title="1. Feature Scaling"></a>1. Feature Scaling</h3><p>特征缩放让特征值取值范围都比较一致，这样在执行梯度下降的时候，“下山的路线”会更加简单，更快的收敛。通常进行特征缩放都会把特征值缩尽量缩放到 [-1,1] 之间<strong>或者这个区间附近</strong>。</p><p>即 $ x_{i} = \frac{x_{i}}{s_{i}}$</p><h3 id="2-Mean-normalization"><a href="#2-Mean-normalization" class="headerlink" title="2. Mean normalization"></a>2. Mean normalization</h3><p>$ x_{i} = \frac{x_{i} - \mu_{i}}{s_{i}}$</p><p>其中，$\mu_{i}$ 是特征值的所有值的平均值，$s_{i}$ 是值的范围（最大 - 最小），或者 $s_{i}$ 是标准偏差</p><p>当然 $x_{0} = 1$ 就不需要经过上述的处理了，因为它永远等于1，不能有均值等于0的情况。</p><hr><h2 id="四-Gradient-Descent-in-Practice-II-Learning-Rate"><a href="#四-Gradient-Descent-in-Practice-II-Learning-Rate" class="headerlink" title="四. Gradient Descent in Practice II - Learning Rate"></a>四. Gradient Descent in Practice II - Learning Rate</h2><p>如果学习率 $\alpha $ 太小的话，就会导致收敛速度过慢的问题。<br>如果学习率 $\alpha $ 太大的话，代价函数可能不会在每次迭代中都下降，甚至可能不收敛，在某种情况下，学习率 $\alpha $ 过大，也有可能出现收敛缓慢。</p><p>可以通过绘制代价函数随迭代步数变化的曲线去调试这个问题。</p><p>$\alpha $ 的取值可以从 0.001，0.003，0.01，0.03，0.1，0.3，1 这几个值去尝试，选一个最优的。</p><hr><h2 id="五-Features-and-Polynomial-Regression"><a href="#五-Features-and-Polynomial-Regression" class="headerlink" title="五. Features and Polynomial Regression"></a>五. Features and Polynomial Regression</h2><p>可以通过改造特征值，例如合并2个特征，用 $ x_{3}$ 来表示 $ x_{1} * x_{2} $</p><p>在多项式回归中，针对 $ h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{1}^{2} + \theta_{3}x_{1}^{3} $ ，我们可以令 $ x_{2} = x_{1}^{2} , x_{3} = x_{1}^{3} $ 降低次数。</p><p>还可以考虑用根号的式子，例如选用  $ h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}\sqrt{x} $</p><p>通过上述转换以后，需要记得用<strong>特征值缩放，均值归一化，调整学习速率的方式调整一下</strong>。</p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p><a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Multivariate_Linear_Regression.ipynb" target="_blank" rel="noopener">Source</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.2_Linear_Regression_With_One_Variable(Gradient_Descent)</title>
      <link href="/2020/01/26/1-2-linear-regression-with-one-variable/"/>
      <url>/2020/01/26/1-2-linear-regression-with-one-variable/</url>
      
        <content type="html"><![CDATA[<h1 id="Linear-Regression-With-One-Variable-Gradient-Descent"><a href="#Linear-Regression-With-One-Variable-Gradient-Descent" class="headerlink" title="Linear Regression With One Variable(Gradient Descent)"></a>Linear Regression With One Variable(Gradient Descent)</h1><h2 id="一-Model-Representation"><a href="#一-Model-Representation" class="headerlink" title="一. Model Representation"></a>一. Model Representation</h2><p>在给定训练集的情况下，学习函数h：X→Y，使得h（x）是y的相应值的“好”预测器。由于历史原因，这个函数h被称为假设。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/68_0_1.png" alt=""></p><p>通过输入住房面积 x，通过学习好的函数，输出房子的估价。</p><h2 id="二-Cost-Function"><a href="#二-Cost-Function" class="headerlink" title="二. Cost Function"></a>二. Cost Function</h2><p>代价函数是线性回归中的一个应用，在线性回归中，要解决的一个问题就是最小化问题。</p><p>假设在一元线性回归中，在一个训练集中，我们需要找到一条直线能和该训练集中的点最接近。假设直线方程为 </p><p>$$h_{\theta}(x) = \theta_{0} + \theta_{1}x$$</p><p>如何选择 $\theta_{0}$、$\theta_{1}$，使得 $h_{\theta}(x)$ 更接近于训练集 (x,y) ？</p><p>上述问题可以转换为求 $$ \rm{CostFunction} = \rm{F}({\theta_{0}},{\theta_{1}}) = \frac{1}{2m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2 $$  求最小值$$\min_{{\theta_{0}} {\theta_{1}}} \rm{F}({\theta_{0},{\theta_{1}})} $$</p><h2 id="三-Gradient-Descent-梯度下降"><a href="#三-Gradient-Descent-梯度下降" class="headerlink" title="三. Gradient Descent 梯度下降"></a>三. Gradient Descent 梯度下降</h2><p>梯度下降的主要思想：</p><ol><li><p>初始化<br>$$<br>{\theta_{0}}和 {\theta_{1}} , {\theta_{0}} = 0 , {\theta_{1}}=0<br>$$</p></li><li><p>不断的改变 ${\theta_{0}}$ 和 ${\theta_{1}}$ 值，不断减少 $F({\theta_{0}},{\theta_{1}})$ 直至达到最小值（或者局部最小）。</p></li></ol><p><img src="https://img.halfrost.com/Blog/ArticleImage/68_1_0.png" alt=""></p><p>想象成下山，如何下山的速度最快？这里涉及到了下山的速度，即步长。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/68_2_.png" alt=""></p><p>有趣的是换旁边一个点，下山，找到的最优解可能就是另一个了。这也是梯度下降的一个特点。它会找到所有的局部最优解出来。</p><p>梯度下降算法，不断更新：</p>\begin{align*}\rm{temp}0 &amp;:= {\theta_{0}} - \alpha * \frac{\partial }{\partial {\theta_{0}}}\rm{F}({\theta_{0}},{\theta_{1}}) \\\rm{temp}1 &amp;:= {\theta_{1}} - \alpha * \frac{\partial }{\partial {\theta_{1}}}\rm{F}({\theta_{0}},{\theta_{1}}) \\{\theta_{0}} &amp;:= \rm{temp}0 \\{\theta_{1}} &amp;:= \rm{temp}1 \\\end{align*}<p>直到收敛。注意 ${\theta_{0}}$ 和 ${\theta_{1}}$ 值要<strong>同时更新</strong>，<strong>切记不要求一次导更新一次！</strong></p><p>$\alpha$ 被称作为学习速率。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/68_3.gif" alt=""></p><p>如果 $\alpha$ 被设置的很小，需要很多次循环才能到底最低点。<br>如果 $\alpha$ 被设置的很大，来来回回可能就会离最低点越来越远，<strong>会导致无法收敛，甚至发散</strong>。</p><p>当快要到最低点的时候，梯度下降会越来越慢，因为 $ \frac{\partial }{\partial {\theta}}$ 越来越小。</p><h2 id="关于-梯度-和-偏导数-的关系"><a href="#关于-梯度-和-偏导数-的关系" class="headerlink" title="关于 梯度 和 偏导数 的关系"></a>关于 梯度 和 偏导数 的关系</h2><p>在上面梯度下降算法中，我们一直用的是偏导数进行讨论的，可能会有人有疑问，偏导数和梯度有啥关系？</p><h3 id="1-导数"><a href="#1-导数" class="headerlink" title="1. 导数"></a>1. 导数</h3><p>如果是一元的，那么偏导数就降级成了求导数</p>$$ f^{'}(x_{0}) = \lim_{\Delta x\rightarrow 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x\rightarrow 0} \frac{f(x_{0} + \Delta x) - f(x_{0})}{\Delta x} $$<p>导数的几何意义是切线在该点的斜率，物理意义是函数在这一点的 (瞬时) 变化率。</p><h3 id="2-偏导数"><a href="#2-偏导数" class="headerlink" title="2. 偏导数"></a>2. 偏导数</h3><p>在来看看偏导数的定义：</p>$$\begin{align*}f_{x}(x_{0},y_{0}) &amp; = \lim_{\Delta x \rightarrow 0} \frac{f(x_{0} + \Delta x , y_{0}) - f(x_{0},y_{0})}{\Delta x} \\ f_{y}(x_{0},y_{0}) &amp; = \lim_{\Delta y \rightarrow 0} \frac{f(x_{0} , y_{0} + \Delta y) - f(x_{0},y_{0})}{\Delta y} \\\end{align*}$$<p><img src="https://img.halfrost.com/Blog/ArticleImage/68_4.png" alt=""></p><p>偏导数的几何意义也是切线的斜率，不过由于在曲面上，在一个点上与该曲面曲线相切的是一个面，就意味着切线有无数条。这里我们感兴趣的是2条切线，一个条是垂直于 y 轴（平行于 xOz 平面）的切线，另外一条是垂直于 x 轴（平行于 yOz 平面）的切线。这两条切线对应的斜率就是对 X 求偏导和对 Y 求偏导。</p><p>一个多变量函数的偏导数是它关于其中一个变量的导数，而保持其他变量恒定（相对于全导数，在其中所有变量都允许变化）。</p><p>偏导数的物理意义表示函数沿着坐标轴正方向上的变化率。</p><h3 id="3-方向导数"><a href="#3-方向导数" class="headerlink" title="3. 方向导数"></a>3. 方向导数</h3><p>在说梯度之前，不应该漏掉方向导数。偏导数是求的在特定的2个方向上的导数，但是任意一个方向上也是存在导数的。这里就引入了方向导数的概念。</p><blockquote><p>设函数 u = u(x,y) 在点 $p_{0}(x_{0},y_{0})$ 的某空间临域 $ U \subset R^{2}$ 内有定义， L 为从点 $p_{0}$ 出发的射线，$p(x_{0},y_{0})$ 为 L 上且在 U 内的任一点，以 $t = \sqrt{(\Delta x)^{2} +(\Delta y)^{2} }$ 表示 $p$ 与 $p_{0}$ 之间的距离，若极限 ：</p></blockquote><blockquote>$$ \left.\begin{matrix}\frac{\partial f}{\partial l}\end{matrix}\right|_{(x_{0},y_{0})} = \lim_{t \rightarrow 0^{+}} \frac{f(x_{0} + tcos \alpha , y_{0}  +   tcos \beta) - f(x_{0},y_{0})}{t}$$<p>存在，则称此极限为函数 u = u(x,y) 在点 $p_{0}$ 沿方向 L 的方向导数，记作 $ \left.\begin{matrix}\frac{\partial f}{\partial l}\end{matrix}\right|_{(x_{0},y_{0})}$ 。</p></blockquote><p>方向导数是偏导数的概念的推广, 偏导数研究的是指定方向 (坐标轴方向) 的变化率，到了方向导数，指定的方向可以是任意方向了。</p><blockquote><p>如果函数 u = u(x,y) 在点 $p_{0}(x_{0},y_{0})$ 可微分，那么函数在该点沿任一方向 L 的方向导数存在，且有</p></blockquote><blockquote>$$ \left.\begin{matrix}\frac{\partial f}{\partial l}\end{matrix}\right|_{(x_{0},y_{0})} = f_{x}(x_{0},y_{0})cos \alpha + f_{y}(x_{0},y_{0})cos \beta$$</blockquote><blockquote><p>其中， $cos \alpha  $ ，$cos \beta$ 是方向 L 的方向余弦。</p></blockquote><p>一个标量场在某点沿着某个向量方向上的方向导数，描绘了该点附近标量场沿着该向量方向变动时的瞬时变化率。这个向量方向可以是任一方向。</p><p>方向导数的物理意义表示函数在某点沿着某一特定方向上的变化率。</p><h3 id="4-梯度"><a href="#4-梯度" class="headerlink" title="4. 梯度"></a>4. 梯度</h3><p>最后来讲讲梯度，梯度的定义：</p><blockquote><p>在二元函数的情形，设函数 $f(x,y)$ 在平面区域 D 内具有一阶连续偏导数，则对于每一点 $P_{0}(x_{0},y_{0}) \in D $,都可定出一个向量：</p></blockquote><blockquote>$$ f_{x}(x_{0},y_{0}) \vec{i} + f_{y}(x_{0},y_{0}) \vec{j} $$</blockquote><blockquote><p>这个向量称为函数 $f(x,y)$ 在点 $p_{0}(x_{0},y_{0})$ 的梯度，记作 $ \textbf{grad}\;\;f(x_{0},y_{0}) $ 或 $ \triangledown f(x_{0},y_{0}) $ , 即</p></blockquote><blockquote>$$ \textbf{grad}\;\;f(x_{0},y_{0}) = \triangledown f(x_{0},y_{0}) = f_{x}(x_{0},y_{0}) \vec{i} + f_{y}(x_{0},y_{0}) \vec{j} $$</blockquote><blockquote><p>其中 $ \triangledown = \frac{\partial }{\partial x} \vec{i} + \frac{\partial }{\partial y} \vec{j} $ 称为 (二维的) 向量微分算子 或者 Nabla 算子， $ \triangledown f = \frac{\partial f}{\partial x} \;\; \vec{i} + \frac{\partial f }{\partial y} \;\; \vec{j} $</p></blockquote><p>如果函数 $f(x,y)$ 在点 $p_{0}(x_{0},y_{0})$ 可微分， $\vec{e_{j}} = (cos \alpha,cos \beta)$ 是与方向 L 同向的单位向量，则：</p>$$\begin{align*}\left.\begin{matrix}\frac{\partial f}{\partial l}\end{matrix}\right|_{(x_{0},y_{0})} &amp;= f_{x}(x_{0},y_{0})cos \alpha + f_{y}(x_{0},y_{0})cos \beta \\&amp;= \textbf{grad}\;\;f(x_{0},y_{0}) \cdot \vec{e_{j}} = \left | \textbf{grad}\;\;f(x_{0},y_{0}) \right | cos \theta \\\end{align*}$$<p>其中 $ \theta $ 为 $ \textbf{grad};;f(x_{0},y_{0}) $ 与 $ \vec{e_{j}} $ 的夹角。</p><ol><li>当 $\theta = 0 $ 的时候，$\left.\begin{matrix}\frac{\partial f}{\partial l}\end{matrix}\right|_{(x_{0},y_{0})}  = \left | \textbf{grad}\;\;f(x_{0},y_{0}) \right |$</li></ol><p>即 <strong>函数 $f(x,y)$ 在一点的梯度 $ \textbf{grad};;f $ 是这样的一个向量，它的方向是函数在这点的方向导数取得最大值的方向，它的模就等于方向导数的最大值</strong> 。</p><ol start="2"><li>当 $\theta = \pi $ 的时候，$\left.\begin{matrix}\frac{\partial f}{\partial l}\end{matrix}\right|_{(x_{0},y_{0})}  = - \left | \textbf{grad}\;\;f(x_{0},y_{0}) \right |$</li></ol><p>即 $ \vec{e_{j}} $ 与 梯度 方向相反的时候，函数减少最快，在这个方向的方向导数达到最小值。</p><p><strong>所以梯度下降就是基于这个原理</strong>。</p><p>函数在某一点处的方向导数在其梯度方向上达到最大值，此最大值即梯度的模数。</p><p>这就是说，沿梯度方向，函数值增加最快。同样可知，方向导数的最小值在梯度的相反方向取得，此最小值为最大值的相反数，从而沿梯度相反方向函数值的减少最快。</p><table><thead><tr><th>概念</th><th>物理意义</th></tr></thead><tbody><tr><td>导数   $ f^{‘}(x)  $</td><td>函数在该点的瞬时变化率</td></tr><tr><td>偏导数 $ \frac{\partial f(x,y) }{\partial x}  $</td><td>函数在坐标轴方向上的变化率</td></tr><tr><td>方向导数</td><td>函数在某点沿某个特定方向的变化率</td></tr><tr><td>梯度  $ \textbf{grad};;f(x,y)  $</td><td>函数在该点沿所有方向变化率最大的那个方向</td></tr></tbody></table><h2 id="四-Linear-Regression-线性回归"><a href="#四-Linear-Regression-线性回归" class="headerlink" title="四. Linear Regression 线性回归"></a>四. Linear Regression 线性回归</h2><p>梯度下降是很常用的算法，它不仅被用在线性回归，还用在线性回归模型、平方误差代价函数中。</p>\begin{align*}\frac{\partial }{\partial {\theta_{j}}}\rm{F}({\theta_{0}},{\theta_{1}}) &amp; = \frac{\partial }{\partial {\theta_{j}}} \frac{1}{2m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2\\\end{align*}<p>令 $ z = (h_{\theta}(x^{(i)})-y^{(i)})^2$ , $ u = h_{\theta}(x^{(i)})-y^{(i)}$ , 则 $ z = u^2 $。 考虑到 $f(z)$  和 $f(u)$ 都是连续的，则有：</p>\begin{align*}\frac{\partial }{\partial {\theta_{j}}}\rm{F}({\theta_{0}},{\theta_{1}}) &amp; = \frac{\partial }{\partial {\theta_{j}}} \frac{1}{2m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2\\&amp; = \frac{1}{2m}\sum_{i = 1}^{m} \frac{\partial z }{\partial u} \frac{\partial u }{\partial {\theta_{j}}} = \frac{1}{2m} * 2 \sum_{i = 1}^{m} u \frac{\partial u }{\partial {\theta_{j}}}\\&amp; = \frac{1}{m} \sum_{i = 1}^{m} u \frac{\partial u }{\partial {\theta_{j}}} \\\end{align*}<p>++++++++++++</p><p>将 u 展开 $ u = \theta_{0} + {\theta_{1}}x^{(i)}-y^{(i)}$ , 令 j = 0,则有</p>\begin{align*}\frac{\partial }{\partial {\theta_{0}}}\rm{F}({\theta_{0}},{\theta_{1}}) &amp;= \frac{1}{m} \sum_{i = 1}^{m} u \frac{\partial u }{\partial {\theta_{0}}} \\&amp;= \frac{1}{m} \sum_{i = 1}^{m}(\theta_{0} + \theta_{1}x^{(i)} - y^{(i)}) = \frac{1}{m} \sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)}) \\\end{align*}<p>令 j = 1,则有</p>\begin{align*}\frac{\partial }{\partial {\theta_{1}}}\rm{F}({\theta_{0}},{\theta_{1}}) &amp;=  \frac{1}{m} \sum_{i = 1}^{m} u \frac{\partial u }{\partial {\theta_{1}}}\\&amp;= \frac{1}{m} \sum_{i = 1}^{m}(\theta_{0} + \theta_{1}x^{(i)} - y^{(i)}) * x^{(i)} = \frac{1}{m} \sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)}) * x^{(i)} \\\end{align*}<p>梯度下降算法：</p> \begin{align*}\rm{temp}0 &amp;:= {\theta_{0}} - \alpha * \frac{\partial }{\partial {\theta_{0}}}\rm{F}({\theta_{0}},{\theta_{1}}) = {\theta_{0}} - \alpha * \frac{1}{m} \sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})  \\\rm{temp}1 &amp;:= {\theta_{1}} - \alpha * \frac{\partial }{\partial {\theta_{1}}}\rm{F}({\theta_{0}},{\theta_{1}}) = {\theta_{1}} - \alpha * \frac{1}{m} \sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)}) * x^{(i)} \\{\theta_{0}} &amp;:= \rm{temp}0  \\{\theta_{1}} &amp;:= \rm{temp}1  \\\end{align*}<p>当然除了用梯度下降的迭代算法，还有其他方法可以算出代价函数的最小值，比如线性代数里面的 正规方程组法。但是两者相比较而言，梯度下降适合更大的数据集。</p><p>举个例子，通过梯度下降不断更新以后，线性回归以后的曲线和原始数据集会越来越拟合。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> npx_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.9</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.91</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8.02</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">4.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.23</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.923</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2.941</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.02</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">6.34</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.543</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.546</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8.744</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.674</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.643</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">5.33</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.31</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.78</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1.01</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.68</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">9.99</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3.54</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.89</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10.9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>y_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.34</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3.86</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.63</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.78</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10.6453</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8.43</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">4.75</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.345</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.546</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.5754</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2.35654</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.43646</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">6.6443</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.64534</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.546</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8.7457</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.6464</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.74643</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">6.32</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.42</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.1243</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1.088</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10.342</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">9.24</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4.22</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.44</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.33</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>y_data <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.9</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.91</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8.02</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">4.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.23</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.923</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2.941</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.02</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">6.34</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.543</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.546</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8.744</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.674</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.643</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">5.33</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.31</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.78</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1.01</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.68</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">9.99</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3.54</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.89</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10.9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token operator">%</span>matplotlib inlineplt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> <span class="token string">'bo'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'real'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_data<span class="token punctuation">,</span> <span class="token string">'r-'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'estimated'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre><code>&lt;matplotlib.legend.Legend at 0x7fa6805ab128&gt;</code></pre><p><img src="/2020/01/26/1-2-linear-regression-with-one-variable/output_7_1.png" alt="output"></p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Gradient_descent.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Gradient_descent.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.1_What_is_Machine_Learning</title>
      <link href="/2020/01/19/1-1-what-is-machine-learning/"/>
      <url>/2020/01/19/1-1-what-is-machine-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-Machine-Learning"><a href="#What-is-Machine-Learning" class="headerlink" title="What is Machine Learning"></a>What is Machine Learning</h1><p><img src="/2020/01/19/1-1-what-is-machine-learning/machine-learning.png" alt="Machine_Learning"></p><h2 id="一、-Definition"><a href="#一、-Definition" class="headerlink" title="一、 Definition"></a>一、 Definition</h2><p>定义：</p><p>1997年，<code>Tom Mitchell</code> 给出机器学习的定义：</p><pre><code>程序利用经验E改善了在任务T中的性能P，就可以说：关于任务T和测量性能P，该程序对经验E进行了学习。</code></pre><h2 id="二、-Classify"><a href="#二、-Classify" class="headerlink" title="二、 Classify"></a>二、 Classify</h2><p>分类：</p><ul><li>有监督学习<code>supervised learning</code> :已知的Data Set中明确了输入/输出，且输入和输出存在关系。 <ul><li><code>Supervised Learning</code>可以分为：分类(Classification)和回归(Regression)问题。</li><li><ol><li>Classification： 预测离散的结果。将输入映射到离散的类别中。</li></ol></li><li><ol start="2"><li>Regression： 预测连续输出中的结果。 从输入映射到某个连续的函数的输出中。</li></ol></li></ul></li></ul><blockquote><p>无监督学习使我们能够很少或根本不知道我们的结果应该是什么样子。<strong>我们可以从数据中得出结构</strong>，我们不一定知道变量的影响。 我们可以通过基于数据中变量之间的关系对数据进行聚类来推导出这种结构。 在无监督学习的情况下，<strong>没有基于预测结果的反馈</strong>。无监督学习可以分为“聚类”和“非聚类”。</p></blockquote><ul><li>无监督学习<code>unsupervised learning</code> : 没有预知的label，从变量的结构中寻找关系，而没有基于预测结果的反馈。<ul><li><code>Unspervised Learning</code>可以分为<code>聚类</code>， 和 <code>非聚类</code>。</li><li><ol><li>聚类： 可以理解为对数据自动分组成不同变量的相似或者相关的簇。</li></ol></li><li><ol start="2"><li>非聚类： 比如“鸡尾酒会算法”–&gt;从混乱的环境中识别和查找结果。</li></ol></li></ul></li></ul><blockquote><p>参考： GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>19年岁末</title>
      <link href="/2019/12/31/19-nian-sui-mo/"/>
      <url>/2019/12/31/19-nian-sui-mo/</url>
      
        <content type="html"><![CDATA[<h1 id="挥别2019，迎接变化"><a href="#挥别2019，迎接变化" class="headerlink" title="挥别2019，迎接变化"></a>挥别2019，迎接变化</h1><p>2020是个很有意思的年份，ABAB的形式朗朗上口，如果要这样严格的算的话，上一个这样的年份是1919年，相隔101年了已经，下一次要到2121，又是101年以后，也就是说，我们一生中，只会经历这样一次的ABAB年份，实在是值得用力去记忆和感受。</p><h2 id="2019，我得到了什么？"><a href="#2019，我得到了什么？" class="headerlink" title="2019，我得到了什么？"></a>2019，我得到了什么？</h2><blockquote><p>Sometimes it lasts in love, sometimes it hurt instead.</p></blockquote><p>一个并不擅长的技能，一段说不清道不明的关系，一个不好不坏的工作机会。还有一个不知道是在成长还是在变老的自己。</p><h3 id="技能"><a href="#技能" class="headerlink" title="技能"></a>技能</h3><blockquote><p>关于项目</p></blockquote><p>在复旦的一年半，说是学习NLP，其实到了最后也只是一直在皮毛的层面晃悠，总是无法深入。</p><p>其实自己也清楚自己的原因在哪边，很难专注一个方向，各种都在好奇，结果就是看两篇论文就算了，其实过两天也就忘了。NLP或者说是深度学习，方向千奇百怪，其实前期找准一个有工业应用前景的方向，比如对话系统，摘要，这种，沿着一篇综述独立开展研究，代码和论文同时开始，再看些相关的比赛开源的代码，应该半年就能入门。这个事情只能说明自己浮躁，浮躁是科研的大忌，所以我不适合读博了，至少目前看来是这样的。</p><p>啊我真是个憨憨。</p><p>最气的是最后Tensorflow和PyTorch居然一个都不熟…我这渣渣工程能力总是会给我巨大的打击…</p><blockquote><p>关于实验室</p></blockquote><p>2018年5月份开始在王老师的EDA实验室呆了一个多月，认识了师兄刘淇，和钟程同学一起搞C++编译器，结果我总是难以入门，那时候我大概连怎么查资料都不会。而且莫名其妙的嫉妒钟程，越来越难受，所以最后还是离开了。</p><p>当时的契机是听刘涛讲算法会更有”钱途“，就心动来了IBICAS，当时实验室还叫做BCRC，事实证明，不管哪一个方向，只要学得好就牛逼，同为IT基本不存在哪个方向明显更有‘’钱途“，抄近路的方式是不可取的…最后我的offer不如一直做IC的同学，很大的教训，仔细想做什么事情都是这个道理吧…<strong>要怀着做到最好的心思去做事情</strong></p><p>最有意思的是最后的offer居然还是C++开发，果然是有一种循环。</p><blockquote><p>关于课程</p></blockquote><p>复旦工硕的课程本来设置的就很多，各种课程各种学分，让人头大…</p><p>研一的时候我总是心大，想着尽快修完学分去实习，所以第一学期选课12门，真的作死，而且累是次要，因为只是为了时间不冲突选了很多不会用到的课程，比如MEMS，贼气的一门，最后绩点也相应的很低。</p><p>毕竟不是计算机专业，大部分的课程都是集成电路，实际上对我的项目和将来的实习工作帮助都不大，现在回忆，还是觉得是在混学分。</p><p>不过实在是对IC兴趣不大，那我喜欢什么呢？大概就是新鲜的能理解能看到的炫酷一点的东西。这样想想我换了软开，换了算法，也不能说是完全看在”钱途“的诱惑上面…hhh</p><h3 id="关系"><a href="#关系" class="headerlink" title="关系"></a>关系</h3><p>2019年是我面临各种关系最多一年，当然主要是男女生的关系发生了巨大的变化。对于这些，当我身处其中的时候，内心忐忑也激动期待，可是到了今天彻底脱离这些，留下的只有空虚和满地的鸡毛…</p><blockquote><p>Chen Jiawen</p></blockquote><p>陈认识在一周cp上，2018.06，在我刚刚来上海，一切都是很好奇很新鲜，我们从网络上认识和开始，到2020.01.01，也从网络上结束，中间分分合合之久，完全写一本让人鸡皮掉落的矫情小说。</p><p>2018.06她同意一起 —&gt; 2018.06她立刻悄悄回家准考研，并没能见面 —&gt; 她在家学习，两个寂寞的人总是互相陪伴 —&gt; 因为突然胖了很多或者其他一些琐碎的小事，我对她生气过很多次 —&gt; 2018.12考研结束，我提出分手 —&gt; 2019.02成绩出来，落榜，她挽回，我们又在一起 —&gt; 定下再陪一年 —&gt; 中间各种矛盾，纠缠，分手不断 —&gt; 2019.10我对文吐露了一些心事，和陈不再想着挽回 —&gt; 2019.11彻底结束 —&gt; 2019.12.31因为汪，她想挽回 —&gt; 2020.01.01,终于轮到她主动提拜拜，好运。</p><p><strong><em>元旦快乐，CJW！祝 2020年，心想事成~</em></strong></p><p>其实我对别人的要求，在这之后再也不会有了，明白了一件事情：喜欢就是喜欢，不喜欢就是不喜欢，喜欢的沾满泥土也不会挑剔。</p><blockquote><p>Wang Lu</p></blockquote><p>抱歉，2019.11-2019.12，一个多月，没能到最后，我真的蠢。</p><p>唯一朋友圈官宣家人也知道的一位，结果越来越累，我想要的感情不是这种被比较衡量之后觉得对方不错就ok的，这段关系里面，我是想护你爱你的，一个比我年龄小的女孩子比我要成熟。进行下去真的太累了，身心俱疲，承认自己并不是之前想的那样对女生无要求。喜欢这种小事，没有触碰到那根弦，就注定没有可能发生了吧。</p><p>虽然我是要说抱歉的一方，但是实际上你也并不喜欢我的吧。</p><p><strong><em>各自安好啦，本命年开心幸福</em></strong></p><blockquote><p>Wen YZ</p></blockquote><p>无</p><p>我理解你了。</p><p>上海对我的意义几乎消失殆尽了，谢谢你没用力拒绝，只是这样依旧不会觉得是很好的方式。我猜我会在离开上海之前再见你最后一次啦，不会打扰你很久的。YZ同学真真是天使。</p><blockquote><p>家人</p></blockquote><p>我姐最最重要，姐姐心眼太少了，对姐姐发了一次火，感到很难受。想一直陪姐姐哈哈哈哈</p><p>奶奶和爸爸，要多关心家人啊，自己早就不是小孩了。</p><blockquote><p>伙伴</p></blockquote><p>不算多，还算好。之后会努力交朋友的，让大家看到我的♥也不是一直很枯燥。以前的现在的各位小伙伴，我们都要努力玩耍，努力学习工作，加油。</p><h3 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h3><blockquote><p>实习</p></blockquote><p>从2019.04-2019.06，大概面了4，5家的算法，都是大厂，被各种蹂躏，我怂了，之后暑假就没再继续了。这是我找工作以来最大的错。</p><p>总结失误：</p><ul><li>不应该上来先面大厂；</li><li>不应该不认真刷题；</li><li>不应该自惭形秽；</li></ul><p>最终2019.09在百姓网实习了4周左右，一边忙着秋招一边实习，真的心都要操碎了。</p><p>谢谢各位。所幸最终的结果是我能接受的。</p><p>因为这样的消极害怕的心理，直接导致自己错过提前批，正式批的笔试都没有通过一家，真的太菜了。还是要努力多学习多刷题，掌握到的编程技能是自己的，会是永远都有用的。</p><blockquote><p>Offer</p></blockquote><p>最后拿到的offer居然是早早就面的第一家，ZTE，这样想想，真的认真面试的只有ZTE和华为两家，华为的依然不是很匹配，被刷掉了。ZTE的说是C++软开，但是偏通信系统协议，依然不能说是理想。而且薪水比较同学差距也有一些。</p><p>谁叫我是菜鸡呢..哭…</p><p>先拿到上海的户口，2020.07入职，2021.04我一定会进入头条或者PDD其中之一的，FLAG立在这了。</p><h2 id="2020，我想要什么？"><a href="#2020，我想要什么？" class="headerlink" title="2020，我想要什么？"></a>2020，我想要什么？</h2><p>2020年鼠年，我刚刚结束了自己的本命年，在实验室做毕设，有几个聊得来的朋友，马上会面临毕业和工作，带着自己并不娴熟的技能，一切好像没那么好，也没那么糟。元旦这天，我想许很多的心愿，希望能在这一年得到回应。</p><p>小孩子都会贪心的。我想要很好的技能，很多的钱，很多的朋友，很充实的时间。</p><p>技能方面：</p><ol><li>C++（工作必须）</li><li>NLP，Python，PyTorch（毕设以及个人发展）</li><li>英语</li></ol><p>生活方面：</p><ol><li>Kindle（想捡起阅读的习惯）</li><li>显示器，桌椅（和我的Surface配合应该算个不错的生产环境）</li><li>羽毛球拍，球鞋</li></ol><p>另外还有，希望顺利毕业，希望工作后也能有生活，希望朋友一直都能毫无芥蒂，希望我喜欢的人也恰好看到了我。</p><p>FLAG:</p><ol><li>新建一个Github账号，把现在的东西慢慢自己写一遍；</li><li>每天commit，少一个commit就减餐一顿；</li><li>27w—&gt;35w，要相信自己努力提高就可以达到。</li></ol><p>总之，2020年，各位一起变得更好吧~ 流年笑掷，未来可期！</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 日记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 年终总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01_PyTorch基础使用</title>
      <link href="/2019/11/07/01-pytorch-ji-chu-shi-yong/"/>
      <url>/2019/11/07/01-pytorch-ji-chu-shi-yong/</url>
      
        <content type="html"><![CDATA[<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torchtorch<span class="token punctuation">.</span>__version__</code></pre><pre><code>'1.3.0'</code></pre><a id="more"></a><h1 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h1><blockquote><p>Tensor的使用：</p></blockquote><ul><li><ol><li>构建</li></ol></li><li><ol start="2"><li>Tensor的基本运算</li></ol></li><li><ol start="3"><li>Tensor与Numpy转换</li></ol></li><li><ol start="4"><li>共享内存的情况</li></ol></li><li><ol start="5"><li>自动微分</li></ol></li></ul><h2 id="1-构建Tensor"><a href="#1-构建Tensor" class="headerlink" title="1. 构建Tensor"></a>1. 构建Tensor</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 构建5×3矩阵，分配空间，不初始化</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 使用特定数据初始化Tensor</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 使用[0, 1]均匀分布随机初始化</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 查看Tensor形状的两种方法</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><pre><code>3</code></pre><h2 id="2-Tensor的基本运算"><a href="#2-Tensor的基本运算" class="headerlink" title="2. Tensor的基本运算"></a>2. Tensor的基本运算</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 加法</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 第一种方式</span>z <span class="token operator">=</span> x <span class="token operator">+</span> y<span class="token comment" spellcheck="true"># 第二种方式</span>torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> out<span class="token operator">=</span>z<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 第三种方式</span>z <span class="token operator">=</span> y<span class="token punctuation">.</span>add<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 第四种方式，修改y值</span>y<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><pre><code>tensor([[1.4718, 0.5690, 1.1329],        [0.3453, 0.8727, 0.7226],        [1.2681, 0.8222, 1.8243],        [1.3840, 0.8803, 1.4788],        [1.5312, 1.0661, 0.9357]])</code></pre><h2 id="3-Tensor与Numpy转换"><a href="#3-Tensor与Numpy转换" class="headerlink" title="3. Tensor与Numpy转换"></a>3. Tensor与Numpy转换</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Tensor的切片与Numpy相似,选出index=1的列</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 新建全为1的Tensor</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Tensor->Numpy</span>b <span class="token operator">=</span> a<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Numpy -> Tensor</span><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np a <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token triple-quoted-string string">'''要注意这里的a和b内存共享，一个改变，另一个会同时跟随改变'''</span><span class="token comment" spellcheck="true"># 获取某个元素值</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 注意：torch.tensor与torch.Tensor有差别，而且，torch.tensor是对原始tensor的拷贝，不再共享同样的内存</span>z_ <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>z <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>z_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>z_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span>z_<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>z_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span></code></pre><pre><code>tensor([5., 3.])tensor([5., 3.])tensor([6., 4.])tensor([5., 3.])/home/xwjia/anaconda3/envs/Torch/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).</code></pre><h2 id="4-共享内存情况"><a href="#4-共享内存情况" class="headerlink" title="4. 共享内存情况"></a>4. 共享内存情况</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 当需要共享内存时，可以：</span>a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 或者</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> a<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 转换为GPU支持的Tensor</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>a <span class="token operator">=</span> a<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>b <span class="token operator">=</span> b<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>c <span class="token operator">=</span> a <span class="token operator">+</span> b</code></pre><h2 id="5-自动微分"><a href="#5-自动微分" class="headerlink" title="5. 自动微分"></a>5. 自动微分</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 为Tensor设定可以求导</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y <span class="token operator">=</span> x<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span>y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>x<span class="token punctuation">.</span>grad<span class="token comment" spellcheck="true"># 第二次求导之前要归零，不然会累加</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>x<span class="token punctuation">.</span>grad</code></pre><pre><code>tensor([[1., 1.],        [1., 1.]])</code></pre><pre class=" language-python"><code class="language-python"></code></pre><h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p>导入<code>torch.nn</code>，封装可以自动求导，只需要在自定义的类继承于<code>nn.Module</code>,类中需要实现<code>__init__</code>和<code>forward</code>方法；</p><p>其中：</p><ul><li><ol><li><code>__init__</code>中存放网络中可以学习的参数；</li></ol></li><li><ol start="2"><li><code>super(Net, self).__init__()</code>等价于父类<code>nn.Module.__init__(self)</code></li></ol></li><li><ol start="3"><li>网络中不学习的参数，比如最大池化或者ReLU，可以放在<code>forward</code>方法中</li></ol></li></ul><h2 id="1-自定义神经网络"><a href="#1-自定义神经网络" class="headerlink" title="1. 自定义神经网络"></a>1. 自定义神经网络</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F <span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">5</span><span class="token operator">*</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#reshape, '-1'表示自适应</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 打印网络可学习的参数</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">)</span>params <span class="token operator">=</span> list<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>len<span class="token punctuation">(</span>params<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> name<span class="token punctuation">,</span> parameters <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> <span class="token string">":"</span><span class="token punctuation">,</span> parameters<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre><code>Net(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))&lt;generator object Module.parameters at 0x7fb59472e7d0&gt;&lt;bound method Module.named_parameters of Net(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))&gt;10conv1.weight : torch.Size([6, 1, 5, 5])conv1.bias : torch.Size([6])conv2.weight : torch.Size([16, 6, 5, 5])conv2.bias : torch.Size([16])fc1.weight : torch.Size([120, 400])fc1.bias : torch.Size([120])fc2.weight : torch.Size([84, 120])fc2.bias : torch.Size([84])fc3.weight : torch.Size([10, 84])fc3.bias : torch.Size([10])</code></pre><pre class=" language-python"><code class="language-python">input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>input<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>out <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span>out<span class="token comment" spellcheck="true"># 所有参数清零</span>net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="需要注意的是，torch-nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用-input-unsqueeze-0-将batch-size设为１。例如-nn-Conv2d-输入必须是4维的，形如"><a href="#需要注意的是，torch-nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用-input-unsqueeze-0-将batch-size设为１。例如-nn-Conv2d-输入必须是4维的，形如" class="headerlink" title="需要注意的是，torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用 input.unsqueeze(0)将batch_size设为１。例如 nn.Conv2d 输入必须是4维的，形如"></a>需要注意的是，torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用 <code>input.unsqueeze(0)</code>将batch_size设为１。例如 <code>nn.Conv2d</code> 输入必须是4维的，形如</h3><p>$$<br>nSamples \times nChannels \times Height \times Width<br>$$</p><h3 id="可将nSample设为1，即"><a href="#可将nSample设为1，即" class="headerlink" title="可将nSample设为1，即"></a>可将nSample设为1，即</h3><p>$$<br>1 \times nChannels \times Height \times Width<br>$$</p><h2 id="2-损失函数"><a href="#2-损失函数" class="headerlink" title="2. 损失函数"></a>2. 损失函数</h2><ul><li><ol><li><code>nn.MSELoss</code>计算均方误差；</li></ol></li><li><ol start="2"><li><code>nn.CrossEntropyLoss</code>计算交叉熵损失；</li></ol></li></ul><pre class=" language-python"><code class="language-python">input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>output <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span>target <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>target<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span></code></pre><pre><code>tensor([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]])tensor(28.4120, grad_fn=&lt;MseLossBackward&gt;)</code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 运行.backward,可以观察调用反向传播之前和之后的grad</span>net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 把net中的所有可学习参数的梯度清零</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"反向传播之前 conv1.bias 的梯度"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>conv1<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"反向传播之后...."</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>conv1<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>grad<span class="token punctuation">)</span></code></pre><pre><code>反向传播之前 conv1.bias 的梯度tensor([0., 0., 0., 0., 0., 0.])反向传播之后....tensor([ 0.0700, -0.0912,  0.0596,  0.0453,  0.0661,  0.0147])</code></pre><h2 id="3-优化器"><a href="#3-优化器" class="headerlink" title="3. 优化器"></a>3. 优化器</h2><p>反向传播计算梯度之后，还需要要优化方法更新网络的权重和参数，比如<code>SGD</code>:</p><p><code>weight = weight - learning_rate * gradient</code></p><p>手动实现如下：</p><pre class=" language-python"><code class="language-python">learning_rate <span class="token operator">=</span> <span class="token number">0.01</span><span class="token keyword">for</span> f <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    f<span class="token punctuation">.</span>data<span class="token punctuation">.</span>sub_<span class="token punctuation">(</span>f<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data <span class="token operator">*</span> learning_rate<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># inplace 减法</span></code></pre><p><code>torch.optim</code>中实现了深度学习中绝大多数的优化方法，例如RMSProp、Adam、SGD等，更便于使用，因此大多数时候并不需要手动写上述代码。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token comment" spellcheck="true"># 新建优化器， 指定需要调整的参数和学习率</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 训练时，梯度先清零：</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 计算loss</span>output <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 反向传播</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 更新参数</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="4-数据加载和预处理"><a href="#4-数据加载和预处理" class="headerlink" title="4. 数据加载和预处理"></a>4. 数据加载和预处理</h2><p><code>torchvision</code>实现了常用的图像数据加载功能，例如Imagenet、CIFAR10、MNIST等，以及常用的数据转换操作，这极大地方便了数据加载，并且代码具有可重用性。</p><h3 id="小试牛刀：CIFAR-10分类"><a href="#小试牛刀：CIFAR-10分类" class="headerlink" title="小试牛刀：CIFAR-10分类"></a>小试牛刀：CIFAR-10分类</h3><p>下面我们来尝试实现对CIFAR-10数据集的分类，步骤如下: </p><ol><li>使用torchvision加载并预处理CIFAR-10数据集</li><li>定义网络</li><li>定义损失函数和优化器</li><li>训练网络并更新网络参数</li><li>测试网络</li></ol><h4 id="CIFAR-10数据加载及预处理"><a href="#CIFAR-10数据加载及预处理" class="headerlink" title="CIFAR-10数据加载及预处理"></a>CIFAR-10数据加载及预处理</h4><p>CIFAR-10<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">^3</a>是一个常用的彩色图片数据集，它有10个类别: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。每张图片都是$3\times32\times32$，也即3-通道彩色图片，分辨率为$32\times32$。</p><h2 id="5-完整CIFAR-10分类练习"><a href="#5-完整CIFAR-10分类练习" class="headerlink" title="5. 完整CIFAR-10分类练习"></a>5. 完整CIFAR-10分类练习</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torchvision  <span class="token keyword">as</span> tv <span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">import</span> ToPILImageshow <span class="token operator">=</span> ToPILImage<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 可以把Tensor转成Image，方便可视化</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#归一化</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 训练集</span>trainset <span class="token operator">=</span> tv<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>    root<span class="token operator">=</span><span class="token string">'/home/xwjia/tmp/data/'</span><span class="token punctuation">,</span>    train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    transform<span class="token operator">=</span>transform<span class="token punctuation">)</span></code></pre><pre><code>0it [00:00, ?it/s]Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/xwjia/tmp/data/cifar-10-python.tar.gz 99%|█████████▉| 169443328/170498071 [00:20&lt;00:00, 11268338.34it/s]Extracting /home/xwjia/tmp/data/cifar-10-python.tar.gz to /home/xwjia/tmp/data/</code></pre><pre class=" language-python"><code class="language-python">trainloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>    trainset<span class="token punctuation">,</span>    batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 测试集</span>testset <span class="token operator">=</span> tv<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>    root<span class="token operator">=</span><span class="token string">'/home/xwjia/tmp/data/'</span><span class="token punctuation">,</span>    train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>testloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>    testset<span class="token punctuation">,</span>    batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>classes <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'plane'</span><span class="token punctuation">,</span> <span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'bird'</span><span class="token punctuation">,</span> <span class="token string">'cat'</span><span class="token punctuation">,</span>           <span class="token string">'deer'</span><span class="token punctuation">,</span> <span class="token string">'dog'</span><span class="token punctuation">,</span> <span class="token string">'frog'</span><span class="token punctuation">,</span> <span class="token string">'horse'</span><span class="token punctuation">,</span> <span class="token string">'ship'</span><span class="token punctuation">,</span> <span class="token string">'truck'</span><span class="token punctuation">)</span></code></pre><pre><code>Files already downloaded and verified</code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 查看某个样本</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token operator">=</span> trainset<span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>classes<span class="token punctuation">[</span>label<span class="token punctuation">]</span><span class="token punctuation">)</span>show<span class="token punctuation">(</span><span class="token punctuation">(</span>data<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre><code>ship</code></pre><p><img src="/2019/11/07/01-pytorch-ji-chu-shi-yong/output_29_1.png" alt="png"></p><p>Dataloader是一个可迭代的对象，它将dataset返回的每一条数据拼接成一个batch，并提供多线程加速优化和数据打乱等操作。当程序对dataset的所有数据遍历完一遍之后，相应的对Dataloader也完成了一次迭代。</p><pre class=" language-python"><code class="language-python">dataiter <span class="token operator">=</span> iter<span class="token punctuation">(</span>trainloader<span class="token punctuation">)</span>images<span class="token punctuation">,</span> labels <span class="token operator">=</span> dataiter<span class="token punctuation">.</span>next<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#返回下一个batch，4张图片和标签</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%11s'</span><span class="token operator">%</span>classes<span class="token punctuation">[</span>labels<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>show<span class="token punctuation">(</span>tv<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span><span class="token punctuation">(</span>images<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">400</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre><code>truck         cat       plane       truck</code></pre><p><img src="/2019/11/07/01-pytorch-ji-chu-shi-yong/output_31_1.png" alt="png"></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 自定义网络结构</span><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>         self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>          self<span class="token punctuation">.</span>fc1   <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">5</span><span class="token operator">*</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>          self<span class="token punctuation">.</span>fc2   <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc3   <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>         x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>         x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>         x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>         x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                <span class="token keyword">return</span> xnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span></code></pre><pre><code>Net(  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))</code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 定义损失函数和优化器</span><span class="token keyword">from</span> torch <span class="token keyword">import</span> optimcriterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 训练网络</span>torch<span class="token punctuation">.</span>set_num_threads<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    running_loss <span class="token operator">=</span> <span class="token number">0.0</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>trainloader<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 输入数据</span>        inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> data        <span class="token comment" spellcheck="true"># 梯度清零</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># forward + backward</span>        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 更新参数</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 打印log信息</span>        <span class="token comment" spellcheck="true"># loss是一个scalar， 需要使用loss.item()获取数值， 不能使用loss[0]</span>        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">2000</span> <span class="token operator">==</span> <span class="token number">1999</span><span class="token punctuation">:</span>   <span class="token comment" spellcheck="true"># 每2000个batch打印一下训练状态</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[%d, %5d] loss: %.3f'</span><span class="token operator">%</span> <span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> running_loss<span class="token operator">/</span><span class="token number">2000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            running_loss <span class="token operator">=</span> <span class="token number">0.0</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Finished Training'</span><span class="token punctuation">)</span></code></pre><pre><code>[1,  2000] loss: 2.163[1,  4000] loss: 1.823[1,  6000] loss: 1.656[1,  8000] loss: 1.586[1, 10000] loss: 1.495[1, 12000] loss: 1.467[2,  2000] loss: 1.405[2,  4000] loss: 1.376[2,  6000] loss: 1.343[2,  8000] loss: 1.322[2, 10000] loss: 1.320[2, 12000] loss: 1.311Finished Training</code></pre><h3 id="接下来看测试集，测试训练的结果"><a href="#接下来看测试集，测试训练的结果" class="headerlink" title="接下来看测试集，测试训练的结果"></a>接下来看测试集，测试训练的结果</h3><pre class=" language-python"><code class="language-python">dataiter <span class="token operator">=</span> iter<span class="token punctuation">(</span>testloader<span class="token punctuation">)</span>images<span class="token punctuation">,</span> labels <span class="token operator">=</span> dataiter<span class="token punctuation">.</span>next<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'实际的label: '</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%08s'</span><span class="token operator">%</span>classes<span class="token punctuation">[</span>labels<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>show<span class="token punctuation">(</span>tv<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>images<span class="token operator">/</span><span class="token number">2</span> <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">400</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 计算图片在每个类别上的分数</span>outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 得分最高的那个类</span>_<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'预测结果： '</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%5s'</span><span class="token operator">%</span>classes<span class="token punctuation">[</span>predicted<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre><code>预测结果：    cat  ship  ship plane</code></pre><pre class=" language-python"><code class="language-python">correct <span class="token operator">=</span> <span class="token number">0</span>   <span class="token comment" spellcheck="true"># 预测正确的图片数</span>total <span class="token operator">=</span> <span class="token number">0</span>     <span class="token comment" spellcheck="true"># 总共的图片数</span><span class="token comment" spellcheck="true"># 由于测试时不需要求导， 所以可以暂时关闭autograd， 提高速度， 节约内存</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> data <span class="token keyword">in</span> testloader<span class="token punctuation">:</span>        images<span class="token punctuation">,</span> labels <span class="token operator">=</span> data        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span>        _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        total <span class="token operator">+=</span> labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        correct <span class="token operator">+=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"10000张测试集中的准确率是： %d %%"</span> <span class="token operator">%</span> <span class="token punctuation">(</span><span class="token number">100</span><span class="token operator">*</span>correct<span class="token operator">/</span>total<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre><code>10000张测试集中的准确率是： 54 %</code></pre><h2 id="在GPU上训练"><a href="#在GPU上训练" class="headerlink" title="在GPU上训练"></a>在GPU上训练</h2><pre class=" language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>images <span class="token operator">=</span> images<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>labels <span class="token operator">=</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>output <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> labels<span class="token punctuation">)</span></code></pre><p>对PyTorch的基础介绍至此结束。总结一下，本节主要包含以下内容。</p><ol><li>Tensor: 类似Numpy数组的数据结构，与Numpy接口类似，可方便地互相转换。</li><li>autograd/: 为tensor提供自动求导功能。</li><li>nn: 专门为神经网络设计的接口，提供了很多有用的功能(神经网络层，损失函数，优化器等)。</li><li>神经网络训练: 以CIFAR-10分类为例演示了神经网络的训练流程，包括数据加载、网络搭建、训练及测试。</li></ol><p>从下一章开始，本书将深入系统地讲解PyTorch的各部分知识。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch框架入门与实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/10/20/hello-world/"/>
      <url>/2019/10/20/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
