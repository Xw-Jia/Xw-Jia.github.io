<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>6.1_Advice_for_Applying_Machine_Learning</title>
      <link href="/2020/02/05/6-1-advice-for-applying-machine-learning/"/>
      <url>/2020/02/05/6-1-advice-for-applying-machine-learning/</url>
      
        <content type="html"><![CDATA[<p><img src="https://upload-images.jianshu.io/upload_images/1194012-0a2226926cdd4e8e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h2 id="一-Evaluating-a-Learning-Algorithm"><a href="#一-Evaluating-a-Learning-Algorithm" class="headerlink" title="一. Evaluating a Learning Algorithm"></a>一. Evaluating a Learning Algorithm</h2><p>想要降低预测误差，即提高预测精度，我们往往会采用这些手段：</p><ul><li>采集更多的样本<br>错误的认为样本越多越好，其实数据多并不是越好。</li><li>降低特征维度<br>降维可能去掉了有用的特征。</li><li>采集更多的特征<br>增加了计算负担，也可能导致过拟合。</li><li>进行高次多项式回归<br>过高的多项式可能造成过拟合。</li><li>调试正规化参数 $\lambda$,增大或者减少 $\lambda$<br>增大或者减少都是凭感觉。</li></ul><p>有这么多种解决办法我们怎么知道是哪一种呢？很多人选择这些方法的标准就是凭感觉随便选择一种，然后花很长的时间最后发现是没用的，走上了不归路。所以下面我们介绍一我们需要一种简单有效的办法，我们将其称为机器学习算法诊断（Machine learning diagnostic）。</p><h3 id="1-Evaluating-a-Hypothesis-评价假设函数"><a href="#1-Evaluating-a-Hypothesis-评价假设函数" class="headerlink" title="1. Evaluating a Hypothesis 评价假设函数"></a>1. Evaluating a Hypothesis 评价假设函数</h3><p>首先我们要评估的是我们的假设函数（Hypothesis）。当我们选择特征值或者参数来使训练集误差最小化，但是我们会遇到过拟合的问题，推广到新的训练集就不再使用了。而且当特征量很多的时候，我们就不能将 $J(\theta)$ 可视化看出其是否随着迭代次数而下降了。所以我们采用以下的方法来评估我们的假设函数：</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/74_1.png" alt=""></p><p>假设有 10 组数据，随机把 70% 做为训练集，剩下的 30% 做为测试集。训练集和测试集尽量保证是随机排列。</p><p>接下来：</p><ol><li>对训练集进行学习得到参数 $\Theta$ ，也就是利用训练集最小化训练误差 $J_{train}(\Theta)$</li><li>计算出测试误差 $J_{test}(\Theta)$，取出之前从训练集中学习得到的参数 $\Theta$ 放在这里，来计算测试误差。</li></ol><p>对于线性回归： $$J_{test}(\theta)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}{(h_\theta(x^{(i)}_{test})-y^{(i)}_{test})^2}$$</p><p>对于逻辑回归：  $$J_{test}(\theta)=-\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}{y^{(i)}_{test}logh_\theta(x^{(i)}_{test})+(1-y^{(i)}_{test})logh_\theta(x^{(i)}_{test})}$$</p><p>逻辑回归不同于线性回归，因为它只有0和1两个值， 所以怎么判断误差如下：</p>$$err(h_\theta(x),y)=\left\{\begin{matrix}1 \;\;\;( if \;\;\; h_\theta(x) \geqslant 0.5 , y=0 \;\;\;or\;\;\; if\;\;\; h_\theta(x) &lt; 0.5 ， y=1 )\\ 0 \;\;\;( otherwise ) \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\end{matrix}\right.$$<p>这里的误差也叫误分类率，也叫 $0/1$ 错分率。</p><p>$( if ;;; h_\theta(x) \geqslant 0.5 , y=0 ;;;or;;; if;;; h_\theta(x) &lt; 0.5 ， y=1 )$</p><p>这种情况下，假设结果更趋向于1，但是实际给出的判断却是0，或者假设结果更趋向于0，实际给出的判断却是1 。</p><p>如果以上情况都没有，那么就没有误差，即为0 ，也代表了假设值能够正确的对样本进行分类。</p><p>测试集的平均测试误差为：</p>$$Test\;Error=\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err(h_{\theta}(x^{(i)}_{test}),y^{(i)}_{test})$$<hr><h3 id="2-Model-Selection-and-Train-Validation-Test-Sets-模型选择和训练集-验证集-测试集"><a href="#2-Model-Selection-and-Train-Validation-Test-Sets-模型选择和训练集-验证集-测试集" class="headerlink" title="2. Model Selection and Train/Validation/Test Sets 模型选择和训练集/验证集/测试集"></a>2. Model Selection and Train/Validation/Test Sets 模型选择和训练集/验证集/测试集</h3><p><img src="https://img.halfrost.com/Blog/ArticleImage/74_2.png" alt=""></p><p>我们这里用 d 表示多项式的个数。我们可以改变多项式次数的多少来选择合适我们的模型。例如上面的 $h_\theta(x)=\theta_0+\theta_1x$ ，这个多项式 $d=1$ 。</p><p>我们可以测试每一个模型得到他们的 $J_{test}(\theta)$ ，判断哪一个模型比较好。</p><p>当选择出了一个多项式 d 能很完美的拟合测试集，接下来就不能再用测试集了，因为 d 本来就已经完美拟合测试集了，再测试就没有意义了，需要换一个测试集。所以更需要关心的对新样本的拟合效果。</p><p>为了解决上述问题，我们把数据分为 3 类，训练集 60% /交叉验证集 20% /测试集 20%。</p><p>通过三个集合，可以算出训练误差：</p><p>$$J_{train}(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^{2}$$</p><p>交叉验证误差：</p>$$J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_{i=1}^{m_{cv}}(h_\theta(x^{(i)}_{cv})-y^{(i)}_{cv})^{2}$$<p>测试误差：</p>$$J_{test}(\theta) = \frac{1}{2m_{test}}\sum_{i=1}^{m}(h_\theta(x^{(i)}_{test})-y^{(i)}_{test})^{2}$$<p>于是我们选择模型不在仅仅通过测试集来选择了，而是：</p><ol><li>利用训练集的数据代入每一个多项式模型。</li><li>用交叉验证集的数据找出最小误差的多项式模型。</li><li>最后在测试集再找出相对较少误差的那个模型。</li></ol><hr><h2 id="二-Bias-vs-Variance"><a href="#二-Bias-vs-Variance" class="headerlink" title="二. Bias vs. Variance"></a>二. Bias vs. Variance</h2><h3 id="1-Diagnosing-Bias-vs-Variance-诊断偏差和方差"><a href="#1-Diagnosing-Bias-vs-Variance-诊断偏差和方差" class="headerlink" title="1. Diagnosing Bias vs. Variance 诊断偏差和方差"></a>1. Diagnosing Bias vs. Variance 诊断偏差和方差</h3><p>在机器学习中，偏差（bias）反映了模型无法描述数据规律，而方差（variance）反映了模型对训练集过度敏感，而丢失了数据规律，高偏差和高方差都会造成新数据到来时，模型给出错误的预测。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/74_3.png" alt=""></p><p>还是以这个图为例，最左边的图是欠拟合，最右边的图是过拟合。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/74_4.png" alt=""></p><p>上图是 训练集、交叉验证集误差随多项式次数 d 的变化规律。横坐标是我们的d，也就是多项式的个数，纵坐标就是我们的代价函数。</p><p>我们先来看一下红色曲线 $J_{training}(\theta)$ ，随着多项式个数的增加，其假设函数是越来越接近要拟合的数据，所以其代价函数会随着多项式个数的增加下降。</p><p>然后绿色的曲线是 $J_{cross-validation}(\theta)$ ,当多项式个数比较少的时候，那当然会出现欠拟合的现象，所以一开始其代价函数 $J_{cross-validation}(\theta)$ 是很大的，随着多项式个数的增加而下降，但是当其多项式个数再继续增加的话，就会出现过拟合的现象， $J_{cross-validation}(\theta)$ 就又会增加。所以 $J_{cross-validation}(\theta)$ 函数是先递减再递增的，在其最低点就是最合适的多项式次数。</p><p>多项式回归中，如果多项式次数较高，则容易造成过拟合，此时训练误差很低，但是对于新数据的泛化能力较差，导致交叉验证集和测试集的误差都很高，此时模型出现了<strong>高方差(过拟合)</strong>：</p>$$\left\{\begin{matrix}J_{train}(\theta) \;\;\;is\;\; low\\ J_{cv}(\theta)&gt;&gt;J_{test}(\theta)\end{matrix}\right.$$<p>过拟合的情况下，训练集误差通常比较小，并且远小于交叉验证误差。</p><p>而当次数较低时，又易出现欠拟合的状况，此时无论是训练集，交叉验证集，还是测试集，都会有很高的误差，此时模型出现了<strong>高偏差(欠拟合)</strong>：</p>$$\left\{\begin{matrix}J_{train}(\theta),J_{cv}(\theta)\;\;\; is \;\; high\\ J_{cv}(\theta) \approx J_{test}(\theta)\end{matrix}\right.$$<p>欠拟合的情况下，训练集误差会很大。</p><p>为什么 $J_{cross-validation}(\theta)$ 会先降后升，而 $J_{training}(\theta)$ 一直下降？</p><p>原因是 $\theta$ 是只针对训练集所训练出来的，当其代入到 $J_{cross-validation}(\theta)$ 后，就会随着多项式的增加数据偏差就会越来越大。</p><hr><h3 id="2-Regularization-and-Bias-Variance-正则化的偏差和方差"><a href="#2-Regularization-and-Bias-Variance-正则化的偏差和方差" class="headerlink" title="2. Regularization and Bias/Variance 正则化的偏差和方差"></a>2. Regularization and Bias/Variance 正则化的偏差和方差</h3><p><img src="https://img.halfrost.com/Blog/ArticleImage/74_5.png" alt=""></p><p>为了防止过拟合的现象，我们加上一个正则化项，但是正则化参数 $\lambda$ 与过拟合又有什么关系呢？</p><p>当 $\lambda$ 很大的时候，就会使得后面的每一个 $\theta_i$ 都被惩罚了，所以只剩下 $\theta_0$ ，那么其假设函数就会变成一条直线，出现欠拟合的现象。</p><p>当 $\lambda$ 很小的话，一个极端例子就是 $\lambda=0$ ，也就是相当于没有加正则化那项，这就会导致过拟合的现象。</p><p>$\lambda$的取值不能过大也不能过小。</p><p>$\lambda$的取值可以在 $\left[0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24\right]$，在这12个不同的模型中针对每一个 $\lambda$ 的值，都去计算出一个最小代价函数,从而得到 $\Theta^{(i)}$</p><p>得到了12个 $\Theta^{(i)}$ 以后，就再用交叉验证集去评价它们。即计算每个 $\Theta$ 在交叉验证集上的平均误差平方和 $J_{cv}(\Theta^{(i)})$</p><p>选择一个交叉验证集误差最小的 $\lambda$ 最能拟合数据的作为正则化参数。</p><p>最后拿这个正则化参数去测试集里面验证 $J_{test}(\Theta^{(i)})$ 预测效果如何。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/74_6.png" alt=""></p><p>随着 $\lambda$ 参数的增大， $J_{train}(\theta)$ 自然也会随之增大，这是因为当 $\lambda=0$ 的时候， $J_{train}(\theta)$ 是没有正则化项的。</p><p>但是对于 $J_{cv}(\theta)$ 来说，它假设函数里面的 $\theta$ 是根据训练集里面拟合出来的，所以在没有加入正则化前， $J_{cv}(\theta)$ 是很大的。但是随着 $\lambda$ 的逐渐增大，也就是随着正则化的效果逐渐体现出来，在交叉验证集里面与测试数据就会越来越拟合，这时候的 $J_{cv}(\theta)$ 自然会慢慢下降。但是当 $\lambda$ 变得足够大的时候，交叉训练集的 $h_\theta(x)$ 就会趋近一条直线，$J_{cv}(\theta)$ 自然会随之上升。</p><hr><h3 id="3-Learning-Curves-学习曲线"><a href="#3-Learning-Curves-学习曲线" class="headerlink" title="3. Learning Curves 学习曲线"></a>3. Learning Curves 学习曲线</h3><p><img src="https://img.halfrost.com/Blog/ArticleImage/74_7.png" alt=""></p><p>假设我们用 $h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2$ 去拟合数据，当数据只有几个的时候，拟合效果那肯定的非常好的，但是，当数据越来越多，我们的假设函数因为多项式太少就不能很好地拟合数据了。所以训练集的误差 $J_{train}(\theta)$ 会随着数据的增多而增大。如上图蓝色的曲线。</p><p>但是对于交叉验证集呢？因为一开始只有几个数据，那么在训练集拟合出来的参数就有很大的可能不适合交叉验证集，所以在数据很小的情况下其误差是很大的，但是随着数据的慢慢增多，虽然个别的数据拟合不上，但是整体的拟合效果那肯定比只有几个数据的时候好了，所以其整体误差是逐步下降的。如上图粉色的曲线。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/74_8.png" alt=""></p><p>当数据存在高偏差也就是欠拟合的时候，即使数据再继续增多也无补于事，所以其误差会趋于一个平衡的位置，而且 $J_{train}(\theta)$ 和 $J_{cv}(\theta)$ 的误差都会很大。</p><p>所以，当数据存在欠拟合的问题，我们选用更多的训练样本是没有办法解决问题的。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/74_9.png" alt=""></p><p>当数据存在高方差也就是过拟合的时候，随着数据的增多，因为过拟合所以在训练集基本能完美拟合其数据，所以训练集的误差虽然会上升，但是其幅度是非常缓慢的，在交叉验证集也一样，所以过拟合的时候其图像如上，在 $J_{train}(\theta)$ 和 $J_{cv}(\theta)$ 之间有一大段空隙。</p><p>所以，当数据存在过拟合的现象，选用更多的样本有利于我们解决这个问题。</p><hr><h3 id="4-Deciding-What-to-Do-Next-Revisited-决定下一步该做什么"><a href="#4-Deciding-What-to-Do-Next-Revisited-决定下一步该做什么" class="headerlink" title="4. Deciding What to Do Next Revisited 决定下一步该做什么"></a>4. Deciding What to Do Next Revisited 决定下一步该做什么</h3><p>总结 ：</p><table><thead><tr><th align="left">手段</th><th align="center">使用场景</th></tr></thead><tbody><tr><td align="left">采集更多的样本</td><td align="center">高方差(过拟合)</td></tr><tr><td align="left">降低特征维度</td><td align="center">高方差(过拟合)</td></tr><tr><td align="left">采集更多的特征</td><td align="center">高偏差(欠拟合)</td></tr><tr><td align="left">进行高次多项式回归</td><td align="center">高偏差(欠拟合)</td></tr><tr><td align="left">降低参数  λ</td><td align="center">高偏差(欠拟合)</td></tr><tr><td align="left">增大参数  λ</td><td align="center">高方差(过拟合)</td></tr></tbody></table><p><img src="https://img.halfrost.com/Blog/ArticleImage/74_10.png" alt=""></p><p>当我们选用一些较小的神经网络，虽然其计算量较少，但是容易出现欠拟合的现象。相反，我们选用一些层数比较多，层的单元比较多的神经网络，容易出现过拟合的现象。我们之前提到越大型的神经网络效果越好，为了防止出现过拟合的现象，我们可以使用正则化的方法来修正。</p><p>使用单个隐藏层是一个很好的默认开始。您可以使用交叉验证集在许多隐藏层上训练您的神经网络。然后您可以选择性能最好的一个。</p><p>模型复杂性影响：</p><ul><li>低阶多项式（低模型复杂度）具有高偏差和低方差。在这种情况下，该模型很难一致</li><li>高阶多项式（高模型复杂度）非常适合训练数据，测试数据极其糟糕。这些对训练数据的偏倚低，但差异很大</li><li>实际上，我们希望选择一个介于两者之间的模型，它可以很好地推广，但也可以很好地适合数据。</li></ul><hr><h2 id="三-Advice-for-Applying-Machine-Learning-测试"><a href="#三-Advice-for-Applying-Machine-Learning-测试" class="headerlink" title="三. Advice for Applying Machine Learning 测试"></a>三. Advice for Applying Machine Learning 测试</h2><h3 id="1-Question-1"><a href="#1-Question-1" class="headerlink" title="1. Question 1"></a>1. Question 1</h3><p>You train a learning algorithm, and find that it has unacceptably high error on the test set. You plot the learning curve, and obtain the figure below. Is the algorithm suffering from high bias, high variance, or neither?</p><p><img src="http://spark-public.s3.amazonaws.com/ml/images/10.1-c.png" alt=""></p><p>A. High variance</p><p>B. Neither</p><p>C. High bias</p><p>解答：A</p><p>高方差的图像</p><h3 id="2-Question-2"><a href="#2-Question-2" class="headerlink" title="2. Question 2"></a>2. Question 2</h3><p>Suppose you have implemented regularized logistic regression to classify what object is in an image (i.e., to do object recognition). However, when you test your hypothesis on a new set of images, you find that it makes unacceptably large errors with its predictions on the new images. However, your hypothesis performs well (has low error) on the training set. Which of the following are promising steps to take? Check all that apply.</p><p>A. Try adding polynomial features.</p><p>B. Get more training examples.</p><p>C. Try using a smaller set of features.</p><p>D. Use fewer training examples.</p><p>解答：B、C</p><p>过拟合可以减少特征量和增加训练样本数量，或者增大正则化参数 $\lambda$ 。</p><h3 id="3-Question-3"><a href="#3-Question-3" class="headerlink" title="3. Question 3"></a>3. Question 3</h3><p>Suppose you have implemented regularized logistic regression to predict what items customers will purchase on a web shopping site. However, when you test your hypothesis on a new set of customers, you find that it makes unacceptably large errors in its predictions. Furthermore, the hypothesis performs poorly on the training set. Which of the following might be promising steps to take? Check all that apply.</p><p>A. Try using a smaller set of features.</p><p>B. Try adding polynomial features.</p><p>C. Try to obtain and use additional features.</p><p>D. Try increasing the regularization parameter $\lambda$.</p><p>解答：B、C</p><p>欠拟合可以增加特征量和假设函数的多项式。</p><h3 id="4-Question-4"><a href="#4-Question-4" class="headerlink" title="4. Question 4"></a>4. Question 4</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. Suppose you are training a regularized linear regression model. The recommended way to choose what value of regularization parameter $\lambda$ to use is to choose the value of $\lambda$ which gives the lowest test set error.</p><p>B. The performance of a learning algorithm on the training set will typically be better than its performance on the test set.</p><p>C. Suppose you are training a regularized linear regression model.The recommended way to choose what value of regularization parameter $\lambda$ to use is to choose the value of $\lambda$ which gives the lowest training set error.</p><p>D. Suppose you are training a regularized linear regression model. The recommended way to choose what value of regularization parameter $\lambda$ to use is to choose the value of $\lambda$ which gives the lowest cross validation error.</p><p>解答：B、D</p><p>在正则化线性回归中，$\lambda$ 选择一个交叉验证集误差最小的  λλ  最能拟合数据的作为正则化参数。</p><h3 id="5-Question-5"><a href="#5-Question-5" class="headerlink" title="5. Question 5"></a>5. Question 5</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. If a learning algorithm is suffering from high variance, adding more training examples is likely to improve the test error.</p><p>B. We always prefer models with high variance (over those with high bias) as they will able to better fit the training set.</p><p>C. If a learning algorithm is suffering from high bias, only adding more training examples may not improve the test error significantly.</p><p>D. When debugging learning algorithms, it is useful to plot a learning curve to understand if there is a high bias or high variance problem.</p><p>解答：A、C、D</p><p>A 过拟合高方差，增加样本数量有用。<br>B 高偏差和高方差的模型都不好。<br>C 增加训练样本对于欠拟合是没用的正确。<br>D 绘制学习曲线有利于帮助我们分析问题正确。  </p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Advice_for_Applying_Machine_Learning.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Advice_for_Applying_Machine_Learning.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5.2_Backpropagation_in_Practice</title>
      <link href="/2020/02/05/5-2-backpropagation-in-practice/"/>
      <url>/2020/02/05/5-2-backpropagation-in-practice/</url>
      
        <content type="html"><![CDATA[<p><img src="https://upload-images.jianshu.io/upload_images/1194012-da4ab697bff80db9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h1 id="5-2-Backpropagation-in-Practice"><a href="#5-2-Backpropagation-in-Practice" class="headerlink" title="5.2 Backpropagation_in_Practice"></a>5.2 Backpropagation_in_Practice</h1><h2 id="一-Backpropagation-in-Practice"><a href="#一-Backpropagation-in-Practice" class="headerlink" title="一. Backpropagation in Practice"></a>一. Backpropagation in Practice</h2><p>为了利用梯度下降的优化算法，需要用到 fminunc 函数。其输入的参数是 $\theta$ ，函数的返回值是代价函数 jVal 和导数值 gradient。然后将返回值传递给高级优化算法 fminunc，然后输出为输入值 @costFunction，以及 $\theta$ 值的初始值。</p><p>其中参数 $\Theta_1,\Theta_2,\Theta_3,\cdots$ 和 $D^{(1)},D^{(2)},D^{(3)},\cdots$ 都为矩阵，那么为了能调用 fminunc 函数，我们要将其变成向量，</p><p>假如我们 $\Theta_1,\Theta_2,\Theta_3$ 参数和 $D^{(1)},D^{(2)},D^{(3)}$ 参数，Theta1 是 $10 * 11$，Theta2 是 $10 * 11$，Theta3 是 $1 * 11$。</p><pre class=" language-c"><code class="language-c"><span class="token operator">%</span> 打包成一个向量thetaVector <span class="token operator">=</span> <span class="token punctuation">[</span> <span class="token function">Theta1</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token function">Theta2</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token function">Theta3</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">]</span>deltaVector <span class="token operator">=</span> <span class="token punctuation">[</span> <span class="token function">D1</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token function">D2</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token function">D3</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">)</span> <span class="token punctuation">]</span><span class="token operator">%</span> 解包还原Theta1 <span class="token operator">=</span> <span class="token function">reshape</span><span class="token punctuation">(</span><span class="token function">thetaVector</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">110</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">)</span>Theta2 <span class="token operator">=</span> <span class="token function">reshape</span><span class="token punctuation">(</span><span class="token function">thetaVector</span><span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">:</span><span class="token number">220</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">)</span>Theta3 <span class="token operator">=</span> <span class="token function">reshape</span><span class="token punctuation">(</span><span class="token function">thetaVector</span><span class="token punctuation">(</span><span class="token number">221</span><span class="token punctuation">:</span><span class="token number">231</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">)</span></code></pre><p>所以<strong>套路</strong>是：</p><ol><li>先将 $\Theta_1,\Theta_2,\Theta_3$ ,这些矩阵展开为一个长向量赋值给 initialTheta，然后作为theta参数的初始设置传入优化函数 fminunc。</li><li>再实现代价函数 costFunction。costFunction 函数将传入参数 thetaVec（就是刚才包含所有 $\Theta$ 参数的向量），然后通过 reshape 函数得到初始的矩阵，这样可以更方便地通过前向传播和反向传播以求得导数 $D^{(1)},D^{(2)},D^{(3)}$ 和代价函数 $F(\Theta)$ 。</li><li>最后按顺序展开得到 gradientVec，让它们保持和之前展开的 $\theta$ 值同样的顺序。以一个向量的形式返回这些导数值。</li></ol><hr><h2 id="二-Gradient-Checking"><a href="#二-Gradient-Checking" class="headerlink" title="二. Gradient Checking"></a>二. Gradient Checking</h2><p>在计算导数的时候，习惯将其等于在该点的导数，在我们使用梯度下降计算导数的时候，虽然可能 $F(\Theta)$ 每次迭代都在下降，但是因为反向传播的复杂性，可能导致我们的代码存在 BUG。有一个办法叫做梯度检验（Gradient Checking），它能减少这种错误的概率（出现这个问题的原因都和反向传播的错误实现有关）。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/73_1.png" alt></p><p>在我们求该点的斜率的时候，我们不直接使用其导数，而是用 $$\frac{d}{d\Theta}F(\Theta)\approx\frac{F(\Theta+\epsilon)-F(\Theta-\epsilon)}{2\epsilon}$$ 代替。通常 $\epsilon$ 取较小的一个数。（其实就是使用导数的定义）</p><p>上面这种算法是双侧差分算法，与之相对的是单侧差分算法</p><p>$$\frac{d}{d\Theta}F(\Theta)\approx\frac{F(\Theta+\epsilon)-F(\Theta)}{\epsilon}$$</p><p>单侧差分和双侧差分相比，双侧差分可以得到更加准确的结果。</p><p>推广一下双侧差分：</p><p>$$\frac{d}{d\Theta_j}J(\Theta)\approx\frac{J(\Theta_1,…,+\Theta_j+\epsilon,…,\Theta_n)-J(\Theta_1,…,+\Theta_j-\epsilon,…,\Theta_n)}{2\epsilon}$$</p><p>对应代码实现如下：</p><pre class=" language-c"><code class="language-c">epsilon <span class="token operator">=</span> <span class="token number">1e-4</span><span class="token punctuation">;</span><span class="token keyword">for</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">:</span>n<span class="token punctuation">,</span>  thetaPlus <span class="token operator">=</span> theta<span class="token punctuation">;</span>  <span class="token function">thetaPlus</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token operator">+</span><span class="token operator">=</span> epsilon<span class="token punctuation">;</span>  thetaMinus <span class="token operator">=</span> theta<span class="token punctuation">;</span>  <span class="token function">thetaMinus</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">=</span> epsilon<span class="token punctuation">;</span>  <span class="token function">gradApprox</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token function">J</span><span class="token punctuation">(</span>thetaPlus<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token function">J</span><span class="token punctuation">(</span>thetaMinus<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>epsilon<span class="token punctuation">)</span>end<span class="token punctuation">;</span></code></pre><p>检查反向传播计算出来的导数 DVec 和 上面程序计算出来的 gradApprox 相比较，如果 $gradApprox \approx DVec$ 代表反向传播的实现是正确的。</p><p>最后在使用算法学习的时候关闭梯度检验。因为梯度检验主要是为了让我们知道我们写的程序算法是否存在错误，而不是用来计算导数的，因为这种方法计算导数相比于之前的会非常慢。</p><p>总结一下：</p><ol><li>通过反向传播来计算 DVec，DVec 是每个矩阵打包展开的形式。</li><li>实现数值上的梯度检测，计算出 gradApprox。</li><li>比较 $gradApprox \approx DVec$ 是否相等或者约等于。</li><li>使用算法学习的时候记得要关闭这个梯度检验，梯度检验只在代码测试阶段进行。</li></ol><hr><h2 id="三-Random-Initialization"><a href="#三-Random-Initialization" class="headerlink" title="三. Random Initialization"></a>三. Random Initialization</h2><p>使用梯度下降算法的时候，需要设置 $\Theta$ 初始值。</p><pre class=" language-c"><code class="language-c">optTheta <span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span>@costFunction<span class="token punctuation">,</span> initialTheta<span class="token punctuation">,</span> options<span class="token punctuation">)</span></code></pre><p>调用 fminunc 函数的时候，initialTheta 如果全部初始化为0，</p><pre class=" language-c"><code class="language-c">initialTheta <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><p>在之前的线性回归和逻辑回归中，使用梯度函数，初始值设置为0是没有问题的，但是到了神经网络里面，如果还这么设置，会出现高度冗余现象。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/73_2.png" alt></p><p>假设我们有这样一个网络，其初始参数都设为0。那么我们会发现其激励 $a_1^{(2)}=a_2^{(2)}$ ,且误差 $\delta_1^{(2)}=\delta_2^{(2)}$ ,且导数 $\frac{d}{d\Theta^{(1)}_{01}}J(\Theta)=\frac{d}{d\Theta^{(1)}_{02}}J(\Theta)$ 。这就导致了在参数更新的情况下，两个参数是一样的。无论怎么重复计算其两边的激励还是一样的。</p><p>上述问题被称为，对称权重问题，也就是所有权重都是一样的。所以随机初始化是解决这个问题的方法。</p><p>我们将初始化权值 $\Theta_{ij}^{(l)}$ 的范围限定在 $[-\Phi ,\Phi ]$ 。</p><p>其代码表示如下：</p><pre class=" language-c"><code class="language-c"><span class="token operator">%</span>If the dimensions of Theta1 is 10x11<span class="token punctuation">,</span> Theta2 is 10x11 and Theta3 is 1x11<span class="token punctuation">.</span>Theta1 <span class="token operator">=</span> <span class="token function">rand</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> INIT_EPSILON<span class="token punctuation">)</span> <span class="token operator">-</span> INIT_EPSILON<span class="token punctuation">;</span>Theta2 <span class="token operator">=</span> <span class="token function">rand</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> INIT_EPSILON<span class="token punctuation">)</span> <span class="token operator">-</span> INIT_EPSILON<span class="token punctuation">;</span>Theta3 <span class="token operator">=</span> <span class="token function">rand</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> INIT_EPSILON<span class="token punctuation">)</span> <span class="token operator">-</span> INIT_EPSILON<span class="token punctuation">;</span></code></pre><p>rand(x，y)是随机函数，它将初始化一个0到1之间的随机实数矩阵。</p><hr><h2 id="四-总结"><a href="#四-总结" class="headerlink" title="四. 总结"></a>四. 总结</h2><p><img src="https://img.halfrost.com/Blog/ArticleImage/73_3.png" alt></p><h3 id="1-准备"><a href="#1-准备" class="headerlink" title="1. 准备"></a>1. 准备</h3><p>首先，我们需要确定神经网络有多少输入单元，有多少隐藏层，每一层隐藏层又有多少个单元，还有多少输出单元。那我们怎么去选择呢？</p><ul><li>输入单元是特征向量 $x^{(i)}$ 的维度</li><li>输出单元是分类的个数</li><li>每个隐藏层的单元数通常是越多越好（必须与计算成本平衡，因为随着更多隐藏单元的增加而增加）</li><li>默认值：1个隐藏层。如果有多个隐藏层，那么建议您在每个隐藏层中都有相同数量的单元。</li></ul><p>输出单元如果是多元分类问题，输出单元需要写成矩阵的形式：</p><p>例如有3个分类， 输出单元应该写成</p> $$\begin{align*}y = \begin{bmatrix} 1\\ 0\\ 0 \\ \end{bmatrix} or\begin{bmatrix} 0\\ 1\\ 0 \\ \end{bmatrix} or\begin{bmatrix} 0\\ 0\\ 1\\ \end{bmatrix}\end{align*}$$<h3 id="2-训练"><a href="#2-训练" class="headerlink" title="2. 训练"></a>2. 训练</h3><p>第一步：随机初始化权重。初始化的值是随机的，值很小，接近于零。</p><p>第二步：执行前向传播算法，对于每一个 $x^{(i)}$ 计算出假设函数 $h_\Theta(x^{(i)})$ 。</p><p>第三步：计算出代价函数 $F(\Theta)$ 。</p><p>第四步：执行反向传播算法，计算出偏导数 $\frac{\partial}{\partial\Theta_{jk}^{(l)}}F(\Theta)$ 。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/73_4.png" alt></p><p>具体操作就是使用一个for循环，先将 $(x^{(1)},y^{(1)})$ 进行一次前向传播和后向传播的操作，然后再对 $(x^{(2)},y^{(2)})$ 进行相同的操作一直到 $(x^{(n)},y^{(n)})$ ，这样就能得到神经网络中每一层中每个单元对应的激励值，和每一层激励的误差 $\delta^{(l)}$ 。</p><p>第五步：利用梯度检查，对比反向传播算法计算得到的偏导数项是否与梯度检验算法计算出的导数项基本相等。<strong>检查完记得删除掉这段检查的代码</strong>。</p><p>第六步：最后我们利用梯度下降算法或者更高级的算法例如 LBFGS、共轭梯度法等，结合之前算出的偏导数项，最小化代价函数 $F(\Theta)$ 算出权值的大小 $\Theta$ 。</p><p>理想情况下，只要满足了 $h_{\Theta}(x^{(i)})\approx y^{(i)}$，就能使我们的代价函数最小。但是，代价函数 $F(\Theta)$ 不是凸的，因此我们最终可以用局部最小值代替全局最小值。</p><hr><h2 id="五-Neural-Networks-Learning-测试"><a href="#五-Neural-Networks-Learning-测试" class="headerlink" title="五. Neural Networks: Learning 测试"></a>五. Neural Networks: Learning 测试</h2><h3 id="1-Question-1"><a href="#1-Question-1" class="headerlink" title="1. Question 1"></a>1. Question 1</h3><p>You are training a three layer neural network and would like to use backpropagation to compute the gradient of the cost function. In the backpropagation algorithm, one of the steps is to update</p>$\Delta^{(2)}_{ij}:=\Delta^{(2)}_{ij}+\delta^{(3)}_{i}*(a^{(2)})_{j}$  <p>for every i,j. Which of the following is a correct vectorization of this step?</p><p>A. $\Delta^{(2)}:=\Delta^{(2)}+(a^{(3)})^T * \delta^{(2)} $<br>B. $\Delta^{(2)}:=\Delta^{(2)}+(a^{(2)})^T * \delta^{(3)} $<br>C. $\Delta^{(2)}:=\Delta^{(2)}+\delta^{(3)}*(a^{(3)})^T $</p><p>D. $\Delta^{(2)}:=\Delta^{(2)}+\delta^{(3)}*(a^{(2)})^T $    </p><p>解答： D</p><h3 id="2-Question-2"><a href="#2-Question-2" class="headerlink" title="2. Question 2"></a>2. Question 2</h3><p>Suppose 𝚃𝚑𝚎𝚝𝚊𝟷 is a 5x3 matrix, and 𝚃𝚑𝚎𝚝𝚊𝟸 is a 4x6 matrix. You set 𝚝𝚑𝚎𝚝𝚊𝚅𝚎𝚌=[𝚃𝚑𝚎𝚝𝚊𝟷(:);𝚃𝚑𝚎𝚝𝚊𝟸(:)]. Which of the following correctly recovers 𝚃𝚑𝚎𝚝𝚊𝟸?</p><p>A. 𝚛𝚎𝚜𝚑𝚊𝚙𝚎(𝚝𝚑𝚎𝚝𝚊𝚅𝚎𝚌(𝟷𝟼:𝟹𝟿),𝟺,𝟼)<br>B. 𝚛𝚎𝚜𝚑𝚊𝚙𝚎(𝚝𝚑𝚎𝚝𝚊𝚅𝚎𝚌(𝟷𝟻:𝟹𝟾),𝟺,𝟼)<br>C. 𝚛𝚎𝚜𝚑𝚊𝚙𝚎(𝚝𝚑𝚎𝚝𝚊𝚅𝚎𝚌(𝟷𝟼:𝟸𝟺),𝟺,𝟼)<br>D. 𝚛𝚎𝚜𝚑𝚊𝚙𝚎(𝚝𝚑𝚎𝚝𝚊𝚅𝚎𝚌(𝟷𝟻:𝟹𝟿),𝟺,𝟼)<br>E. 𝚛𝚎𝚜𝚑𝚊𝚙𝚎(𝚝𝚑𝚎𝚝𝚊𝚅𝚎𝚌(𝟷𝟼:𝟹𝟿),𝟼,𝟺)  </p><p>解答：A</p><h3 id="3-Question-3"><a href="#3-Question-3" class="headerlink" title="3. Question 3"></a>3. Question 3</h3><p>Let $J(\theta)=2\theta^3+2$ . Let $\theta=1$ , and  $\epsilon=0.01$ . Use the formula $\frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}$ to numerically compute an approximation to the derivative at $\theta=1$ . What value do you get? (When $\theta=1$ , the true/exact derivati ve is $\frac{dJ(\theta)}{d\theta}=6$ .)</p><p>A.6<br>B.8<br>C.5.9998<br>D.6.0002  </p><p>解答： D</p><h3 id="4-Question-4"><a href="#4-Question-4" class="headerlink" title="4. Question 4"></a>4. Question 4</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. Gradient checking is useful if we are using gradient descent as our optimization algorithm. However, it serves little purpose if we are using one of the advanced optimization methods (such as in fminunc).  </p><p>B. If our neural network overfits the training set, one reasonable step to take is to increase the regularization parameter λ .  </p><p>C. Using gradient checking can help verify if one’s implementation of backpropagation is bug-free.  </p><p>D. Using a large value of λ cannot hurt the performance of your neural network; the only reason we do not set λ to be too large is to avoid numerical problems.  </p><p>E. For computational efficiency, after we have performed gradient checking to verify that our backpropagation code is correct, we usually disable gradient checking before using backpropagation to train the network.  </p><p>F. Computing the gradient of the cost function in a neural network has the same efficiency when we use backpropagation or when we numerically compute it using the method of gradient checking.  </p><p>解答：B、C、E</p><p>A.梯度检验只是用来检验我们算偏导数的算法是否正确，而不是用来计算的。<br>B.过拟合增大正则化参数 λ 正确。<br>C.梯度检验能检验反向传播算法是否正确。<br>D.正则化参数 λ 太大会导致欠拟合。<br>E.还是在说梯度检验能验证反向传播算法的正确性。<br>F.还是在说梯度检验可以用来在算法里算偏导数。  </p><h3 id="5-Question-5"><a href="#5-Question-5" class="headerlink" title="5. Question 5"></a>5. Question 5</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. Suppose you have a three layer network with parameters  $\Theta^{(1)}$ (controlling the function mapping from the inputs to the hidden units) and  $\Theta^{(2)}$ (controlling the mapping from the hidden units to the outputs). If we set all the elements of  $\Theta^{(1)}$ to be 0, and all the elements of  $\Theta^{(2)}$ to be 1, then this suffices for symmetry breaking, since the neurons are no longer all computing the same function of the input.</p><p>B. If we are training a neural network using gradient descent, one reasonable “debugging” step to make sure it is working is to plot $J(\Theta)$ as a function of the number of iterations, and make sure it is decreasing (or at least non-increasing) after each iteration.</p><p>C. Suppose you are training a neural network using gradient descent. Depending on your random initialization, your algorithm may converge to different local optima (i.e., if you run the algorithm twice with different random initializations, gradient descent may converge to two different solutions).</p><p>D. If we initialize all the parameters of a neural network to ones instead of zeros, this will suffice for the purpose of “symmetry breaking” because the parameters are no longer symmetrically equal to zero.</p><p>E. If we are training a neural network using gradient descent, one reasonable “debugging” step to make sure it is working is to plot $J(\Theta)$ as a function of the number of iterations, and make sure it is decreasing (or at least non-increasing) after each iteration.</p><p>F. Suppose we have a correct implementation of backpropagation, and are training a neural network using gradient descent. Suppose we plot $J(\Theta)$ as a function of the number of iterations, and find that it is increasing rather than decreasing. One possible cause of this is that the learning rate $\alpha$ is too large.</p><p>G. Suppose that the parameter $\Theta^{(1)}$ is a square matrix (meaning the number of rows equals the number of columns). If we replace $\Theta^{(1)}$ with its transpose $(\Theta^{(1)})^T$ , then we have not changed the function that the network is computing.</p><p>H. Suppose we are using gradient descent with learning rate $\alpha$ . For logistic regression and linear regression, $J(\Theta)$ was a convex optimization problem and thus we did not want to choose a learning rate $\alpha$ that is too large. For a neural network however, $J(\Theta)$ may not be convex, and thus choosing a very large value of $\alpha$ can only speed up convergence.</p><p>解答：B、C、F</p><p>A.一层的权重都是一样的数字不能打破对称。<br>B.迭代次数的越多，代价函数 $J(\Theta)$ 下降正确。<br>C.学习速率 $\alpha$ 太大会导致代价函数随着迭代次数的增加也增加正确。<br>D.权重全部为1也不能打破对称的。<br>E.保证 $J(\Theta)$ 随着迭代次数的增加而下降用以验证算法的正确。<br>F.同B。<br>G.矩阵的倒置一般不相等。<br>H.选择大的学习速率 $\alpha$ 会导致 $J(\Theta)$ 不收敛的。  </p><hr><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Backpropagation_in_Practice.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Backpropagation_in_Practice.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5.1_Neural_Networks_Learning</title>
      <link href="/2020/02/05/5-1-neural-networks-learning/"/>
      <url>/2020/02/05/5-1-neural-networks-learning/</url>
      
        <content type="html"><![CDATA[<p><img src="https://upload-images.jianshu.io/upload_images/1194012-03b47f040418215a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h1 id="5-1-Neural-Networks-Learning"><a href="#5-1-Neural-Networks-Learning" class="headerlink" title="5.1_Neural_Networks_Learning"></a>5.1_Neural_Networks_Learning</h1><h2 id="一-Cost-Function-and-Backpropagation"><a href="#一-Cost-Function-and-Backpropagation" class="headerlink" title="一. Cost Function and Backpropagation"></a>一. Cost Function and Backpropagation</h2><h3 id="1-Cost-Function"><a href="#1-Cost-Function" class="headerlink" title="1. Cost Function"></a>1. Cost Function</h3><p><img src="https://img.halfrost.com/Blog/ArticleImage/72_3.png" alt></p><p>假设训练集中有 m 个训练样本，$\begin{Bmatrix} (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \cdots ,(x^{(m)},y^{(m)}) \end{Bmatrix}$，L 表示神经网络的总层数 Layer，用 $S_{l}$ 表示第 L 层的单元数(神经元的数量)，但是不包括第 L 层的偏差单元(常数项)。令 K 为输出层的单元数目，即 最后一层的单元数。</p><p><strong>符号约定</strong>：</p><p>$z_i^{(j)}$ =  第 $j$ 层的第 $i$ 个节点（神经元）的“计算值”<br>$a_i^{(j)}$ = 第 $j$ 层的第 $i$ 个节点（神经元）的“激活值”;</p> $\Theta^{(l)}_{i,j}$  = 映射第 $l$ 层到第 $l+1$ 层的权值矩阵的第 $i$ 行第 $j$ 列的分量<p>$L$ = 神经网络总层数（包括输入层、隐层和输出层）<br>$s_l$ = 第 $l$ 层节点（神经元）个数，不包括偏移量节点。<br>$K$ = 输出节点个数<br>$h_{\theta}(x)_k$ = 第 $k$ 个预测输出结果<br>$x^{(i)}$ = 第 $i$ 个样本特征向量<br>$x^{(i)}_k$ = 第 $i$ 个样本的第 $k$ 个特征值<br>$y^{(i)}$ = 第 $i$ 个样本实际结果向量<br>$y^{(i)}_k$ = 第 $i$ 个样本结果向量的第 $k$ 个分量   </p><p>之前讨论的逻辑回归中代价函数如下：</p>$$\begin{align*}\rm{CostFunction} = \rm{F}({\theta}) &amp;= -\frac{1}{m}\left [ \sum_{i=1}^{m} y^{(i)}logh_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)})) \right ] +\frac{\lambda}{2m} \sum_{j=1}^{n}\theta_{j}^{2}  \\\end{align*}$$<p>扩展到神经网络中：</p>$$\begin{align*}\rm{CostFunction} = \rm{F}({\Theta}) &amp;= -\frac{1}{m}\left [ \sum_{i=1}^{m} \sum_{k=1}^{K} y^{(i)}_{k} log(h_{\Theta}(x^{(i)}))_{k} + (1-y^{(i)}_{k})log(1-(h_{\Theta}(x^{(i)}))_{k}) \right ] +\frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{S_{l}}\sum_{j=1}^{S_{l} +1}(\Theta_{j,i}^{(l)})^{2}  \\h_{\Theta}(x) &amp;\in \mathbb{R}^{K} \;\;\;\;\;\;\;\;\; (h_{\Theta}(x))_{i} = i^{th} \;\;output \\\end{align*}$$<p>$h_{\Theta}(x)$ 是一个 K 维向量，$ i $ 表示选择输出神经网络输出向量中的第 i 个元素。</p><p>神经网络的代价函数相比逻辑回归的代价函数，前一项的求和过程中多了一个 $ \sum_{k=1}^{K} $ ,由于 K 代表了最后一层的单元数，所以这里就是累加了 k 个输出层的代价函数。</p><p>后一项是正则化项，神经网络的正则化项看起来特别复杂，其实就是对 $ (\Theta_{j,i}^{(l)})^{2} $ 项对所有的 i，j，l的值求和。正如在逻辑回归中的一样，这里要除去那些对应于偏差值的项，因为我们不对它们进行求和，即不对 $ (\Theta_{j,0}^{(l)})^{2} ;;;;(i=0) $ 项求和。</p><h3 id="2-Backpropagation-Algorithm-反向传播算法"><a href="#2-Backpropagation-Algorithm-反向传播算法" class="headerlink" title="2. Backpropagation Algorithm 反向传播算法"></a>2. Backpropagation Algorithm 反向传播算法</h3><p>令 $ \delta_{j}^{(l)} $ 表示第 $l$ 层第 $j$ 个结点的误差。</p><p>反向传播从最后一层开始往前推：</p>$$\begin{align*}\delta_{j}^{(L)} &amp;= a_{j}^{(L)} - y_{j} \\&amp;=(h_{\theta}(x))_{j} - y_{j} \\\end{align*}$$<p>往前计算几步：</p>$$\begin{align*}\delta^{(3)} &amp;= (\Theta^{(3)})^{T}\delta^{(4)} . * g^{'}(z^{(3)}) \\\delta^{(2)} &amp;= (\Theta^{(2)})^{T}\delta^{(3)} . * g^{'}(z^{(2)}) \\\end{align*}$$<p>逻辑函数（Sigmoid函数）求导：</p>$$\begin{align*}\sigma(x)'&amp;=\left(\frac{1}{1+e^{-x}}\right)'=\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\frac{-1'-(e^{-x})'}{(1+e^{-x})^2}=\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\frac{e^{-x}}{(1+e^{-x})^2} \newline &amp;=\left(\frac{1}{1+e^{-x}}\right)\left(\frac{e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{+1-1 + e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{1 + e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}}\right)\\&amp;=\sigma(x)(1 - \sigma(x))\\\end{align*}$$<p>可以算出 $g^{‘}(z^{(3)}) = a^{(3)} . * (1-a^{(3)})$ ， $g^{‘}(z^{(2)}) = a^{(2)} . * (1-a^{(2)})$。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/72_4.png" alt></p><p>于是可以给出反向传播的算法步骤：</p><p>首先有一个训练集 $\begin{Bmatrix} (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \cdots ,(x^{(m)},y^{(m)}) \end{Bmatrix}$，初始值对每一个 $(l,i,j)$ 都设置 $\Delta^{(l)}_{i,j} := 0$ ，即初始矩阵是全零矩阵。</p><p>针对 $1-m$ 训练集开始以下步骤的训练：</p><h3 id="1-前向传播"><a href="#1-前向传播" class="headerlink" title="(1) 前向传播"></a>(1) 前向传播</h3><p>设置 $ a^{(1)} := x^{(t)} $，并按照前向传播的方法，计算出每一层的激励 $a^{(l)}$ 。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/72_5.png" alt></p><h3 id="2-计算误差"><a href="#2-计算误差" class="headerlink" title="(2) 计算误差"></a>(2) 计算误差</h3><p>利用 $y^{(t)}$，计算 $\delta^{(L)} = a^{(L)} - y^{t}$</p><p>其中 $L$ 是我们的总层数，$a^{(L)}$ 是最后一层激活单元输出的向量。所以我们最后一层的“误差值”仅仅是我们在最后一层的实际结果和 y 中的正确输出的差异。为了获得最后一层之前的图层的增量值，我们可以使用下面步骤中的方程，让我们从右向左前进：</p><h3 id="3-反向传播"><a href="#3-反向传播" class="headerlink" title="(3) 反向传播"></a>(3) 反向传播</h3><p>通过 $\delta^{(l)} = ((\Theta^{(l)})^{(T)}\delta^{(l+1)}).* a^{(l)} .*(1-a^{(l)})$，计算 $\delta^{(L-1)},\delta^{(L-2)},\cdots,\delta^{(2)}$ 计算出每一层神经节点的误差。</p><h3 id="4-计算偏导数"><a href="#4-计算偏导数" class="headerlink" title="(4) 计算偏导数"></a>(4) 计算偏导数</h3><p>最后利用 $\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_{j}^{(l)}\delta_{i}^{(l+1)}$，或者矢量表示为 $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^{T}$。</p>$$\frac{\partial }{\partial \Theta_{i,j}^{(l)} }F(\Theta) = D_{i,j}^{(l)} := \left\{\begin{matrix}\frac{1}{m} \left( \Delta_{i,j}^{(l)} + \lambda\Theta_{i,j}^{(l)}  \right) \;\;\;\;\;\;\;\; j\neq 0\\ \frac{1}{m}\Delta_{i,j}^{(l)} \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; j = 0\end{matrix}\right.$$<h3 id="5-更新矩阵"><a href="#5-更新矩阵" class="headerlink" title="(5) 更新矩阵"></a>(5) 更新矩阵</h3><p>更新各层的权值矩阵 $\Theta^{(l)}$ ，其中 $\alpha$  为学习率：</p><p>$$\Theta^{(l)} = \Theta^{(l)} - \alpha D^{(l)}$$</p><hr><h2 id="二-推导"><a href="#二-推导" class="headerlink" title="二. 推导"></a>二. 推导</h2><h3 id="1-目标"><a href="#1-目标" class="headerlink" title="1. 目标"></a>1. 目标</h3><p>求 $\min_\Theta F(\Theta)$</p><h3 id="2-思路"><a href="#2-思路" class="headerlink" title="2. 思路"></a>2. 思路</h3><p>类似梯度下降法，给定一个初值后，计算出所有节点的计算值和激活值，然后根据代价函数的变化不断调整参数值（权值），最终不断逼近最优结果，使代价函数值最小。</p><h3 id="3-推导过程"><a href="#3-推导过程" class="headerlink" title="3. 推导过程"></a>3. 推导过程</h3><p>为了实现上述思路，我们必须首先计算代价函数的偏导数：</p><p>$$\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}F(\Theta)$$</p><p>这个偏导并不好求，为了方便推导，我们假设只有一个样本（$m=1$，可忽略代价函数中的外部求和），并舍弃正规化部分，然后分为两种情况来求。</p><h3 id="情况1-隐藏层-→-输出层"><a href="#情况1-隐藏层-→-输出层" class="headerlink" title="情况1 隐藏层 → 输出层"></a>情况1 隐藏层 → 输出层</h3><p>我们知道：</p>$$\begin{align*}h_\Theta(x) &amp;= a^{(j+1)} = g(z^{(j+1)}) \\z^{(j)} &amp;= \Theta^{(j-1)}a^{(j-1)} \\\end{align*}$$<p>另外，输出层即第$L$层。</p><p>所以：</p>$$\dfrac{\partial}{\partial \Theta_{i,j}^{(L)}}F(\Theta)= \dfrac{\partial F(\Theta)}{\partial h_{\Theta}(x)_i} \dfrac{\partial h_{\Theta}(x)_i}{\partial z_i^{(L)}} \dfrac{\partial z_i^{(L)}}{\partial  \Theta_{i,j}^{(L)}}= \dfrac{\partial F(\Theta)}{\partial a_i^{(L)}} \dfrac{\partial a_i^{(L)}}{\partial z_i^{(L)}} \dfrac{\partial z_i^{(L)}}{\partial \Theta_{i,j}^{(L)}}$$<p>其中：</p>$$\begin{align*}\dfrac{\partial F(\Theta)}{\partial a_i^{(L)}} &amp;= \dfrac{a_i^{(L)} - y_i}{(1 - a_i^{(L)})a_i^{(L)}} \\\dfrac{\partial a_i^{(L)}}{\partial z_i^{(L)}} &amp;= \dfrac{\partial g(z_i^{(L)})}{\partial z_i^{(L)}} = \dfrac{e^{z_i^{(L)}}}{(e^{z_i^{(L)}}+1)^2} = a_i^{(L)} (1 - a_i^{(L)}) \\\dfrac{\partial z_i^{(L)}}{\partial \Theta_{i,j}^{(L)}} &amp;= \dfrac{\partial ( \sum_{k=0}^{s_{(L-1)}}\; \Theta_{i,k}^{(L)} a_k^{(L-1)})}{\partial  \Theta_{i,j}^{(L)}} = a_j^{(L-1)} \\\end{align*}$$<p>综上：</p>$$\begin{split}\dfrac{\partial}{\partial \Theta_{i,j}^{(L)}}F(\Theta)=&amp; \dfrac{\partial F(\Theta)}{\partial a_i^{(L)}} \dfrac{\partial a_i^{(L)}}{\partial z_i^{(L)}} \dfrac{\partial z_i^{(L)}}{\partial \Theta_{i,j}^{(L)}} \newline  =&amp; \dfrac{a_i^{(L)} - y_i}{(1 - a_i^{(L)})a_i^{(L)}} a_i^{(L)} (1 - a_i^{(L)}) a_j^{(L-1)} \newline  =&amp; (a_i^{(L)} - y_i)a_j^{(L-1)}\end{split}$$<h3 id="情况2-隐藏层-输入层-→-隐藏层"><a href="#情况2-隐藏层-输入层-→-隐藏层" class="headerlink" title="情况2 隐藏层 / 输入层 → 隐藏层"></a>情况2 隐藏层 / 输入层 → 隐藏层</h3><p>因为 $a^{(1)}=x$，所以可以将输入层和隐藏层同样对待。</p>$$\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}F(\Theta)=\dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} \dfrac{\partial a_i^{(l)}}{\partial z_i^{(l)}} \dfrac{\partial z_i^{(l)}}{\partial \Theta_{i,j}^{(l)}}\ (l = 1, 2, ..., L-1)$$<p>其中后两部分偏导很容易根据前面所得类推出来：</p>$$\begin{align*}\dfrac{\partial a_i^{(l)}}{\partial z_i^{(l)}} &amp;= \dfrac{e^{z_i^{(l)}}}{(e^{z_i^{(l)}}+1)^2} = a_i^{(l)} (1 - a_i^{(l)}) \\\dfrac{\partial z_i^{(l)}}{\partial \Theta_{i,j}^{(l)}} &amp;= a_j^{(l-1)} \\\end{align*}$$<p>第一部分偏导是不好求解的，或者说是没法直接求解的，我们可以得到一个递推式：</p>$$\dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} = \sum_{k=1}^{s_{(l+1)}} \Bigg[\dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \dfrac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}}\Bigg]$$<blockquote><p>因为该层的激活值与下一层各节点都有关，链式法则求导时需一一求导，所以有上式中的求和。</p></blockquote><p>递推式中第一部分是递推项，后两部分同样易求：</p>$$\begin{align*}\dfrac{\partial a_k^{(l+1)}}{\partial z_{k}^{(l+1)}} &amp;= \dfrac{e^{z_{k}^{(l+1)}}}{(e^{z_{k}^{(l+1)}}+1)^2} = a_k^{(l+1)} (1 - a_k^{(l+1)}) \\\dfrac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}} &amp;= \dfrac{\partial ( \sum_{j=0}^{s_l} \Theta_{k,j}^{(l+1)} a_j^{(l)})}{\partial a_i^{(l)}} = \Theta_{k,i}^{(l+1)} \\\end{align*}$$<p>所以，递推式为：</p>$$\begin{split}\dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} =&amp; \sum_{k=1}^{s_{(l+1)}} \Bigg[\dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \dfrac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}}\Bigg] \newline  =&amp; \sum_{k=1}^{s_{(l+1)}} \Bigg[ \dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \Theta_{k,i}^{(l+1)} \Bigg] \newline  =&amp; \sum_{k=1}^{s_{(l+1)}} \Bigg[ \dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} a_k^{(l+1)} (1 - a_k^{(l+1)}) \Theta_{k,i}^{(l+1)} \Bigg]\end{split}$$<p>为了简化表达式，定义第 $l$ 层第 $i$ 个节点的误差：</p>$$\begin{split}\delta^{(l)}_i =&amp; \dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} \dfrac{\partial a_i^{(l)}}{\partial z_i^{(l)}} \newline  =&amp; \dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} a_i^{(l)} (1 - a_i^{(l)})  \newline  =&amp; \sum_{k=1}^{s_{(l+1)}} \Bigg[ \dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \Theta_{k,i}^{(l+1)} \Bigg] a_i^{(l)} (1 - a_i^{(l)}) \newline  =&amp; \sum_{k=1}^{s_{(l+1)}} \Big[\delta^{(l+1)}_k \Theta_{k,i}^{(l+1)} \Big] a_i^{(l)} (1 - a_i^{(l)})\end{split}$$<p>可知，<strong>情况1</strong>的误差为：</p>$$\begin{split}\delta^{(L)}_i =&amp; \dfrac{\partial F(\Theta)}{\partial a_i^{(L)}} \dfrac{\partial a_i^{(L)}}{\partial z_i^{(L)}} \newline  =&amp; \dfrac{a_i^{(L)} - y_i}{(1 - a_i^{(L)})a_i^{(L)}} a_i^{(L)} (1 - a_i^{(L)}) \newline  =&amp; a_i^{(L)} - y_i\end{split}$$<p>最终的代价函数的偏导为：</p>$$\begin{split}\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}F(\Theta) =&amp; \dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} \dfrac{\partial a_i^{(l)}}{\partial z_i^{(l)}} \dfrac{\partial z_i^{(l)}}{\partial \Theta_{i,j}^{(l)}} \newline  =&amp; \delta^{(l)}_i \dfrac{\partial z_i^{(l)}}{\partial \Theta_{i,j}^{(l)}} \newline  =&amp; \delta^{(l)}_i a_j^{(l-1)} \end{split}$$<p>我们发现，引入误差 $\delta^{(l)}_i$ 后，这个公式可以通用于<strong>情况1</strong>和<strong>情况2</strong>。</p><p>可以看出，当前层的代价函数偏导，需要依赖于后一层的计算结果。这也是为什么这个算法的名称叫做“反向传播算法”。</p><h3 id="4-总结算法公式"><a href="#4-总结算法公式" class="headerlink" title="4. 总结算法公式"></a>4. 总结算法公式</h3><ul><li>输出层误差</li></ul><p>$$\delta^{(L)}_i = a_i^{(L)} - y_i$$</p><ul><li><p>隐藏层误差（反向传播计算）</p>$$\delta^{(l)}_i = \sum_{k=1}^{s_{(l+1)}} \Big[\delta^{(l+1)}_k \Theta_{k,i}^{(l+1)} \Big] a_i^{(l)} (1 - a_i^{(l)})$$</li><li><p>代价函数偏导计算（通用）</p></li></ul><p>$$\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}F(\Theta) = \delta^{(l)}_i a_j^{(l-1)}$$</p><hr><h2 id="三-Backpropagation-Algorithm-反向传播算法过程"><a href="#三-Backpropagation-Algorithm-反向传播算法过程" class="headerlink" title="三. Backpropagation Algorithm 反向传播算法过程"></a>三. Backpropagation Algorithm 反向传播算法过程</h2><p><img src="https://img.halfrost.com/Blog/ArticleImage/72_2_.png" alt></p><p>有了上述推导，我们描述一下算法具体的操作流程：</p><ul><li>输入：输入样本数据，初始化权值参数（建议随机生成较小的数）。</li><li>前馈：计算各层（$l=2, 3, …, L$）各节点的计算值（$z^{(l)}=\Theta^{(l-1)}a^{(l-1)}$）和激活值（$a^{(l)}=g(z^{(l)})$）。</li><li>输出层误差：计算输出层误差<script type="math/tex">\delta^{(L)}</script>（公式见前文）。</li><li>反向传播误差：计算各层（$l=L-1, L-2, …, 2$）的误差 $\delta^{(l)}$（公式见前文）。</li><li>输出：得到代价函数的梯度 $\nabla F(\Theta)$（参考前文偏导计算公式）。</li></ul><p>反向传播算法帮助我们得到了代价函数的梯度，我们就可以借助梯度下降法训练神经网络了。</p><p>$$\Theta := \Theta - \alpha  \nabla F(\Theta)$$</p><p>$\alpha $ 为学习速率。</p><hr><h2 id="四-Backpropagation-Algorithm-implementation-算法实现"><a href="#四-Backpropagation-Algorithm-implementation-算法实现" class="headerlink" title="四. Backpropagation Algorithm implementation 算法实现"></a>四. Backpropagation Algorithm implementation 算法实现</h2><p>以3层神经网络（输入层、隐层、输出层各一）为例。</p><ul><li>X 为大小为样本数∗特征数的样本特征矩阵</li><li>Y 为大小为样本数∗输出节点数的样本类别（结果）矩阵</li><li>Theta1 为输入层→隐层的权值矩阵</li><li>Theta2 为隐藏层→输出层的权值矩阵</li><li>m 为样本数</li><li>K 为输出层节点数</li><li>H 为隐藏层节点数</li><li>sigmoid 函数即逻辑函数（S型函数，Sigmoid函数）</li><li>sigmoidGradient 函数即 Sigmoid 函数的导函数</li><li>代码实现中，考虑了正规化，避免出现过拟合问题。</li></ul><h3 id="1-前馈阶段"><a href="#1-前馈阶段" class="headerlink" title="1. 前馈阶段"></a>1. 前馈阶段</h3><p>逐层计算各节点值和激活值。</p><pre class=" language-c"><code class="language-c">a1 <span class="token operator">=</span> X<span class="token punctuation">;</span>z2 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token function">ones</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> a1<span class="token punctuation">]</span> <span class="token operator">*</span> Theta1'<span class="token punctuation">;</span>a2 <span class="token operator">=</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z2<span class="token punctuation">)</span><span class="token punctuation">;</span>z3 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token function">ones</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> a2<span class="token punctuation">]</span> <span class="token operator">*</span> Theta2'<span class="token punctuation">;</span>a3 <span class="token operator">=</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z3<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h3 id="2-代价函数"><a href="#2-代价函数" class="headerlink" title="2. 代价函数"></a>2. 代价函数</h3><p>正规化部分需注意代价函数不惩罚偏移参数，即 $\Theta_{i,0}$（代码表示为 $Theta(:,1)$）。</p><pre class=" language-c"><code class="language-c">F <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> m <span class="token operator">*</span> <span class="token function">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token function">log</span><span class="token punctuation">(</span>a3<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">*</span> Y <span class="token operator">-</span> <span class="token function">log</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token punctuation">.</span><span class="token operator">-</span> a3<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> Y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> # 代价部分 lambda <span class="token operator">/</span> <span class="token number">2</span> <span class="token operator">/</span> m <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token function">Theta1</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span>end<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">^</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token function">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token function">Theta2</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span>end<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">^</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  # 正规化部分，lambda为正规参数，需除去偏移参数Theta<span class="token operator">*</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><h3 id="3-反向传播-1"><a href="#3-反向传播-1" class="headerlink" title="3. 反向传播"></a>3. 反向传播</h3><p>输出层误差和 $\Theta^{(2)}$ 梯度计算，反向传播计算隐层误差和 $\Theta^{(1)}$ 梯度。</p><p>仍需注意正规化时排除偏移参数，另外注意为激活值补一个偏移量 $1$。</p><pre class=" language-c"><code class="language-c">function g <span class="token operator">=</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span>    g <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token punctuation">.</span><span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">+</span> <span class="token function">exp</span><span class="token punctuation">(</span><span class="token operator">-</span>z<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>endfunction g <span class="token operator">=</span> <span class="token function">sigmoidGradient</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span>    g <span class="token operator">=</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>enddelta3 <span class="token operator">=</span> a3 <span class="token operator">-</span> Y<span class="token punctuation">;</span>Theta2_grad <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> m <span class="token operator">*</span> delta3' <span class="token operator">*</span> <span class="token punctuation">[</span><span class="token function">ones</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> a2<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  lambda <span class="token operator">/</span> m <span class="token operator">*</span> <span class="token punctuation">[</span><span class="token function">zeros</span><span class="token punctuation">(</span>K<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">Theta2</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span>end<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">;</span> # 正规化部分delta2 <span class="token operator">=</span> <span class="token punctuation">(</span>delta3 <span class="token operator">*</span> Theta2 <span class="token punctuation">.</span><span class="token operator">*</span> <span class="token function">sigmoidGradient</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token function">ones</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> z2<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>delta2 <span class="token operator">=</span> <span class="token function">delta2</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span>end<span class="token punctuation">)</span><span class="token punctuation">;</span> # 反向计算多一个偏移参数误差，除去Theta1_grad <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> m <span class="token operator">*</span>  delta2' <span class="token operator">*</span> <span class="token punctuation">[</span><span class="token function">ones</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> a1<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  lambda <span class="token operator">/</span> m <span class="token operator">*</span> <span class="token punctuation">[</span><span class="token function">zeros</span><span class="token punctuation">(</span>H<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">Theta1</span><span class="token punctuation">(</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span>end<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">;</span> # 正规化部分</code></pre><hr><p>推荐阅读：</p><p>[Principles of training multi-layer neural network using backpropagation</p><p>](<a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="noopener">http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html</a>)</p><p><a href="https://www.zhihu.com/question/27239198" target="_blank" rel="noopener">如何直观地解释 back propagation 算法？</a></p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Learning.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Learning.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4.1_Neural_Networks_Representation</title>
      <link href="/2020/02/03/4-1-neural-networks-representation/"/>
      <url>/2020/02/03/4-1-neural-networks-representation/</url>
      
        <content type="html"><![CDATA[<p><img src="https://img.halfrost.com/Blog/ArticleImage/71_1.png" alt></p><h1 id="4-1-Neural-Networks-Representation"><a href="#4-1-Neural-Networks-Representation" class="headerlink" title="4.1_Neural_Networks_Representation"></a>4.1_Neural_Networks_Representation</h1><h2 id="一-Motivations"><a href="#一-Motivations" class="headerlink" title="一. Motivations"></a>一. Motivations</h2><p>假如我们用之前的逻辑回归解决以下分类问题：</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_1_1.png" alt></p><p>我们需要构造一个有很多项的非线性的逻辑回归函数。当只有两个特征量的时候，这还算比较简单的，但是假如我们有100个特征量呢？我们只考虑二阶项的话，其二阶项的个数大约是 $\frac{n^2}{2}$ 。假如我们要包含所有的二阶项的话这样看起来不是一个好办法，因为项数实在太多运算量也很多，而且最后结果往往容易造成过拟合。当然我们只是考虑了二阶项，考虑二阶项以上的就更多了。</p><p>当初始特征个数 n 增大时，这些高阶多项式项数将以几何级数上升，特征空间也会随之急剧膨胀 。所以当特征个数 n比较大的时候，用这个方法建立分类器并不是一个好的做法。</p><p>而对于大多数的机器学习问题， n  一般是比较大的。</p><p>对一个拥有很多特征的复杂数据集进行线性回归是代价很高的。比如我们对 50 * 50 像素的黑白图分类，我们就拥有了 2500 个特征。如果我们还要包含所有二次特征，复杂度为 $O(n^{2}/2)$，也就是说一共要有 $2500^{2}/2=3125000$ 个特征。这样计算的代价是高昂的。</p><p>人工神经网络是对具有很多特征的复杂问题进行机器学习的一种方法。</p><hr><h2 id="二-Neural-Networks"><a href="#二-Neural-Networks" class="headerlink" title="二. Neural Networks"></a>二. Neural Networks</h2><p>人工神经网络是对生物神经网络的一种简化的模拟。那么，我们先从生物中的神经元入手，进而了解神经网络的工作方式。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_6.svg" alt></p><p>用一个简单的模型来模拟神经元的工作，我们将神经元模拟成一个逻辑单元：</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_1_2.png" alt></p><p>$x_{1},x_{2},x_{3}$ 可以将其看成输入神经树突，黄色的圆圈则可以看成中心处理器细胞核， $h_\theta(x)$ 则可看成输出神经轴突。因为这里是逻辑单元，所以我们的输出函数为： $h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$ 。一般我们把这称为一个有 s 型函数（逻辑函数）作为激励的人工神经元。</p><p>那么神经网络其实就是这些神经元组合在一起的集合，如下图：</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_2.png" alt></p><p>左边第一层 Layer1 被称为<strong>输入层</strong>。在输入层我们输入我们的特征项 $x_{1},x_{2},x_{3}$ 。</p><p>右边最后一层被称为<strong>输出层</strong>。输出函数为： $h_\Theta(x)$ 。</p><p>中间这层被称为<strong>隐藏层</strong>。</p><p>我们现在要计算当前神经元的值，在当前神经元所在层的前一层，有很多个突触前神经元（当前神经元也是相对于他们的突触后神经元）。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_7.png" alt></p><p>对于前一层的每一个突触前神经元，都有一个输出值，作为当前神经元的输入值，经过轴突传递到当前神经元。当然，如果是第一层神经元，则直接从输入样本数据中接受刺激（对应图中的 $x_{i}$）。</p><p>轴突具有权值（对应图中的权值 weights 列：$w_{ij}$），对每一个输出值加权求和，得到该神经元的输入值。这个加权求和对应图中的transfer function（转移函数），但这个函数的名称并不明确，有人把它称作激活函数（activation function），不同的人可能有不同的叫法，这里仅供参考。</p><p>得到了该神经元的值，就要判定该神经元是否激活兴奋。这对应于图中的activation function（激活函数），但也有人将这个函数叫做输出函数（output function），而把前面说的那一部分叫做激活函数（activation function），并把这两部分合称为转移函数（transfer function）。</p><p>有几种函数可以作为激活函数：</p><ul><li>阶跃函数。这是最简单直接的形式，也是人工神经网络定义时一般采用的。</li><li>逻辑函数。就是S型函数（Sigmoid函数），具有可无限微分的优势。</li><li>斜坡函数</li><li>高斯函数</li><li>…</li></ul><p>可以注意到图中的threshold（阈值），$\theta_{j}$，即激活阈值。也就是说，仅当神经元的值大于这个阈值时，该神经元激活兴奋，输出1；否则无法激活，输出0。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_3.png" alt></p><p>其中隐藏层中的元素我们用 $a_i^{(j)}$ 表示。上标 j 表示的是第几层（有时候我们并不只有简单一层），下标 i 表示第几个，<strong>第j层的第i个节点（神经元）的“激活值”</strong>。</p><p>上面的神经网络可以简单的表示为：</p><p>$$\begin{bmatrix} x_{0}\ x_{1}\ x_{2}\ x_{3} \end{bmatrix} \rightarrow \begin{bmatrix} a_{1}^{(2)}\ a_{2}^{(2)}\ a_{3}^{(2)} \end{bmatrix} \rightarrow h_{\theta}(x) $$</p><p>左边输入层多增加了一个偏置单元(偏置神经元)，$x_{0}$</p><p>用 $\Theta^{(j)}$ 表示特征量前的参数，是一个有权重的矩阵控制着一层参数的大小，<strong>映射第j层到第j+1层的权值矩阵</strong>。</p><p>上述的神经网络可用数学表达，如下：</p>$$\begin{align*}a_{1}^{(2)} &amp;= g(\Theta_{10}^{(1)}x_{0}+\Theta_{11}^{(1)}x_{1}+\Theta_{12}^{(1)}x_{2}+\Theta_{13}^{(1)}x_{3}) \\a_{2}^{(2)} &amp;= g(\Theta_{20}^{(1)}x_{0}+\Theta_{21}^{(1)}x_{1}+\Theta_{22}^{(1)}x_{2}+\Theta_{23}^{(1)}x_{3}) \\a_{3}^{(2)} &amp;= g(\Theta_{30}^{(1)}x_{0}+\Theta_{31}^{(1)}x_{1}+\Theta_{32}^{(1)}x_{2}+\Theta_{33}^{(1)}x_{3}) \\h_{\Theta}(x) &amp;= a_{1}^{(3)} = g(\Theta_{10}^{(2)}a_{0}^{(2)}+\Theta_{11}^{(2)}a_{1}^{(2)}+\Theta_{12}^{(2)}a_{2}^{(2)}+\Theta_{13}^{(2)}a_{3}^{(2)}) \\\end{align*}$$<p>$\Theta$ 矩阵也被称作为模型的权重。这里的 $g(x)$ 都是 sigmoid 激活函数，即 $g(x) = \frac{1}{1+e^{-x}}$</p><p>对上面的神经网络数学表达方式进行向量化推导，令：</p>$$\begin{align*}z_{1}^{(2)} &amp;= \Theta_{10}^{(1)}x_{0}+\Theta_{11}^{(1)}x_{1}+\Theta_{12}^{(1)}x_{2}+\Theta_{13}^{(1)}x_{3} \\z_{2}^{(2)} &amp;= \Theta_{20}^{(1)}x_{0}+\Theta_{21}^{(1)}x_{1}+\Theta_{22}^{(1)}x_{2}+\Theta_{23}^{(1)}x_{3} \\z_{3}^{(2)} &amp;= \Theta_{30}^{(1)}x_{0}+\Theta_{31}^{(1)}x_{1}+\Theta_{32}^{(1)}x_{2}+\Theta_{33}^{(1)}x_{3} \\\vdots \\z_{k}^{(2)} &amp;= \Theta_{k,0}^{(1)}x_{0}+\Theta_{k,1}^{(1)}x_{1}+\Theta_{k,2}^{(1)}x_{2}+\Theta_{k,3}^{(1)}x_{3} \\\end{align*}$$<p>于是可以得到：</p>$$\begin{align*}a_{1}^{(2)} &amp;= g(z_{1}^{(2)}) \\a_{2}^{(2)} &amp;= g(z_{2}^{(2)}) \\a_{3}^{(2)} &amp;= g(z_{3}^{(2)}) \\\end{align*}$$<p>用向量即可表示为：</p>$$x = \begin{bmatrix}x_{0}\\ x_{1}\\ x_{2}\\ x_{3}\end{bmatrix},z^{(2)} = \begin{bmatrix}z_{1}^{(2)}\\ z_{2}^{(2)}\\ z_{3}^{(2)}\\ \end{bmatrix} = \Theta^{(1)}x$${% raw %}统一一下前后两层的输入输出关系，将 $x=a^{(1)}$，即可得到：{% raw %}$$\begin{align*}x &amp;= \begin{bmatrix}x_{0}\\ x_{1}\\ \vdots \\ x_{n}\end{bmatrix},z^{(j)} = \begin{bmatrix}z_{1}^{(j)}\\ z_{2}^{(j)}\\\vdots \\ z_{3}^{(j)}\\ \end{bmatrix}, \\ \Rightarrow  z^{(j)} &amp;=\Theta^{(j-1)}a^{(j-1)}\\\end{align*}$${% endraw %}<p>这里也可以得到一个结论：</p><p>假如一个网络里面在第 j  层有 $s_j$ 个单元，在第 j+1 层有 $s_{j+1}$ 个单元，那么 $\Theta^{(j)}$ 则控制着第 j 层到第 j+1 层的映射矩阵，矩阵的维度是： $s_{j+1} * (s_j + 1)$ 。(例如： j=1 , $s_j=1$， $s_{j+1}$=1 ，也就是说第一层只有一个单元，第二层也只有一个单元，那么 $\Theta^{(1)}$ 矩阵维度就是 1 * 2 ,因为要算上偏置单元)</p><p>因为我们通常有 $a_0^{(j)}=1$ ，所以：</p>{% raw %}$$\begin{align*}a^{(j)}&amp;=g(z^{(j)})\\z^{(j+1)}&amp;=\Theta^{(j)}a^{(j)}\\h_\Theta(x)&amp;=a^{(j+1)}=g(z^{(j+1)})\\\end{align*}$${% endraw %}<p>由这个关系其实可以看出，神经网络跟之前所学的逻辑回归根本区别在于，它是将上一层的输出当做下一层的输入，这个从输入层到隐藏层再到输出层一次计算激励的过程叫做 <strong>forward propagation（前向传播）</strong>。</p><hr><h2 id="三-Applications"><a href="#三-Applications" class="headerlink" title="三. Applications"></a>三. Applications</h2><h3 id="1-逻辑运算"><a href="#1-逻辑运算" class="headerlink" title="1. 逻辑运算"></a>1. 逻辑运算</h3><p>利用神经网络进行 逻辑与运算</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_4.png" alt></p><p>利用神经网络进行 逻辑非运算</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_1_3.png" alt></p><p>但是单一一层无法完成异或运算。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_1_4.png" alt></p><p>异或在几何上的问题其实是将红叉和蓝圈分开，但是我们的输出函数是： $h_\Theta(x)=g(\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2)$ ,这是线性的，那么在图上无论怎么画一条直线，也没有办法将两个不同的训练集分开。既然一条直线不行，那么神经网络增加一层。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_5.png" alt></p><p>如上图，将第二层第一个元素 $a_1^{(2)}$ 作为与运算的结果，第二个元素 $a_2^{(2)}$ 作为或非运算的结果， $a_1^{(2)}$ 和 $a_2^{(2)}$ 再作为输入，进行或运算，作为第三层输出的结果，最后得到的结果与输入的关系正是异或运算的关系。</p><h3 id="2-本质"><a href="#2-本质" class="headerlink" title="2. 本质"></a>2. 本质</h3><p><img src="https://img.halfrost.com/Blog/ArticleImage/71_1_5.png" alt></p><p>神经网络正是这样解决比较复杂的函数，当层数很多的时候，我们有一个相对简单的输入量，通过加以权重和不同的运算送到第二层，而第三层在第二层作为输入的基础上再来进行一些更复杂的运算，一层一层下去解决问题。</p><hr><h2 id="四-Neural-Networks-Representation-测试"><a href="#四-Neural-Networks-Representation-测试" class="headerlink" title="四. Neural Networks: Representation 测试"></a>四. Neural Networks: Representation 测试</h2><h3 id="1-Question-1"><a href="#1-Question-1" class="headerlink" title="1. Question 1"></a>1. Question 1</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. Suppose you have a multi-class classification problem with three classes, trained with a 3 layer network. Let a(3)1=(hΘ(x))1 be the activation of the first output unit, and similarly a(3)2=(hΘ(x))2 and a(3)3=(hΘ(x))3. Then for any input x, it must be the case that a(3)1+a(3)2+a(3)3=1.</p><p>B. The activation values of the hidden units in a neural network, with the sigmoid activation function applied at every layer, are always in the range (0, 1).</p><p>C. A two layer (one input layer, one output layer; no hidden layer) neural network can represent the XOR function.</p><p>D. Any logical function over binary-valued (0 or 1) inputs x1 and x2 can be (approximately) represented using some neural network.</p><p>解答： B、D</p><p>B.S型函数作为判断函数运用到每一层，其范围是[0,1]，正确。<br>D.任何二进制输入的逻辑运算都可以神经网络解决，正确。<br>C.异或不可以用一层神经网络解决。<br>A.不一定，决策函数不是S型函数的话最后结果相加就不是1了。   </p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Representation.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Representation.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.2_Regularization</title>
      <link href="/2020/02/03/3-2-regularization/"/>
      <url>/2020/02/03/3-2-regularization/</url>
      
        <content type="html"><![CDATA[<p><img src="https://upload-images.jianshu.io/upload_images/1194012-756b3fa02ecf03db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h1 id="3-2-Regularization"><a href="#3-2-Regularization" class="headerlink" title="3.2_Regularization"></a>3.2_Regularization</h1><h2 id="一-Solving-the-Problem-of-Overfitting"><a href="#一-Solving-the-Problem-of-Overfitting" class="headerlink" title="一. Solving the Problem of Overfitting"></a>一. Solving the Problem of Overfitting</h2><p>考虑从 $x \in \mathbb{R}$ 预测 y 的问题。下面最左边的图显示了将 $y =\theta_{0}+\theta_{1}x$ 拟合到数据集的结果。我们看到这些数据并不是直线的，所以这个数据并不是很好。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/70_2.png" alt></p><p>相反，如果我们添加了一个额外的特征 x2，并且拟合 $y =\theta_{0}+\theta_{1}x+\theta_{2}x^{2}$，那么我们获得的数据稍微更适合,如上图。</p><p>但是并不是添加的多项式越多越好。但是，添加太多特征也是一个危险：最右边的数字是拟合五阶多项式 $y =\theta_{0}+\theta_{1}x+\theta_{2}x^{2}+\theta_{3}x^{3}+\theta_{4}x^{4}+\theta_{5}x^{5} $ 的结果。我们看到即使拟合曲线完美地传递了数据，我们也不会认为这是一个很好的预测，上图最右边的图就是过度拟合的例子。</p><p>上图最右边的图也称有<strong>高方差</strong>。如果我们拟合一个高阶多项式，有过度的特征，并且这个假设函数能拟合几乎所有的数据，这就面临可能的函数太过于庞大，变量太多的问题。我们没有足够的数据去约束它，来获得一个好的假设函数，这就是过度拟合。</p><p>欠拟合或高偏倚是当我们的假设函数h的形式很难与数据的趋势作图时。它通常是由一个功能太简单或功能太少造成的。另一方面，过度拟合或高度方差是由适合现有数据的假设函数引起的，但不能很好地预测新数据。它通常是由一个复杂的函数造成的，它会产生大量与数据无关的不必要的曲线和角度。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/70_3.png" alt></p><p>这个术语适用于线性和逻辑回归。解决过度配合问题有两个主要选项：</p><h3 id="1-减少特征的数量："><a href="#1-减少特征的数量：" class="headerlink" title="1. 减少特征的数量："></a>1. 减少特征的数量：</h3><ul><li>手动选择要保留的特征，哪些变量更为重要，哪些变量应该保留，哪些应该舍弃。 </li><li>使用模型选择算法（稍后在课程中学习），算法会自动选择哪些特征变量保留，哪些舍弃。</li></ul><p>缺点是舍弃了一些特征以后，也就舍弃了一些问题的关键信息。</p><h3 id="2-正则化"><a href="#2-正则化" class="headerlink" title="2. 正则化"></a>2. 正则化</h3><ul><li>保留所有的特征，但减少参数 $\theta_{j}$ 的大小或者减少量级。 </li><li>当有很多个特征的时候，并且每个特征都会对最终预测值产生影响，正则化可以保证运作良好。</li></ul><p>正则化目的是尽量去简化这个假设模型。因为这些参数都接近0的时候，越简单的模型也被证明越不容易出现过拟合的问题。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/70_4.png" alt></p><p>减少一些数量级的特征，加一些“惩罚”项(为了使代价函数最小，乘以 1000 就是惩罚)。</p><p>代价函数：</p>$$ \rm{CostFunction} = \rm{F}({\theta}) = \frac{1}{2m} \left [ \sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2 + \lambda \sum_{i = 1}^{m} \theta_{j}^{2} \right ]$$<p>$\lambda \sum_{i = 1}^{m} \theta_{j}^{2}$ 是正则化项，它缩小每个参数的值。 $\lambda$ 是正则化参数，$\lambda$ 控制两个不同目标之间的取舍，即更好的去拟合训练集的目标 和 将参数控制的更小的目标，从而保持假设模型的相对简单，避免出现过拟合的情况。</p><p>但是如果选择的 $\lambda $ 太大，可能会过多地消除特征，导致 $\theta$ 都约等于 0 了，最终预测函数变成了水平直线了。这就变成了欠拟合的例子了(偏见性太强，偏差过高)。</p><hr><h2 id="二-Regularized-Linear-Regression-线性回归正则化"><a href="#二-Regularized-Linear-Regression-线性回归正则化" class="headerlink" title="二. Regularized Linear Regression 线性回归正则化"></a>二. Regularized Linear Regression 线性回归正则化</h2><h3 id="1-Gradient-Descent-线性回归梯度下降正则化"><a href="#1-Gradient-Descent-线性回归梯度下降正则化" class="headerlink" title="1. Gradient Descent 线性回归梯度下降正则化"></a>1. Gradient Descent 线性回归梯度下降正则化</h3><p>$$\theta_{0} := \theta_{0} - \alpha \frac{1}{m} \sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}$$</p>$$\theta_{j} := \theta_{j} - \alpha \left [ \left ( \frac{1}{m} \sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}\right ) + \frac{\lambda}{m}\theta_{j} \right ]  \;\;\;\;\;\;\;\;j \in \begin{Bmatrix} 1,2,3,4, \cdots n\end{Bmatrix}$$<p>将上面的式子化简得：</p>$$\theta_{j} := \theta_{j}(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}   \;\;\;\;\;\;\;\;j \in \begin{Bmatrix} 1,2,3,4, \cdots n\end{Bmatrix}$$<p>在上面的式子中 $(1-\alpha \frac{\lambda}{m}) &lt; 1$ 恒小于 1，约等于 1(0.999) 。于是梯度下降的过程就是每次更新都把参数乘以 0.999，缩小一点点，然后再向最小点的方向移动一下。</p><h3 id="2-Normal-Equation-线性回归正规方程正则化"><a href="#2-Normal-Equation-线性回归正规方程正则化" class="headerlink" title="2. Normal Equation 线性回归正规方程正则化"></a>2. Normal Equation 线性回归正规方程正则化</h3><p>之前推导过的正规方程结论：</p><p>$$\Theta = (X^{T}X)^{-1}X^{T}Y$$</p><p>正则化以后，上述式子变成了：</p>$$\Theta = \left( X^{T}X +\lambda \begin{bmatrix}0 &amp;  &amp;  &amp;  &amp; \\  &amp; 1 &amp;  &amp;  &amp; \\  &amp;  &amp; 1 &amp;  &amp; \\  &amp;  &amp;  &amp; \ddots  &amp; \\  &amp;  &amp;  &amp;  &amp; 1\end{bmatrix} \right) ^{-1}X^{T}Y$$<p>在之前的讨论中，有一个<strong>前提条件是 $X^{T}X$ 是非奇异(非退化)矩阵， 即 $ \left | X^{T}X \right | \neq 0 $</strong></p><p>在上述正则化的式子里面，只要 $\lambda &gt; 0$，就不存在不可逆的问题了。因为 $\left( X^{T}X +\lambda \begin{bmatrix}0 &amp;  &amp;  &amp;  &amp; \\  &amp; 1 &amp;  &amp;  &amp; \\  &amp;  &amp; 1 &amp;  &amp; \\  &amp;  &amp;  &amp; \ddots  &amp; \\  &amp;  &amp;  &amp;  &amp; 1\end{bmatrix} \right)$ 这一项一定是可逆的，因为它一定不是奇异矩阵。所以<strong>正则化还能解决不可逆的情况</strong>。</p><hr><h2 id="三-Regularized-Logistic-Regression-逻辑回归正则化"><a href="#三-Regularized-Logistic-Regression-逻辑回归正则化" class="headerlink" title="三. Regularized Logistic Regression 逻辑回归正则化"></a>三. Regularized Logistic Regression 逻辑回归正则化</h2><p><img src="https://img.halfrost.com/Blog/ArticleImage/70_5.png" alt></p><p>之前讨论过的代价函数是：</p>$$\begin{align*}\rm{CostFunction} = \rm{F}({\theta}) &amp;= -\frac{1}{m}\left [ \sum_{i=1}^{m} y^{(i)}logh_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)})) \right ] \\\left( h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}} \right ) \end{align*}$$<p>正则化以后：</p>$$\begin{align*}\rm{CostFunction} = \rm{F}({\theta}) &amp;= -\frac{1}{m}\left [ \sum_{i=1}^{m} y^{(i)}logh_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)})) \right ] +\frac{\lambda}{2m} \sum_{j=1}^{n}\theta_{j}^{2}  \\\end{align*}$$<h3 id="1-Gradient-Descent-逻辑回归梯度下降正则化"><a href="#1-Gradient-Descent-逻辑回归梯度下降正则化" class="headerlink" title="1. Gradient Descent 逻辑回归梯度下降正则化"></a>1. Gradient Descent 逻辑回归梯度下降正则化</h3><p>式子等同于线性回归正则化</p>$$\begin{align*}\theta_{0} &amp;:= \theta_{0} - \alpha \frac{1}{m} \sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)} \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;j = 1 \\\theta_{j} &amp;:= \theta_{j}(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}   \;\;\;\;\;\;\;\;j \in \begin{Bmatrix} 1,2,3,4, \cdots n\end{Bmatrix} \\\end{align*}$$<p>虽然式子和线性回归的一模一样，不过这里的 $h_{\theta}(x)$ 代表的意义不同，逻辑回归中：</p><p>$$h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}}$$</p><hr><h2 id="四-Regularization-测试"><a href="#四-Regularization-测试" class="headerlink" title="四. Regularization 测试"></a>四. Regularization 测试</h2><h3 id="1-Question-1"><a href="#1-Question-1" class="headerlink" title="1. Question 1"></a>1. Question 1</h3><p>You are training a classification model with logistic regression. Which of the following statements are true? Check all that apply.</p><p>A. Introducing regularization to the model always results in equal or better performance on the training set.</p><p>B. Introducing regularization to the model always results in equal or better performance on examples not in the training set.</p><p>C. Adding many new features to the model makes it more likely to overfit the training set.</p><p>D. Adding a new feature to the model always results in equal or better performance on examples not in the training set.</p><p>解答： D  </p><p>A、B 正则化的引入是解决过拟合的问题，而过拟合正是过度拟合数据但无法泛化到新的数据样本中。<br>D 增加一些特征量可能导致拟合在训练集原本没有被拟合到的数据，正确，这就是过拟合。</p><h3 id="2-Question-2"><a href="#2-Question-2" class="headerlink" title="2. Question 2"></a>2. Question 2</h3><p>Suppose you ran logistic regression twice, once with λ=0, and once with λ=1. One of the times, you got</p><p>parameters $\theta = \begin{bmatrix}26.29\\ 65.41\end{bmatrix}$, and the other time you got $\theta = \begin{bmatrix}2.75\\ 1.32\end{bmatrix}$. However, you forgot which value of λ corresponds to which value of θ. Which one do you think corresponds to λ=1?</p><p>A. $\theta = \begin{bmatrix}26.29\\ 65.41\end{bmatrix}$   </p><p>B. $\theta = \begin{bmatrix}2.75\\ 1.32\end{bmatrix}$</p><p>解答： B</p><p>$\lambda = 1$表示正则化以后。正则化其实让我们的 $\theta_j$变小，所以选B。</p><h3 id="3-Question-3"><a href="#3-Question-3" class="headerlink" title="3. Question 3"></a>3. Question 3</h3><p>Which of the following statements about regularization are true? Check all that apply.</p><p>A. Using too large a value of λ can cause your hypothesis to overfit the data; this can be avoided by reducing λ.</p><p>B. Consider a classification problem. Adding regularization may cause your classifier to incorrectly classify some training examples (which it had correctly classified when not using regularization, i.e. when λ=0).</p><p>C. Because logistic regression outputs values 0≤hθ(x)≤1, its range of output values can only be “shrunk” slightly by regularization anyway, so regularization is generally not helpful for it.</p><p>D. Using a very large value of λ cannot hurt the performance of your hypothesis; the only reason we do not set λ to be too large is to avoid numerical problems.</p><p>解答： B</p><p>C 正则化对逻辑回归没用，错误。<br>A、D   $\lambda$过大会导致欠拟合。</p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Regularization.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Regularization.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.1_Logistic_Regression</title>
      <link href="/2020/02/03/3-1-logistic-regression/"/>
      <url>/2020/02/03/3-1-logistic-regression/</url>
      
        <content type="html"><![CDATA[<p><img src="https://img.halfrost.com/Blog/ArticleImage/69_6.png" alt></p><h1 id="3-1-Logistic-Regression"><a href="#3-1-Logistic-Regression" class="headerlink" title="3.1_Logistic_Regression"></a>3.1_Logistic_Regression</h1><h2 id="一-Classification-and-Representation"><a href="#一-Classification-and-Representation" class="headerlink" title="一. Classification and Representation"></a>一. Classification and Representation</h2><p>要尝试分类，一种方法是使用线性回归，并将所有大于0.5的预测值映射为1，将小于0.5的所有预测值映射为0.但是，此方法效果不佳，因为分类实际上不是线性函数。 分类问题就像回归问题一样，除了我们现在想要预测的值只有少数离散值。</p><p><strong>线性回归用来解决分类问题，通常不是一个好主意</strong>。</p><p>我们解决分类问题，忽略y是离散值，并使用我们的旧线性回归算法来尝试预测给定的x。但是，构建这种方法性能很差的示例很容易。直观地说，当知道$y\in \begin{Bmatrix}<br>0,1<br>\end{Bmatrix}$时，$h_{\theta}(x)$ 取大于1或小于0的值也是没有意义的。为了解决这个问题，让我们改变我们的假设 $h_{\theta}(x)$ 的形式以满足 $0\leqslant h_{\theta}(x)\leqslant 1$。这是通过将 $\theta^{T}x$ 插入 Logistic 函数来完成的：</p><p>$$g(x) = \frac{1}{1+e^{-x}}$$</p><p>上式称为 Sigmoid Function 或者 Logistic Function</p><p>令 $h_{\theta}(x) = g(\theta^{T}x)$,$z = \theta^{T}x$,则:</p><p>$$g(x) = \frac{1}{1+e^{-\theta^{T}x}}$$</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/69_8.png" alt></p><p>这里显示的函数$g(x)$将任何实数映射到（0,1）区间，使得它可用于将任意值函数转换为更适合分类的函数。</p><p><strong>决策边界不是训练集的属性，而是假设本身及其参数的属性</strong>。</p><hr><h2 id="二-Logistic-Regression-Model"><a href="#二-Logistic-Regression-Model" class="headerlink" title="二. Logistic Regression Model"></a>二. Logistic Regression Model</h2><h3 id="1-Cost-Function"><a href="#1-Cost-Function" class="headerlink" title="1. Cost Function"></a>1. Cost Function</h3><p>之前定义的代价函数：</p><p>$$ \rm{CostFunction} = \rm{F}({\theta}) = \frac{1}{m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2 $$</p><p>如果将 $$h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}} $$ 代入到上面的式子中，$\rm{CostFunction}$ 的函数图像会是一个非凸函数，会有很多个局部极值点。</p><p>于是我们重新寻找一个新的代价函数：</p><p>$$\rm{CostFunction} = \rm{F}({\theta}) = \frac{1}{m}\sum_{i = 1}^{m} \rm{Cost}(h_{\theta}(x^{(i)}),y^{(i)})$$</p>$$\rm{Cost}(h_{\theta}(x^{(i)}),y^{(i)}) = \left\{\begin{matrix}-log(h_{\theta}(x)) &amp;if \; y = 1 \\ -log(1-h_{\theta}(x)) &amp; if\; y = 0\end{matrix}\right.$$<p>需要说明的一点是，在我们的训练集中，甚至不在训练集中的样本，y 的值总是等于 0 或者 1 。</p><h3 id="2-Simplified-Cost-Function-and-Gradient-Descent"><a href="#2-Simplified-Cost-Function-and-Gradient-Descent" class="headerlink" title="2. Simplified Cost Function and Gradient Descent"></a>2. Simplified Cost Function and Gradient Descent</h3><p>于是进一步我们把代价函数写成一个式子：</p><p>$$\rm{Cost}(h_{\theta}(x),y) = - ylog(h_{\theta}(x)) - (1-y)log(1-h_{\theta}(x))$$</p><p>所以代价函数最终表示为：</p>$$\begin{align*}\rm{CostFunction} = \rm{F}({\theta}) &amp;= \frac{1}{m}\sum_{i = 1}^{m} \rm{Cost}(h_{\theta}(x^{(i)}),y^{(i)})\\&amp;= -\frac{1}{m}\left [ \sum_{i=1}^{m} y^{(i)}logh_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)})) \right ] \\\left( h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}} \right ) \end{align*}$$<p>向量化形式：</p>$$\begin{align*}h &amp;= g(X\theta)\\ \rm{CostFunction} = \rm{F}({\theta}) &amp;= \frac{1}{m} \left ( -\overrightarrow{y}^{T}log(h) - (1-\overrightarrow{y})^{T}log(1-h) \right ) \\ \end{align*}$$<p>为了把式子写成上面这样子是来自于统计学的极大似然估计法得来的，它是统计学里为不同的模型快速寻找参数的方法。它的性质之一是它是凸函数。</p><p>利用梯度下降的方法，得到代价函数的最小值：</p><p>$$ \theta_{j} := \theta_{j} - \alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_{j}$$</p><p>矢量化，即：</p><p>$$ \theta := \theta - \alpha \frac{1}{m} X^{T}(g(X\Theta)-\vec{y})$$</p><p><strong>这里需要注意的是</strong>，</p><p><strong>线性回归中，$h_{\theta}(x) = \theta^{T}x $</strong>,</p><p><strong>而 Logistic 回归中，$h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}}$</strong> 。</p><p>最后，特征缩放的方法同样适用于 Logistic 回归，让其梯度下降收敛更快。</p><hr><h3 id="3-求导过程"><a href="#3-求导过程" class="headerlink" title="3. 求导过程"></a>3. 求导过程</h3><p>逻辑函数</p><p>我们先来看看如何对逻辑函数（Sigmoid函数）求导：</p>$$\begin{align*}\sigma(x)'&amp;=\left(\frac{1}{1+e^{-x}}\right)'=\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\frac{-1'-(e^{-x})'}{(1+e^{-x})^2}=\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\frac{e^{-x}}{(1+e^{-x})^2} \newline &amp;=\left(\frac{1}{1+e^{-x}}\right)\left(\frac{e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{+1-1 + e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{1 + e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}}\right)\\&amp;=\sigma(x)(1 - \sigma(x))\\\end{align*}$$<p>代价函数</p><p>利用上面的结果，借助复合函数求导公式等，可得：</p>$$\begin{align*}\frac{\partial}{\partial \theta_j} J(\theta) &amp;= \frac{\partial}{\partial \theta_j} \frac{-1}{m}\sum_{i=1}^m \left [ y^{(i)} log (h_\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\theta(x^{(i)})) \right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} \frac{\partial}{\partial \theta_j} log (h_\theta(x^{(i)}))   + (1-y^{(i)}) \frac{\partial}{\partial \theta_j} log (1 - h_\theta(x^{(i)}))\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \frac{\partial}{\partial \theta_j} h_\theta(x^{(i)})}{h_\theta(x^{(i)})}   + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - h_\theta(x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \frac{\partial}{\partial \theta_j} \sigma(\theta^T x^{(i)})}{h_\theta(x^{(i)})}   + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - \sigma(\theta^T x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})}   + \frac{- (1-y^{(i)}) \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})}   - \frac{(1-y^{(i)}) h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} (1 - h_\theta(x^{(i)})) x^{(i)}_j - (1-y^{(i)}) h_\theta(x^{(i)}) x^{(i)}_j\right ] \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} (1 - h_\theta(x^{(i)})) - (1-y^{(i)}) h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} - y^{(i)} h_\theta(x^{(i)}) - h_\theta(x^{(i)}) + y^{(i)} h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&amp;= - \frac{1}{m}\sum_{i=1}^m \left [ y^{(i)} - h_\theta(x^{(i)}) \right ] x^{(i)}_j  \newline&amp;= \frac{1}{m}\sum_{i=1}^m \left [ h_\theta(x^{(i)}) - y^{(i)} \right ] x^{(i)}_j\end{align*}$$<p>向量化形式：</p><p>$$\nabla J(\theta) = \frac{1}{m} \cdot  X^T \cdot \left(g\left(X\cdot\theta\right) - \vec{y}\right)$$</p><hr><h3 id="4-Advanced-Optimization"><a href="#4-Advanced-Optimization" class="headerlink" title="4. Advanced Optimization"></a>4. Advanced Optimization</h3><p>除去梯度下降法，还有其他的优化方法，</p><p>conjugate gradient 共轭梯度法，<br>BFGS，<br>L_BFGS，  </p><p>上述3种算法在高等数值计算中。它们相比梯度下降，有以下一些优点：</p><ol><li>不需要手动选择学习率 $\alpha$ 。可以理解为它们有一个智能的内循环(线搜索算法)，它会自动尝试不同的学习速率 $\alpha$，并自动选择一个最好的学习速率 $\alpha$ 。甚至还可以为每次迭代选择不同的学习速率，那么就不需要自己选择了。</li><li>收敛速度远远快于梯度下降。</li></ol><p>缺点就是相比梯度下降而言，更加复杂。</p><p>举个例子：</p><pre class=" language-c"><code class="language-c">function <span class="token punctuation">[</span>jVal<span class="token punctuation">,</span> gradient<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">costFunction</span><span class="token punctuation">(</span>theta<span class="token punctuation">)</span>jVal <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token function">theta</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token operator">^</span><span class="token number">2</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token function">theta</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token operator">^</span><span class="token number">2</span><span class="token punctuation">;</span>gradient <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">gradient</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span><span class="token punctuation">(</span><span class="token function">theta</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">gradient</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span><span class="token punctuation">(</span><span class="token function">theta</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>调用高级函数 fminunc:</p><pre class=" language-c"><code class="language-c">options <span class="token operator">=</span> <span class="token function">optimset</span><span class="token punctuation">(</span><span class="token string">'GrabObj'</span><span class="token punctuation">,</span><span class="token string">'on'</span><span class="token punctuation">,</span><span class="token string">'MaxIter'</span><span class="token punctuation">,</span><span class="token string">'100'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>initialTheta <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">[</span>optTheta<span class="token punctuation">,</span> functionVal<span class="token punctuation">,</span> exitFlag<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span>@costFunction<span class="token punctuation">,</span> initialTheta<span class="token punctuation">,</span> options<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>最终结果:</p><pre class=" language-c"><code class="language-c">optTheta <span class="token operator">=</span>     <span class="token number">5.0000</span>    <span class="token number">5.0000</span>functionVal <span class="token operator">=</span> <span class="token number">1.5777e-030</span>exitFlag <span class="token operator">=</span> <span class="token number">1</span></code></pre><p>optTheta 表示的是最终求得的结果，functionVal 表示的是代价函数的最小值，这里是 0，是我们期望的。exitFlag 表示的是最终是否收敛，1表示收敛。</p><p>这里的 fminunc 是试图找到一个多变量函数的最小值，从一个估计的初试值开始，这通常被认为是无约束非线性优化问题。</p><p>另外一些例子：</p><pre class=" language-c"><code class="language-c">x <span class="token operator">=</span><span class="token function">fminunc</span><span class="token punctuation">(</span>fun<span class="token punctuation">,</span>x0<span class="token punctuation">)</span>                                   <span class="token operator">%</span>试图从x0附近开始找到函数的局部最小值，x0可以是标量，向量或矩阵x <span class="token operator">=</span><span class="token function">fminunc</span><span class="token punctuation">(</span>fun<span class="token punctuation">,</span>x0<span class="token punctuation">,</span>options<span class="token punctuation">)</span>                           <span class="token operator">%</span>根据结构体options中的设置来找到最小值，可用optimset来设置optionsx <span class="token operator">=</span><span class="token function">fminunc</span><span class="token punctuation">(</span>problem<span class="token punctuation">)</span>                                  <span class="token operator">%</span>为problem找到最小值<span class="token punctuation">,</span>而problem是在Input Arguments中定义的结构体<span class="token punctuation">[</span>x<span class="token punctuation">,</span>fval<span class="token punctuation">]</span><span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>                               <span class="token operator">%</span>返回目标函数fun在解x处的函数值<span class="token punctuation">[</span>x<span class="token punctuation">,</span>fval<span class="token punctuation">,</span>exitflag<span class="token punctuation">]</span><span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>                      <span class="token operator">%</span>返回一个描述退出条件的值exitflag<span class="token punctuation">[</span>x<span class="token punctuation">,</span>fval<span class="token punctuation">,</span>exitflag<span class="token punctuation">,</span>output<span class="token punctuation">]</span><span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>               <span class="token operator">%</span>返回一个叫output的结构体，它包含着优化的信息<span class="token punctuation">[</span>x<span class="token punctuation">,</span>fval<span class="token punctuation">,</span>exitflag<span class="token punctuation">,</span>output<span class="token punctuation">,</span>grad<span class="token punctuation">]</span><span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>          <span class="token operator">%</span>返回函数在解x处的梯度的值，存储在grad中<span class="token punctuation">[</span>x<span class="token punctuation">,</span>fval<span class="token punctuation">,</span>exitflag<span class="token punctuation">,</span>output<span class="token punctuation">,</span>grad<span class="token punctuation">,</span>hessian<span class="token punctuation">]</span><span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>  <span class="token operator">%</span>返回函数在解x处的Hessian矩阵的值，存储在hessian中</code></pre><hr><h2 id="三-Multiclass-Classification"><a href="#三-Multiclass-Classification" class="headerlink" title="三. Multiclass Classification"></a>三. Multiclass Classification</h2><p>这一章节我们来讨论一下如何利用逻辑回归来解决多类别分类问题。介绍一个一对多的分类算法。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/69_7.png" alt></p><p>现在，当我们有两个以上的类别时，我们将处理数据的分类。我们将扩展我们的定义，使得y = {0,1 … n}，而不是y = {0,1}。 由于y = {0,1 … n}，我们将问题分成n + 1（+1，因为索引从0开始）二元分类问题;在每一个中，我们都预测’y’是我们其中一个类的成员的概率。</p><p>最终在 n + 1 个分类器中分别输入 x ，然后取这 n + 1 个分类器概率的最大值,即是对应 $y=i$ 的概率值。</p><hr><h2 id="四-Logistic-Regression-测试"><a href="#四-Logistic-Regression-测试" class="headerlink" title="四. Logistic Regression 测试"></a>四. Logistic Regression 测试</h2><h3 id="1-Question-1"><a href="#1-Question-1" class="headerlink" title="1. Question 1"></a>1. Question 1</h3><p>Suppose that you have trained a logistic regression classifier, and it outputs on a new example x a prediction hθ(x) = 0.7. This means (check all that apply):</p><p>A. Our estimate for P(y=1|x;θ) is 0.7.<br>B. Our estimate for P(y=0|x;θ) is 0.3.<br>C. Our estimate for P(y=1|x;θ) is 0.3.<br>D. Our estimate for P(y=0|x;θ) is 0.7.  </p><p>解答： A、B  </p><h3 id="2-Question-2"><a href="#2-Question-2" class="headerlink" title="2. Question 2"></a>2. Question 2</h3><p>Suppose you have the following training set, and fit a logistic regression classifier hθ(x)=g(θ0+θ1x1+θ2x2).</p><p>Which of the following are true? Check all that apply.</p><p>A. Adding polynomial features (e.g., instead using hθ(x)=g(θ0+θ1x1+θ2x2+θ3x21+θ4x1x2+θ5x22) ) could increase how well we can fit the training data.  </p><p>B. At the optimal value of θ (e.g., found by fminunc), we will have J(θ)≥0.  </p><p>C. Adding polynomial features (e.g., instead using hθ(x)=g(θ0+θ1x1+θ2x2+θ3x21+θ4x1x2+θ5x22) ) would increase J(θ) because we are now summing over more terms.  </p><p>D. If we train gradient descent for enough iterations, for some examples x(i) in the training set it is possible to obtain hθ(x(i))&gt;1.  </p><p>解答： A、B</p><h3 id="3-Question-3"><a href="#3-Question-3" class="headerlink" title="3. Question 3"></a>3. Question 3</h3><p>For logistic regression, the gradient is given by ∂∂θjJ(θ)=1m∑mi=1(hθ(x(i))−y(i))x(i)j. Which of these is a correct gradient descent update for logistic regression with a learning rate of α? Check all that apply.</p><p>A. θj:=θj−α1m∑mi=1(hθ(x(i))−y(i))x(i) (simultaneously update for all j).  </p><p>B. θj:=θj−α1m∑mi=1(hθ(x(i))−y(i))x(i)j (simultaneously update for all j).  </p><p>C. θj:=θj−α1m∑mi=1(11+e−θTx(i)−y(i))x(i)j (simultaneously update for all j).  </p><p>D. θ:=θ−α1m∑mi=1(θTx−y(i))x(i).  </p><p>解答： A、D</p><p>线性回归与逻辑回归的区别</p><h3 id="4-Question-4"><a href="#4-Question-4" class="headerlink" title="4. Question 4"></a>4. Question 4</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. The cost function J(θ) for logistic regression trained with m≥1 examples is always greater than or equal to zero.  </p><p>B. Linear regression always works well for classification if you classify by using a threshold on the prediction made by linear regression.  </p><p>C. The one-vs-all technique allows you to use logistic regression for problems in which each y(i) comes from a fixed, discrete set of values.   </p><p>D. For logistic regression, sometimes gradient descent will converge to a local minimum (and fail to find the global minimum). This is the reason we prefer more advanced optimization algorithms such as fminunc (conjugate gradient/BFGS/L-BFGS/etc).  </p><p>解答： A、C</p><p>D由于使用代价函数为线性回归代价函数，会有很多局部最优值</p><h3 id="5-Question-5"><a href="#5-Question-5" class="headerlink" title="5. Question 5"></a>5. Question 5</h3><p>Suppose you train a logistic classifier hθ(x)=g(θ0+θ1x1+θ2x2). Suppose θ0=6,θ1=0,θ2=−1. Which of the following figures represents the decision boundary found by your classifier?</p><p>解答： C</p><p>6-x2&gt;=0 即X2&lt;6时为1</p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Logistic_Regression.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Logistic_Regression.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.2_Computing_Parameters_Analytically</title>
      <link href="/2020/01/27/2-2-computing-parameters-analytically/"/>
      <url>/2020/01/27/2-2-computing-parameters-analytically/</url>
      
        <content type="html"><![CDATA[<h1 id="2-2-Computing-Parameters-Analytically"><a href="#2-2-Computing-Parameters-Analytically" class="headerlink" title="2.2_Computing_Parameters_Analytically"></a>2.2_Computing_Parameters_Analytically</h1><h2 id="一-Normal-Equation"><a href="#一-Normal-Equation" class="headerlink" title="一. Normal Equation"></a>一. Normal Equation</h2><h3 id="1-正规方程"><a href="#1-正规方程" class="headerlink" title="1. 正规方程"></a>1. 正规方程</h3><p>正规方程法相对梯度下降法，它可以一步找到最小值。而且它也不需要进行特征值的缩放。</p><p>样本集是 $ m * n $ 的矩阵，每行样本表示为 $ \vec{x^{(i)}} $ ,第 i 行第 n 列分别表示为 $ x^{(i)}_{0} , x^{(i)}_{1} , x^{(i)}_{2} , x^{(i)}_{3} \cdots x^{(i)}_{n} $, m 行向量分别表示为 $ \vec{x^{(1)}} , \vec{x^{(2)}} , \vec{x^{(3)}} , \cdots \vec{x^{(m)}} $</p><p>令 </p>$$ \vec{x^{(i)}} = \begin{bmatrix} x^{(i)}_{0}\\ x^{(i)}_{1}\\ \vdots \\ x^{(i)}_{n}\\ \end{bmatrix} $$<p>$ \vec{x^{(i)}} $ 是这样一个 $(n+1)*1$ 维向量。每行都对应着 i 行 0-n 个变量。</p><p>再构造几个矩阵：</p>$$ X = \begin{bmatrix} (\vec{x^{(1)}})^{T}\\  \vdots \\  (\vec{x^{(m)}})^{T} \end{bmatrix} \;\;\;\;\Theta = \begin{bmatrix} \theta_{0}\\ \theta_{1}\\ \vdots \\ \theta_{n}\\ \end{bmatrix} \;\;\;\;Y = \begin{bmatrix} y^{(1)}\\ y^{(2)}\\ \vdots \\ y^{(m)}\\ \end{bmatrix} $$<p>X 是一个 $ m * (n+1)$ 的矩阵，$ \Theta $ 是一个 $ (n+1) * 1$ 的向量，Y 是一个 $ m * 1$的矩阵。</p><p>对比之前代价函数中，$$ \rm{CostFunction} = \rm{F}({\theta_{0}},{\theta_{1}}) = \frac{1}{2m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2 $$</p>  $$\begin{align*}X \cdot \Theta - Y = \begin{bmatrix}(\vec{x^{(1)}})^{T}\\ \vdots \\ (\vec{x^{(m)}})^{T}\end{bmatrix} \cdot \begin{bmatrix} \theta_{0}\\ \theta_{1}\\ \vdots \\ \theta_{n}\\ \end{bmatrix} - \begin{bmatrix} y^{(1)}\\ y^{(2)}\\ \vdots \\ y^{(m)}\\ \end{bmatrix} = \begin{bmatrix} h_{\theta}(x^{(1)})-y^{(1)}\\ h_{\theta}(x^{(2)})-y^{(2)}\\ \vdots \\ h_{\theta}(x^{(m)})-y^{(m)}\\ \end{bmatrix}\end{align*}$$<p>代入到之前代价函数中，</p>$$\begin{align*}\rm{CostFunction} = \rm{F}({\theta_{0}},{\theta_{1}}) &amp;= \frac{1}{2m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2\\&amp; = \frac{1}{2m} (X \cdot \Theta - Y)^{T}(X \cdot \Theta - Y)\\\end{align*}$$<hr><h3 id="2-矩阵的微分和矩阵的迹"><a href="#2-矩阵的微分和矩阵的迹" class="headerlink" title="2. 矩阵的微分和矩阵的迹"></a>2. 矩阵的微分和矩阵的迹</h3><p>接下来在进行推导之前，需要引入矩阵迹的概念，因为迹是求解一阶矩阵微分的工具。</p><p>矩阵迹的定义是 </p><p>$$ \rm{tr} A =  \sum_{i=1}^{n}A_{ii}$$ </p><p>简单的说就是左上角到右下角对角线上元素的和。</p><p>接下来有几个性质在下面推导过程中需要用到：</p><ol><li><p>$ \rm{tr};a = a $ ， a 是标量 ( $ a \in \mathbb{R} $)  </p></li><li><p>$ \rm{tr};AB = \rm{tr};BA $ 更近一步 $ \rm{tr};ABC = \rm{tr};CAB = \rm{tr};BCA $<br>证明：假设 A 是 $n * m$ 矩阵， B 是 $m * n$ 矩阵，则有<br>$$ \rm{tr};AB = \sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji} = \sum_{j=1}^{n} \sum_{i=1}^{m}B_{ji}A_{ij}= \rm{tr};BA $$<br>同理：$$ \rm{tr};ABC = \rm{tr};(AB)C = \rm{tr};C(AB) = \rm{tr};CAB$$<br>$$ \rm{tr};ABC = \rm{tr};A(BC) = \rm{tr};(BC)A = \rm{tr};BCA$$<br>连起来，即 $$ \rm{tr};ABC = \rm{tr};CAB = \rm{tr};BCA $$</p></li><li><p>$ \triangledown_{A}\rm{tr};AB = \triangledown_{A}\rm{tr};BA = B^{T}$<br>证明：按照矩阵梯度的定义：</p>   $$\triangledown_{X}f(X) = \begin{bmatrix}   \frac{\partial f(X) }{\partial x_{11}} &amp; \cdots &amp; \frac{\partial f(X) }{\partial x_{1n}}\\    \vdots &amp; \ddots  &amp; \vdots \\    \frac{\partial f(X) }{\partial x_{m1}} &amp; \cdots &amp; \frac{\partial f(X) }{\partial x_{mn}}   \end{bmatrix} = \frac{\partial f(X) }{\partial X}$$   <p>假设 A 是 $n * m$ 矩阵， B 是 $m * n$ 矩阵，则有</p>   $$\begin{align*}\triangledown_{A}\rm{tr}\;AB &amp;= \triangledown_{A} \sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji}  = \frac{\partial}{\partial A}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji})\\ &amp; = \begin{bmatrix}   \frac{\partial}{\partial A_{11}}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji}) &amp; \cdots &amp; \frac{\partial}{\partial A_{1m}}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji})\\    \vdots &amp; \ddots  &amp; \vdots \\    \frac{\partial}{\partial A_{n1}}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji}) &amp; \cdots &amp; \frac{\partial}{\partial A_{nm}}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji})   \end{bmatrix} \\ &amp; = \begin{bmatrix}   B_{11} &amp; \cdots &amp; B_{m1} \\    \vdots &amp; \ddots  &amp; \vdots \\    B_{1n} &amp; \cdots &amp; B_{mn}   \end{bmatrix} = B^{T}\\ \end{align*}$$   </li></ol>      $$\begin{align*}\triangledown_{A}\rm{tr}\;BA &amp;= \triangledown_{A} \sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji}  = \frac{\partial}{\partial A}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji})\\ &amp; = \begin{bmatrix}   \frac{\partial}{\partial A_{11}}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji}) &amp; \cdots &amp; \frac{\partial}{\partial A_{1m}}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji})\\    \vdots &amp; \ddots  &amp; \vdots \\    \frac{\partial}{\partial A_{n1}}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji}) &amp; \cdots &amp; \frac{\partial}{\partial A_{nm}}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji})   \end{bmatrix} \\ &amp; = \begin{bmatrix}   B_{11} &amp; \cdots &amp; B_{m1} \\    \vdots &amp; \ddots  &amp; \vdots \\    B_{1n} &amp; \cdots &amp; B_{mn}   \end{bmatrix} = B^{T}\\ \end{align*}$$   <p>   所以有 $ \triangledown_{A}\rm{tr};AB = \triangledown_{A}\rm{tr};BA = B^{T}$</p><ol start="4"><li><p>$\triangledown_{A^{T}}a = (\triangledown_{A}a)^{T};;;; (a \in \mathbb{R})$<br>证明：假设 A 是 $n * m$ 矩阵</p>   $$\begin{align*}\triangledown_{A^{T}}a &amp; = \begin{bmatrix}   \frac{\partial}{\partial A_{11}}a &amp; \cdots &amp; \frac{\partial}{\partial A_{1n}}a\\    \vdots &amp; \ddots  &amp; \vdots \\    \frac{\partial}{\partial A_{m1}}a &amp; \cdots &amp; \frac{\partial}{\partial A_{mn}}a   \end{bmatrix}  = (\begin{bmatrix}   \frac{\partial}{\partial A_{11}}a &amp; \cdots &amp; \frac{\partial}{\partial A_{1m}}a\\    \vdots &amp; \ddots  &amp; \vdots \\    \frac{\partial}{\partial A_{n1}}a &amp; \cdots &amp; \frac{\partial}{\partial A_{nm}}a   \end{bmatrix})^{T} \\ &amp; = (\triangledown_{A}a)^{T}\\ \end{align*}$$   </li><li><p>$\mathrm{d}(\rm{tr};A) = \rm{tr}(\mathrm{d}A)$<br>证明：<br>$$\mathrm{d}(\rm{tr};A) = \mathrm{d}(\sum_{i=1}^{n}a_{ii}) = \sum_{i=1}^{n}\mathrm{d}a_{ii} = \rm{tr}(\mathrm{d}A)$$<br>矩阵的迹的微分等于矩阵的微分的迹。</p></li><li><p>$\triangledown_{A}\rm{tr};ABA^{T}C = CAB + C^{T}AB^{T}$<br>证明：<br>根据实标量函数梯度的乘法法则：<br>若 f(A)、g(A)、h(A) 分别是矩阵 A 的实标量函数，则有   $$\begin{align*}\frac{\partial f(A)g(A)}{\partial A} &amp;= g(A)\frac{\partial f(A)}{\partial A} + f(A)\frac{\partial g(A)}{\partial A}\\ \frac{\partial f(A)g(A)h(A)}{\partial A} &amp;= g(A)h(A)\frac{\partial f(A)}{\partial A} + f(A)h(A)\frac{\partial g(A)}{\partial A}+ f(A)g(A)\frac{\partial h(A)}{\partial A}\\ \end{align*}$$<br>令 $f(A) = AB,g(A) = A^{T}C$，由性质5，矩阵的迹的微分等于矩阵的微分的迹，那么则有：</p>   $$\begin{align*} \triangledown_{A}\rm{tr}\;ABA^{T}C &amp; = \rm{tr}(\triangledown_{A}ABA^{T}C) = \rm{tr}(\triangledown_{A}f(A)g(A)) = \rm{tr}\triangledown_{A_{1}}(A_{1}BA^{T}C) + \rm{tr}\triangledown_{A_{2}}(ABA_{2}^{T}C)  \\ &amp; = (BA^{T}C)^{T} + \rm{tr}\triangledown_{A_{2}}(ABA_{2}^{T}C) = C^{T}AB^{T} + \triangledown_{A_{2}}\rm{tr}(ABA_{2}^{T}C)\\ &amp; = C^{T}AB^{T} + \triangledown_{A_{2}}\rm{tr}(A_{2}^{T}CAB) = C^{T}AB^{T} + (\triangledown_{{A_{2}}^{T}}\;\rm{tr}\;A_{2}^{T}CAB)^{T} \\ &amp; = C^{T}AB^{T} + ((CAB)^{T})^{T}  \\ &amp; = C^{T}AB^{T} + CAB  \\ \end{align*}$$   </li></ol><hr><h3 id="3-推导"><a href="#3-推导" class="headerlink" title="3. 推导"></a>3. 推导</h3><p>回到之前的代价函数中：<br>$$<br>\rm{CostFunction} = \rm{F}({\theta_{0}},{\theta_{1}}) = \frac{1}{2m} (X \cdot \Theta - Y)^{T}(X \cdot \Theta - Y)<br>$$<br>求导：</p>$$\begin{align*}\triangledown_{\theta}\rm{F}(\theta) &amp; = \frac{1}{2m} \triangledown_{\theta}(X \cdot \Theta - Y)^{T}(X \cdot \Theta - Y) = \frac{1}{2m}\triangledown_{\theta}(\Theta^{T}X^{T}-Y^{T})(X\Theta-Y)\\&amp; = \frac{1}{2m}\triangledown_{\theta}(\Theta^{T}X^{T}X\Theta-Y^{T}X\Theta-\Theta^{T}X^{T}Y+Y^{T}Y) \\ \end{align*}$$<p>上式中，对 $\Theta $矩阵求导，$ Y^{T}Y $ 与 $\Theta $ 无关，所以这一项为 0 。 $Y^{T}X\Theta$ 是标量，由性质4可以知道，$Y^{T}X\Theta = (Y^{T}X\Theta)^{T} = \Theta^{T}X^{T}Y$，因为 $\Theta^{T}X^{T}X\Theta , Y^{T}X\Theta $都是标量，所以它们的也等于它们的迹，（处理矩阵微分的问题常常引入矩阵的迹），于是有</p>$$\begin{align*}\triangledown_{\theta}\rm{F}(\theta) &amp; = \frac{1}{2m}\triangledown_{\theta}(\Theta^{T}X^{T}X\Theta-2Y^{T}X\Theta) \\ &amp; = \frac{1}{2m}\triangledown_{\theta}\rm{tr}\;(\Theta^{T}X^{T}X\Theta-2Y^{T}X\Theta) \\ &amp; = \frac{1}{2m}\triangledown_{\theta}\rm{tr}\;(\Theta\Theta^{T}X^{T}X-2Y^{T}X\Theta) \\ &amp; = \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr}\;\Theta\Theta^{T}X^{T}X -\triangledown_{\theta}\rm{tr}\;Y^{T}X\Theta) \\ &amp; = \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr}\;\Theta\Theta^{T}X^{T}X -(Y^{T}X)^{T}) = \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr}\;\Theta\Theta^{T}X^{T}X -X^{T}Y)\\ \end{align*}$$<p>上面第三步用的性质2矩阵迹的交换律，第五步用的性质3。</p><p>为了能进一步化简矩阵的微分，我们在矩阵的迹上面乘以一个单位矩阵，不影响结果。于是：</p>$$\begin{align*}\triangledown_{\theta}\rm{F}(\theta) &amp; = \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr}\;\Theta\Theta^{T}X^{T}X -X^{T}Y) \\ &amp;= \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr}\;\Theta I \Theta^{T}X^{T}X -X^{T}Y) \end{align*}$$<p>利用性质6 展开上面的式子，令 $ A = \Theta , B = I , C = X^{T}X $。</p>$$\begin{align*}\triangledown_{\theta}\rm{F}(\theta) &amp;= \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr}\;\Theta I \Theta^{T}X^{T}X -X^{T}Y) \\ &amp; = \frac{1}{m}(\frac{1}{2}(X^{T}X\Theta I + (X^{T}X)^{T}\Theta I^{T}) -X^{T}Y) \\ &amp; = \frac{1}{m}(\frac{1}{2}(X^{T}X\Theta I + (X^{T}X)^{T}\Theta I^{T}) -X^{T}Y) \\ &amp; = \frac{1}{m}(\frac{1}{2}(X^{T}X\Theta + X^{T}X\Theta) -X^{T}Y)  = \frac{1}{m}(X^{T}X\Theta -X^{T}Y) \\ \end{align*}$$<p>令 $\triangledown_{\theta}\rm{F}(\theta) = 0$，即 $X^{T}X\Theta -X^{T}Y = 0$, 于是 $ X^{T}X\Theta = X^{T}Y $ ，这里假设 $ X^{T}X$ 这个矩阵是可逆的，等号两边同时左乘$ X^{T}X$的逆矩阵，得到 $\Theta = (X^{T}X)^{-1}X^{T}Y$</p><p>最终结果也就推导出来了，$$\Theta = (X^{T}X)^{-1}X^{T}Y$$</p><p>但是这里有一个<strong>前提条件是 $X^{T}X$ 是非奇异(非退化)矩阵， 即 $ \left | X^{T}X \right | \neq 0 $</strong></p><hr><h3 id="4-梯度下降和正规方程法比较："><a href="#4-梯度下降和正规方程法比较：" class="headerlink" title="4. 梯度下降和正规方程法比较："></a>4. 梯度下降和正规方程法比较：</h3><p>优点：<br>梯度下降在超大数据集面前也能运行的很良好。<br>正规方程在超大数据集合面前性能会变得很差，因为需要计算 $(x^{T}x)^{-1}$,时间复杂度在 $O(n^{3})$ 这个级别。  </p><p>缺点：<br>梯度下降需要合理的选择学习速率 $\alpha$ , 需要很多次迭代的操作去选择合理的 $\alpha$，寻找最小值的时候也需要迭代很多次才能收敛。<br>正规方程的优势相比而言，不需要选择学习速率 $\alpha$，也不需要多次的迭代或者画图检测是否收敛。</p><hr><h2 id="二-Normal-Equation-Noninvertibility"><a href="#二-Normal-Equation-Noninvertibility" class="headerlink" title="二. Normal Equation Noninvertibility"></a>二. Normal Equation Noninvertibility</h2><p>上一章谈到了如何利用正规方程法求解 $\Theta $,但是在线性代数中存在这样一个问题，如果是奇异(退化)矩阵，是不存在逆矩阵的。也就是说用上面正规方程的公式是不一定能求解出正确结果的。</p><p>在 Octave 软件中，存在2个求解逆矩阵的函数，一个是 pinv 和 inv。pinv (pseudo-inverse)求解的是<strong>伪逆矩阵</strong>，inv 求解的是逆矩阵，所以用 pinv 求解问题，就算是 $ X^{T}X $ 不存在逆矩阵，也一样可以得到最后的结果。</p><p>导致$ X^{T}X $ 不存在逆矩阵有2种情况：</p><ol><li>多余的特征。特征之间呈倍数关系，线性依赖。</li><li>过多的特征。当 $ m \leqslant n $ 的时候，会导致过多的特征。解决办法是删除一些特征，或者进行正则化。</li></ol><p>所以解决$ X^{T}X $ 不存在逆矩阵的办法也就是对应上面2种情况：</p><ol><li>删掉多余的特征，线性相关的，倍数关系的。直到没有多余的特征</li><li>再删除一些不影响结果的特征，或者进行正则化。</li></ol><hr><h2 id="三-Linear-Regression-with-Multiple-Variables-测试"><a href="#三-Linear-Regression-with-Multiple-Variables-测试" class="headerlink" title="三. Linear Regression with Multiple Variables 测试"></a>三. Linear Regression with Multiple Variables 测试</h2><h3 id="1-Question-1"><a href="#1-Question-1" class="headerlink" title="1. Question 1"></a>1. Question 1</h3><p>Suppose m=4 students have taken some class, and the class had a midterm exam and a final exam. You have collected a dataset of their scores on the two exams, which is as follows:</p><p>midterm exam    (midterm exam)2    final exam<br>89    7921    96<br>72    5184    74<br>94    8836    87<br>69    4761    78<br>You’d like to use polynomial regression to predict a student’s final exam score from their midterm exam score. Concretely, suppose you want to fit a model of the form hθ(x)=θ0+θ1x1+θ2x2, where x1 is the midterm score and x2 is (midterm score)2. Further, you plan to use both feature scaling (dividing by the “max-min”, or range, of a feature) and mean normalization.</p><p>What is the normalized feature x(2)2? (Hint: midterm = 72, final = 74 is training example 2.) Please round off your answer to two decimal places and enter in the text box below.</p><p>解答：<br>标准化 $$x = \frac{x_{2}^{2}-\frac{(7921+5184+8836+4761)}{4}}{\max - \min } = \frac{5184 - 6675.5}{8836-4761} = -0.37$$</p><h3 id="2-Question-2"><a href="#2-Question-2" class="headerlink" title="2. Question 2"></a>2. Question 2</h3><p>You run gradient descent for 15 iterations</p><p>with α=0.3 and compute J(θ) after each</p><p>iteration. You find that the value of J(θ) increases over</p><p>time. Based on this, which of the following conclusions seems</p><p>most plausible?</p><p>A. Rather than use the current value of α, it’d be more promising to try a smaller value of α (say α=0.1).</p><p>B. α=0.3 is an effective choice of learning rate.</p><p>C. Rather than use the current value of α, it’d be more promising to try a larger value of α (say α=1.0).</p><p>解答：  A </p><p>下降太快所以a下降速率过大，a越大下降越快，a小下降慢，在本题中，代价函数快速收敛到最小值，代表此时a最合适。</p><h3 id="3-Question-3"><a href="#3-Question-3" class="headerlink" title="3. Question 3"></a>3. Question 3</h3><p>Suppose you have m=28 training examples with n=4 features (excluding the additional all-ones feature for the intercept term, which you should add). The normal equation is θ=(XTX)−1XTy. For the given values of m and n, what are the dimensions of θ, X, and y in this equation?</p><p>A. X is 28×4, y is 28×1, θ is 4×4</p><p>B. X is 28×5, y is 28×5, θ is 5×5</p><p>C. X is 28×5, y is 28×1, θ is 5×1</p><p>D. X is 28×4, y is 28×1, θ is4×1</p><p>解答：  C </p><p>这里需要注意的是，题目中说了额外添加一列全部为1的，所以列数是5 。</p><h3 id="4-Question-4"><a href="#4-Question-4" class="headerlink" title="4. Question 4"></a>4. Question 4</h3><p>Suppose you have a dataset with m=50 examples and n=15 features for each example. You want to use multivariate linear regression to fit the parameters θ to our data. Should you prefer gradient descent or the normal equation?</p><p>A. Gradient descent, since it will always converge to the optimal θ.</p><p>B. Gradient descent, since (XTX)−1 will be very slow to compute in the normal equation.</p><p>C. The normal equation, since it provides an efficient way to directly find the solution.</p><p>D. The normal equation, since gradient descent might be unable to find the optimal θ.</p><p>解答：  C </p><p>数据量少，选择正规方程法更加高效</p><h3 id="5-Question-5"><a href="#5-Question-5" class="headerlink" title="5. Question 5"></a>5. Question 5</h3><p>Which of the following are reasons for using feature scaling?</p><p>A. It prevents the matrix XTX (used in the normal equation) from being non-invertable (singular/degenerate).</p><p>B. It is necessary to prevent the normal equation from getting stuck in local optima.</p><p>C. It speeds up gradient descent by making it require fewer iterations to get to a good solution.</p><p>D. It speeds up gradient descent by making each iteration of gradient descent less expensive to compute.</p><p>解答：  C </p><p>normal equation 不需要 Feature Scaling，排除AB， 特征缩放减少迭代数量，加快梯度下降，然而不能防止梯度下降陷入局部最优。</p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Computing_Parameters_Analytically.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Computing_Parameters_Analytically.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.1_Multivariate_Linear_Regression</title>
      <link href="/2020/01/27/2-1-multivariate-linear-regression/"/>
      <url>/2020/01/27/2-1-multivariate-linear-regression/</url>
      
        <content type="html"><![CDATA[<h1 id="Multivariate-Linear-Regression"><a href="#Multivariate-Linear-Regression" class="headerlink" title="Multivariate Linear Regression"></a>Multivariate Linear Regression</h1><h2 id="一-Multiple-Features"><a href="#一-Multiple-Features" class="headerlink" title="一. Multiple Features"></a>一. Multiple Features</h2><p>具有多个变量的线性回归也被称为“多元线性回归”。</p><p>$x_{j}^{(i)}$: 训练集第 i 个向量中的第 j 个元素(第 i 行第 j 列)<br>$x^{(i)}$: 训练集第 i 个向量(第 i 行)<br>$ m $: 总共 m 行<br>$ n $: 总共 n 列  </p><p>适应这些多特征的假设函数的多变量形式如下：</p><p>$$ h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} + \cdots + \theta_{n}x_{n} $$</p><p>使用矩阵乘法的定义，我们的多变量假设函数可以简洁地表示为：</p><p>$$ h_{\theta}(x) = \begin{bmatrix}<br>\theta_{0} &amp; \theta_{1} &amp; \cdots  &amp; \theta_{n}<br>\end{bmatrix} \begin{bmatrix}<br>x_{0}\<br>x_{1}\<br> \vdots \<br>x_{n}<br>\end{bmatrix} = \theta^{T}x$$</p><p>其中 $ x_{0}^{(i)} = 1 (i\in 1,\cdots,m)$</p><hr><h2 id="二-Gradient-Descent-for-Multiple-Variables"><a href="#二-Gradient-Descent-for-Multiple-Variables" class="headerlink" title="二. Gradient Descent for Multiple Variables"></a>二. Gradient Descent for Multiple Variables</h2><p>多个变量的梯度下降，同时更新 n 个变量。</p><p>$$ \theta_{j} := \theta_{j} - \alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_{j}$$</p><p>其中 $ j \in [0,n]$</p><hr><h2 id="三-Gradient-Descent-in-Practice-I-Feature-Scaling"><a href="#三-Gradient-Descent-in-Practice-I-Feature-Scaling" class="headerlink" title="三. Gradient Descent in Practice I - Feature Scaling"></a>三. Gradient Descent in Practice I - Feature Scaling</h2><p>特征缩放包括将输入值除以输入变量的范围（即最大值减去最小值），导致新的范围仅为1。</p><p>均值归一化包括从输入变量的值中减去输入变量的平均值，从而导致输入变量的新平均值为零。</p><h3 id="1-Feature-Scaling"><a href="#1-Feature-Scaling" class="headerlink" title="1. Feature Scaling"></a>1. Feature Scaling</h3><p>特征缩放让特征值取值范围都比较一致，这样在执行梯度下降的时候，“下山的路线”会更加简单，更快的收敛。通常进行特征缩放都会把特征值缩尽量缩放到 [-1,1] 之间<strong>或者这个区间附近</strong>。</p><p>即 $ x_{i} = \frac{x_{i}}{s_{i}}$</p><h3 id="2-Mean-normalization"><a href="#2-Mean-normalization" class="headerlink" title="2. Mean normalization"></a>2. Mean normalization</h3><p>$ x_{i} = \frac{x_{i} - \mu_{i}}{s_{i}}$</p><p>其中，$\mu_{i}$ 是特征值的所有值的平均值，$s_{i}$ 是值的范围（最大 - 最小），或者 $s_{i}$ 是标准偏差</p><p>当然 $x_{0} = 1$ 就不需要经过上述的处理了，因为它永远等于1，不能有均值等于0的情况。</p><hr><h2 id="四-Gradient-Descent-in-Practice-II-Learning-Rate"><a href="#四-Gradient-Descent-in-Practice-II-Learning-Rate" class="headerlink" title="四. Gradient Descent in Practice II - Learning Rate"></a>四. Gradient Descent in Practice II - Learning Rate</h2><p>如果学习率 $\alpha $ 太小的话，就会导致收敛速度过慢的问题。<br>如果学习率 $\alpha $ 太大的话，代价函数可能不会在每次迭代中都下降，甚至可能不收敛，在某种情况下，学习率 $\alpha $ 过大，也有可能出现收敛缓慢。</p><p>可以通过绘制代价函数随迭代步数变化的曲线去调试这个问题。</p><p>$\alpha $ 的取值可以从 0.001，0.003，0.01，0.03，0.1，0.3，1 这几个值去尝试，选一个最优的。</p><hr><h2 id="五-Features-and-Polynomial-Regression"><a href="#五-Features-and-Polynomial-Regression" class="headerlink" title="五. Features and Polynomial Regression"></a>五. Features and Polynomial Regression</h2><p>可以通过改造特征值，例如合并2个特征，用 $ x_{3}$ 来表示 $ x_{1} * x_{2} $</p><p>在多项式回归中，针对 $ h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{1}^{2} + \theta_{3}x_{1}^{3} $ ，我们可以令 $ x_{2} = x_{1}^{2} , x_{3} = x_{1}^{3} $ 降低次数。</p><p>还可以考虑用根号的式子，例如选用  $ h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}\sqrt{x} $</p><p>通过上述转换以后，需要记得用<strong>特征值缩放，均值归一化，调整学习速率的方式调整一下</strong>。</p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p><a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Multivariate_Linear_Regression.ipynb" target="_blank" rel="noopener">Source</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.2_Linear_Regression_With_One_Variable(Gradient_Descent)</title>
      <link href="/2020/01/26/1-2-linear-regression-with-one-variable/"/>
      <url>/2020/01/26/1-2-linear-regression-with-one-variable/</url>
      
        <content type="html"><![CDATA[<h1 id="Linear-Regression-With-One-Variable-Gradient-Descent"><a href="#Linear-Regression-With-One-Variable-Gradient-Descent" class="headerlink" title="Linear Regression With One Variable(Gradient Descent)"></a>Linear Regression With One Variable(Gradient Descent)</h1><h2 id="一-Model-Representation"><a href="#一-Model-Representation" class="headerlink" title="一. Model Representation"></a>一. Model Representation</h2><p>在给定训练集的情况下，学习函数h：X→Y，使得h（x）是y的相应值的“好”预测器。由于历史原因，这个函数h被称为假设。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/68_0_1.png" alt></p><p>通过输入住房面积 x，通过学习好的函数，输出房子的估价。</p><h2 id="二-Cost-Function"><a href="#二-Cost-Function" class="headerlink" title="二. Cost Function"></a>二. Cost Function</h2><p>代价函数是线性回归中的一个应用，在线性回归中，要解决的一个问题就是最小化问题。</p><p>假设在一元线性回归中，在一个训练集中，我们需要找到一条直线能和该训练集中的点最接近。假设直线方程为 </p><p>$$h_{\theta}(x) = \theta_{0} + \theta_{1}x$$</p><p>如何选择 $\theta_{0}$、$\theta_{1}$，使得 $h_{\theta}(x)$ 更接近于训练集 (x,y) ？</p><p>上述问题可以转换为求 $$ \rm{CostFunction} = \rm{F}({\theta_{0}},{\theta_{1}}) = \frac{1}{2m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2 $$  求最小值$$\min_{{\theta_{0}} {\theta_{1}}} \rm{F}({\theta_{0},{\theta_{1}})} $$</p><h2 id="三-Gradient-Descent-梯度下降"><a href="#三-Gradient-Descent-梯度下降" class="headerlink" title="三. Gradient Descent 梯度下降"></a>三. Gradient Descent 梯度下降</h2><p>梯度下降的主要思想：</p><ol><li><p>初始化<br>$$<br>{\theta_{0}}和 {\theta_{1}} , {\theta_{0}} = 0 , {\theta_{1}}=0<br>$$</p></li><li><p>不断的改变 ${\theta_{0}}$ 和 ${\theta_{1}}$ 值，不断减少 $F({\theta_{0}},{\theta_{1}})$ 直至达到最小值（或者局部最小）。</p></li></ol><p><img src="https://img.halfrost.com/Blog/ArticleImage/68_1_0.png" alt></p><p>想象成下山，如何下山的速度最快？这里涉及到了下山的速度，即步长。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/68_2_.png" alt></p><p>有趣的是换旁边一个点，下山，找到的最优解可能就是另一个了。这也是梯度下降的一个特点。它会找到所有的局部最优解出来。</p><p>梯度下降算法，不断更新：</p>\begin{align*}\rm{temp}0 &amp;:= {\theta_{0}} - \alpha * \frac{\partial }{\partial {\theta_{0}}}\rm{F}({\theta_{0}},{\theta_{1}}) \\\rm{temp}1 &amp;:= {\theta_{1}} - \alpha * \frac{\partial }{\partial {\theta_{1}}}\rm{F}({\theta_{0}},{\theta_{1}}) \\{\theta_{0}} &amp;:= \rm{temp}0 \\{\theta_{1}} &amp;:= \rm{temp}1 \\\end{align*}<p>直到收敛。注意 ${\theta_{0}}$ 和 ${\theta_{1}}$ 值要<strong>同时更新</strong>，<strong>切记不要求一次导更新一次！</strong></p><p>$\alpha$ 被称作为学习速率。</p><p><img src="https://img.halfrost.com/Blog/ArticleImage/68_3.gif" alt></p><p>如果 $\alpha$ 被设置的很小，需要很多次循环才能到底最低点。<br>如果 $\alpha$ 被设置的很大，来来回回可能就会离最低点越来越远，<strong>会导致无法收敛，甚至发散</strong>。</p><p>当快要到最低点的时候，梯度下降会越来越慢，因为 $ \frac{\partial }{\partial {\theta}}$ 越来越小。</p><h2 id="关于-梯度-和-偏导数-的关系"><a href="#关于-梯度-和-偏导数-的关系" class="headerlink" title="关于 梯度 和 偏导数 的关系"></a>关于 梯度 和 偏导数 的关系</h2><p>在上面梯度下降算法中，我们一直用的是偏导数进行讨论的，可能会有人有疑问，偏导数和梯度有啥关系？</p><h3 id="1-导数"><a href="#1-导数" class="headerlink" title="1. 导数"></a>1. 导数</h3><p>如果是一元的，那么偏导数就降级成了求导数</p>$$ f^{'}(x_{0}) = \lim_{\Delta x\rightarrow 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x\rightarrow 0} \frac{f(x_{0} + \Delta x) - f(x_{0})}{\Delta x} $$<p>导数的几何意义是切线在该点的斜率，物理意义是函数在这一点的 (瞬时) 变化率。</p><h3 id="2-偏导数"><a href="#2-偏导数" class="headerlink" title="2. 偏导数"></a>2. 偏导数</h3><p>在来看看偏导数的定义：</p>$$\begin{align*}f_{x}(x_{0},y_{0}) &amp; = \lim_{\Delta x \rightarrow 0} \frac{f(x_{0} + \Delta x , y_{0}) - f(x_{0},y_{0})}{\Delta x} \\ f_{y}(x_{0},y_{0}) &amp; = \lim_{\Delta y \rightarrow 0} \frac{f(x_{0} , y_{0} + \Delta y) - f(x_{0},y_{0})}{\Delta y} \\\end{align*}$$<p><img src="https://img.halfrost.com/Blog/ArticleImage/68_4.png" alt></p><p>偏导数的几何意义也是切线的斜率，不过由于在曲面上，在一个点上与该曲面曲线相切的是一个面，就意味着切线有无数条。这里我们感兴趣的是2条切线，一个条是垂直于 y 轴（平行于 xOz 平面）的切线，另外一条是垂直于 x 轴（平行于 yOz 平面）的切线。这两条切线对应的斜率就是对 X 求偏导和对 Y 求偏导。</p><p>一个多变量函数的偏导数是它关于其中一个变量的导数，而保持其他变量恒定（相对于全导数，在其中所有变量都允许变化）。</p><p>偏导数的物理意义表示函数沿着坐标轴正方向上的变化率。</p><h3 id="3-方向导数"><a href="#3-方向导数" class="headerlink" title="3. 方向导数"></a>3. 方向导数</h3><p>在说梯度之前，不应该漏掉方向导数。偏导数是求的在特定的2个方向上的导数，但是任意一个方向上也是存在导数的。这里就引入了方向导数的概念。</p><blockquote><p>设函数 u = u(x,y) 在点 $p_{0}(x_{0},y_{0})$ 的某空间临域 $ U \subset R^{2}$ 内有定义， L 为从点 $p_{0}$ 出发的射线，$p(x_{0},y_{0})$ 为 L 上且在 U 内的任一点，以 $t = \sqrt{(\Delta x)^{2} +(\Delta y)^{2} }$ 表示 $p$ 与 $p_{0}$ 之间的距离，若极限 ：</p></blockquote><blockquote>$$ \left.\begin{matrix}\frac{\partial f}{\partial l}\end{matrix}\right|_{(x_{0},y_{0})} = \lim_{t \rightarrow 0^{+}} \frac{f(x_{0} + tcos \alpha , y_{0}  +   tcos \beta) - f(x_{0},y_{0})}{t}$$<p>存在，则称此极限为函数 u = u(x,y) 在点 $p_{0}$ 沿方向 L 的方向导数，记作 $ \left.\begin{matrix}\frac{\partial f}{\partial l}\end{matrix}\right|_{(x_{0},y_{0})}$ 。</p></blockquote><p>方向导数是偏导数的概念的推广, 偏导数研究的是指定方向 (坐标轴方向) 的变化率，到了方向导数，指定的方向可以是任意方向了。</p><blockquote><p>如果函数 u = u(x,y) 在点 $p_{0}(x_{0},y_{0})$ 可微分，那么函数在该点沿任一方向 L 的方向导数存在，且有</p></blockquote><blockquote>$$ \left.\begin{matrix}\frac{\partial f}{\partial l}\end{matrix}\right|_{(x_{0},y_{0})} = f_{x}(x_{0},y_{0})cos \alpha + f_{y}(x_{0},y_{0})cos \beta$$</blockquote><blockquote><p>其中， $cos \alpha  $ ，$cos \beta$ 是方向 L 的方向余弦。</p></blockquote><p>一个标量场在某点沿着某个向量方向上的方向导数，描绘了该点附近标量场沿着该向量方向变动时的瞬时变化率。这个向量方向可以是任一方向。</p><p>方向导数的物理意义表示函数在某点沿着某一特定方向上的变化率。</p><h3 id="4-梯度"><a href="#4-梯度" class="headerlink" title="4. 梯度"></a>4. 梯度</h3><p>最后来讲讲梯度，梯度的定义：</p><blockquote><p>在二元函数的情形，设函数 $f(x,y)$ 在平面区域 D 内具有一阶连续偏导数，则对于每一点 $P_{0}(x_{0},y_{0}) \in D $,都可定出一个向量：</p></blockquote><blockquote>$$ f_{x}(x_{0},y_{0}) \vec{i} + f_{y}(x_{0},y_{0}) \vec{j} $$</blockquote><blockquote><p>这个向量称为函数 $f(x,y)$ 在点 $p_{0}(x_{0},y_{0})$ 的梯度，记作 $ \textbf{grad}\;\;f(x_{0},y_{0}) $ 或 $ \triangledown f(x_{0},y_{0}) $ , 即</p></blockquote><blockquote>$$ \textbf{grad}\;\;f(x_{0},y_{0}) = \triangledown f(x_{0},y_{0}) = f_{x}(x_{0},y_{0}) \vec{i} + f_{y}(x_{0},y_{0}) \vec{j} $$</blockquote><blockquote><p>其中 $ \triangledown = \frac{\partial }{\partial x} \vec{i} + \frac{\partial }{\partial y} \vec{j} $ 称为 (二维的) 向量微分算子 或者 Nabla 算子， $ \triangledown f = \frac{\partial f}{\partial x} \;\; \vec{i} + \frac{\partial f }{\partial y} \;\; \vec{j} $</p></blockquote><p>如果函数 $f(x,y)$ 在点 $p_{0}(x_{0},y_{0})$ 可微分， $\vec{e_{j}} = (cos \alpha,cos \beta)$ 是与方向 L 同向的单位向量，则：</p>$$\begin{align*}\left.\begin{matrix}\frac{\partial f}{\partial l}\end{matrix}\right|_{(x_{0},y_{0})} &amp;= f_{x}(x_{0},y_{0})cos \alpha + f_{y}(x_{0},y_{0})cos \beta \\&amp;= \textbf{grad}\;\;f(x_{0},y_{0}) \cdot \vec{e_{j}} = \left | \textbf{grad}\;\;f(x_{0},y_{0}) \right | cos \theta \\\end{align*}$$<p>其中 $ \theta $ 为 $ \textbf{grad};;f(x_{0},y_{0}) $ 与 $ \vec{e_{j}} $ 的夹角。</p><ol><li>当 $\theta = 0 $ 的时候，$\left.\begin{matrix}\frac{\partial f}{\partial l}\end{matrix}\right|_{(x_{0},y_{0})}  = \left | \textbf{grad}\;\;f(x_{0},y_{0}) \right |$</li></ol><p>即 <strong>函数 $f(x,y)$ 在一点的梯度 $ \textbf{grad};;f $ 是这样的一个向量，它的方向是函数在这点的方向导数取得最大值的方向，它的模就等于方向导数的最大值</strong> 。</p><ol start="2"><li>当 $\theta = \pi $ 的时候，$\left.\begin{matrix}\frac{\partial f}{\partial l}\end{matrix}\right|_{(x_{0},y_{0})}  = - \left | \textbf{grad}\;\;f(x_{0},y_{0}) \right |$</li></ol><p>即 $ \vec{e_{j}} $ 与 梯度 方向相反的时候，函数减少最快，在这个方向的方向导数达到最小值。</p><p><strong>所以梯度下降就是基于这个原理</strong>。</p><p>函数在某一点处的方向导数在其梯度方向上达到最大值，此最大值即梯度的模数。</p><p>这就是说，沿梯度方向，函数值增加最快。同样可知，方向导数的最小值在梯度的相反方向取得，此最小值为最大值的相反数，从而沿梯度相反方向函数值的减少最快。</p><table><thead><tr><th>概念</th><th>物理意义</th></tr></thead><tbody><tr><td>导数   $ f^{‘}(x)  $</td><td>函数在该点的瞬时变化率</td></tr><tr><td>偏导数 $ \frac{\partial f(x,y) }{\partial x}  $</td><td>函数在坐标轴方向上的变化率</td></tr><tr><td>方向导数</td><td>函数在某点沿某个特定方向的变化率</td></tr><tr><td>梯度  $ \textbf{grad};;f(x,y)  $</td><td>函数在该点沿所有方向变化率最大的那个方向</td></tr></tbody></table><h2 id="四-Linear-Regression-线性回归"><a href="#四-Linear-Regression-线性回归" class="headerlink" title="四. Linear Regression 线性回归"></a>四. Linear Regression 线性回归</h2><p>梯度下降是很常用的算法，它不仅被用在线性回归，还用在线性回归模型、平方误差代价函数中。</p>\begin{align*}\frac{\partial }{\partial {\theta_{j}}}\rm{F}({\theta_{0}},{\theta_{1}}) &amp; = \frac{\partial }{\partial {\theta_{j}}} \frac{1}{2m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2\\\end{align*}<p>令 $ z = (h_{\theta}(x^{(i)})-y^{(i)})^2$ , $ u = h_{\theta}(x^{(i)})-y^{(i)}$ , 则 $ z = u^2 $。 考虑到 $f(z)$  和 $f(u)$ 都是连续的，则有：</p>\begin{align*}\frac{\partial }{\partial {\theta_{j}}}\rm{F}({\theta_{0}},{\theta_{1}}) &amp; = \frac{\partial }{\partial {\theta_{j}}} \frac{1}{2m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2\\&amp; = \frac{1}{2m}\sum_{i = 1}^{m} \frac{\partial z }{\partial u} \frac{\partial u }{\partial {\theta_{j}}} = \frac{1}{2m} * 2 \sum_{i = 1}^{m} u \frac{\partial u }{\partial {\theta_{j}}}\\&amp; = \frac{1}{m} \sum_{i = 1}^{m} u \frac{\partial u }{\partial {\theta_{j}}} \\\end{align*}<p>++++++++++++</p><p>将 u 展开 $ u = \theta_{0} + {\theta_{1}}x^{(i)}-y^{(i)}$ , 令 j = 0,则有</p>\begin{align*}\frac{\partial }{\partial {\theta_{0}}}\rm{F}({\theta_{0}},{\theta_{1}}) &amp;= \frac{1}{m} \sum_{i = 1}^{m} u \frac{\partial u }{\partial {\theta_{0}}} \\&amp;= \frac{1}{m} \sum_{i = 1}^{m}(\theta_{0} + \theta_{1}x^{(i)} - y^{(i)}) = \frac{1}{m} \sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)}) \\\end{align*}<p>令 j = 1,则有</p>\begin{align*}\frac{\partial }{\partial {\theta_{1}}}\rm{F}({\theta_{0}},{\theta_{1}}) &amp;=  \frac{1}{m} \sum_{i = 1}^{m} u \frac{\partial u }{\partial {\theta_{1}}}\\&amp;= \frac{1}{m} \sum_{i = 1}^{m}(\theta_{0} + \theta_{1}x^{(i)} - y^{(i)}) * x^{(i)} = \frac{1}{m} \sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)}) * x^{(i)} \\\end{align*}<p>梯度下降算法：</p> \begin{align*}\rm{temp}0 &amp;:= {\theta_{0}} - \alpha * \frac{\partial }{\partial {\theta_{0}}}\rm{F}({\theta_{0}},{\theta_{1}}) = {\theta_{0}} - \alpha * \frac{1}{m} \sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})  \\\rm{temp}1 &amp;:= {\theta_{1}} - \alpha * \frac{\partial }{\partial {\theta_{1}}}\rm{F}({\theta_{0}},{\theta_{1}}) = {\theta_{1}} - \alpha * \frac{1}{m} \sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)}) * x^{(i)} \\{\theta_{0}} &amp;:= \rm{temp}0  \\{\theta_{1}} &amp;:= \rm{temp}1  \\\end{align*}<p>当然除了用梯度下降的迭代算法，还有其他方法可以算出代价函数的最小值，比如线性代数里面的 正规方程组法。但是两者相比较而言，梯度下降适合更大的数据集。</p><p>举个例子，通过梯度下降不断更新以后，线性回归以后的曲线和原始数据集会越来越拟合。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> npx_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.9</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.91</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8.02</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">4.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.23</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.923</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2.941</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.02</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">6.34</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.543</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.546</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8.744</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.674</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.643</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">5.33</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.31</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.78</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1.01</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.68</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">9.99</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3.54</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.89</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10.9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>y_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.34</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3.86</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.63</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.78</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10.6453</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8.43</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">4.75</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.345</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.546</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.5754</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2.35654</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.43646</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">6.6443</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.64534</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.546</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8.7457</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.6464</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.74643</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">6.32</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.42</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.1243</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1.088</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10.342</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">9.24</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4.22</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.44</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.33</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>y_data <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.9</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.91</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8.02</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">4.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.23</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.923</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2.941</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.02</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">6.34</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.543</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7.546</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8.744</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.674</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.643</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">5.33</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5.31</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.78</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1.01</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9.68</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    <span class="token punctuation">[</span><span class="token number">9.99</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3.54</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6.89</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10.9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token operator">%</span>matplotlib inlineplt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> <span class="token string">'bo'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'real'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_data<span class="token punctuation">,</span> <span class="token string">'r-'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'estimated'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre><code>&lt;matplotlib.legend.Legend at 0x7fa6805ab128&gt;</code></pre><p><img src="/2020/01/26/1-2-linear-regression-with-one-variable/output_7_1.png" alt="output"></p><hr><blockquote><p>GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p><p>Follow: <a href="https://github.com/halfrost" target="_blank" rel="noopener">halfrost · GitHub</a></p><p>Source: <a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Gradient_descent.ipynb" target="_blank" rel="noopener">https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Gradient_descent.ipynb</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.1_What_is_Machine_Learning</title>
      <link href="/2020/01/19/1-1-what-is-machine-learning/"/>
      <url>/2020/01/19/1-1-what-is-machine-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-Machine-Learning"><a href="#What-is-Machine-Learning" class="headerlink" title="What is Machine Learning"></a>What is Machine Learning</h1><p><img src="/2020/01/19/1-1-what-is-machine-learning/machine-learning.png" alt="Machine_Learning"></p><h2 id="一、-Definition"><a href="#一、-Definition" class="headerlink" title="一、 Definition"></a>一、 Definition</h2><p>定义：</p><p>1997年，<code>Tom Mitchell</code> 给出机器学习的定义：</p><pre><code>程序利用经验E改善了在任务T中的性能P，就可以说：关于任务T和测量性能P，该程序对经验E进行了学习。</code></pre><h2 id="二、-Classify"><a href="#二、-Classify" class="headerlink" title="二、 Classify"></a>二、 Classify</h2><p>分类：</p><ul><li>有监督学习<code>supervised learning</code> :已知的Data Set中明确了输入/输出，且输入和输出存在关系。 <ul><li><code>Supervised Learning</code>可以分为：分类(Classification)和回归(Regression)问题。</li><li><ol><li>Classification： 预测离散的结果。将输入映射到离散的类别中。</li></ol></li><li><ol start="2"><li>Regression： 预测连续输出中的结果。 从输入映射到某个连续的函数的输出中。</li></ol></li></ul></li></ul><blockquote><p>无监督学习使我们能够很少或根本不知道我们的结果应该是什么样子。<strong>我们可以从数据中得出结构</strong>，我们不一定知道变量的影响。 我们可以通过基于数据中变量之间的关系对数据进行聚类来推导出这种结构。 在无监督学习的情况下，<strong>没有基于预测结果的反馈</strong>。无监督学习可以分为“聚类”和“非聚类”。</p></blockquote><ul><li>无监督学习<code>unsupervised learning</code> : 没有预知的label，从变量的结构中寻找关系，而没有基于预测结果的反馈。<ul><li><code>Unspervised Learning</code>可以分为<code>聚类</code>， 和 <code>非聚类</code>。</li><li><ol><li>聚类： 可以理解为对数据自动分组成不同变量的相似或者相关的簇。</li></ol></li><li><ol start="2"><li>非聚类： 比如“鸡尾酒会算法”–&gt;从混乱的环境中识别和查找结果。</li></ol></li></ul></li></ul><blockquote><p>参考： GitHub Repo：<a href="https://github.com/halfrost/Halfrost-Field" target="_blank" rel="noopener">Halfrost-Field</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>19年岁末</title>
      <link href="/2019/12/31/19-nian-sui-mo/"/>
      <url>/2019/12/31/19-nian-sui-mo/</url>
      
        <content type="html"><![CDATA[<h1 id="挥别2019，迎接变化"><a href="#挥别2019，迎接变化" class="headerlink" title="挥别2019，迎接变化"></a>挥别2019，迎接变化</h1><p>2020是个很有意思的年份，ABAB的形式朗朗上口，如果要这样严格的算的话，上一个这样的年份是1919年，相隔101年了已经，下一次要到2121，又是101年以后，也就是说，我们一生中，只会经历这样一次的ABAB年份，实在是值得用力去记忆和感受。</p><h2 id="2019，我得到了什么？"><a href="#2019，我得到了什么？" class="headerlink" title="2019，我得到了什么？"></a>2019，我得到了什么？</h2><blockquote><p>Sometimes it lasts in love, sometimes it hurt instead.</p></blockquote><p>一个并不擅长的技能，一段说不清道不明的关系，一个不好不坏的工作机会。还有一个不知道是在成长还是在变老的自己。</p><h3 id="技能"><a href="#技能" class="headerlink" title="技能"></a>技能</h3><blockquote><p>关于项目</p></blockquote><p>在复旦的一年半，说是学习NLP，其实到了最后也只是一直在皮毛的层面晃悠，总是无法深入。</p><p>其实自己也清楚自己的原因在哪边，很难专注一个方向，各种都在好奇，结果就是看两篇论文就算了，其实过两天也就忘了。NLP或者说是深度学习，方向千奇百怪，其实前期找准一个有工业应用前景的方向，比如对话系统，摘要，这种，沿着一篇综述独立开展研究，代码和论文同时开始，再看些相关的比赛开源的代码，应该半年就能入门。这个事情只能说明自己浮躁，浮躁是科研的大忌，所以我不适合读博了，至少目前看来是这样的。</p><p>啊我真是个憨憨。</p><p>最气的是最后Tensorflow和PyTorch居然一个都不熟…我这渣渣工程能力总是会给我巨大的打击…</p><blockquote><p>关于实验室</p></blockquote><p>2018年5月份开始在王老师的EDA实验室呆了一个多月，认识了师兄刘淇，和钟程同学一起搞C++编译器，结果我总是难以入门，那时候我大概连怎么查资料都不会。而且莫名其妙的嫉妒钟程，越来越难受，所以最后还是离开了。</p><p>当时的契机是听刘涛讲算法会更有”钱途“，就心动来了IBICAS，当时实验室还叫做BCRC，事实证明，不管哪一个方向，只要学得好就牛逼，同为IT基本不存在哪个方向明显更有‘’钱途“，抄近路的方式是不可取的…最后我的offer不如一直做IC的同学，很大的教训，仔细想做什么事情都是这个道理吧…<strong>要怀着做到最好的心思去做事情</strong></p><p>最有意思的是最后的offer居然还是C++开发，果然是有一种循环。</p><blockquote><p>关于课程</p></blockquote><p>复旦工硕的课程本来设置的就很多，各种课程各种学分，让人头大…</p><p>研一的时候我总是心大，想着尽快修完学分去实习，所以第一学期选课12门，真的作死，而且累是次要，因为只是为了时间不冲突选了很多不会用到的课程，比如MEMS，贼气的一门，最后绩点也相应的很低。</p><p>毕竟不是计算机专业，大部分的课程都是集成电路，实际上对我的项目和将来的实习工作帮助都不大，现在回忆，还是觉得是在混学分。</p><p>不过实在是对IC兴趣不大，那我喜欢什么呢？大概就是新鲜的能理解能看到的炫酷一点的东西。这样想想我换了软开，换了算法，也不能说是完全看在”钱途“的诱惑上面…hhh</p><h3 id="关系"><a href="#关系" class="headerlink" title="关系"></a>关系</h3><p>2019年是我面临各种关系最多一年，当然主要是男女生的关系发生了巨大的变化。对于这些，当我身处其中的时候，内心忐忑也激动期待，可是到了今天彻底脱离这些，留下的只有空虚和满地的鸡毛…</p><blockquote><p>Chen Jiawen</p></blockquote><p>陈认识在一周cp上，2018.06，在我刚刚来上海，一切都是很好奇很新鲜，我们从网络上认识和开始，到2020.01.01，也从网络上结束，中间分分合合之久，完全写一本让人鸡皮掉落的矫情小说。</p><p>2018.06她同意一起 —&gt; 2018.06她立刻悄悄回家准考研，并没能见面 —&gt; 她在家学习，两个寂寞的人总是互相陪伴 —&gt; 因为突然胖了很多或者其他一些琐碎的小事，我对她生气过很多次 —&gt; 2018.12考研结束，我提出分手 —&gt; 2019.02成绩出来，落榜，她挽回，我们又在一起 —&gt; 定下再陪一年 —&gt; 中间各种矛盾，纠缠，分手不断 —&gt; 2019.10我对文吐露了一些心事，和陈不再想着挽回 —&gt; 2019.11彻底结束 —&gt; 2019.12.31因为汪，她想挽回 —&gt; 2020.01.01,终于轮到她主动提拜拜，好运。</p><p><strong><em>元旦快乐，CJW！祝 2020年，心想事成~</em></strong></p><p>其实我对别人的要求，在这之后再也不会有了，明白了一件事情：喜欢就是喜欢，不喜欢就是不喜欢，喜欢的沾满泥土也不会挑剔。</p><blockquote><p>Wang Lu</p></blockquote><p>抱歉，2019.11-2019.12，一个多月，没能到最后，我真的蠢。</p><p>唯一朋友圈官宣家人也知道的一位，结果越来越累，我想要的感情不是这种被比较衡量之后觉得对方不错就ok的，这段关系里面，我是想护你爱你的，一个比我年龄小的女孩子比我要成熟。进行下去真的太累了，身心俱疲，承认自己并不是之前想的那样对女生无要求。喜欢这种小事，没有触碰到那根弦，就注定没有可能发生了吧。</p><p>虽然我是要说抱歉的一方，但是实际上你也并不喜欢我的吧。</p><p><strong><em>各自安好啦，本命年开心幸福</em></strong></p><blockquote><p>Wen YZ</p></blockquote><p>无</p><p>我理解你了。</p><p>上海对我的意义几乎消失殆尽了，谢谢你没用力拒绝，只是这样依旧不会觉得是很好的方式。我猜我会在离开上海之前再见你最后一次啦，不会打扰你很久的。YZ同学真真是天使。</p><blockquote><p>家人</p></blockquote><p>我姐最最重要，姐姐心眼太少了，对姐姐发了一次火，感到很难受。想一直陪姐姐哈哈哈哈</p><p>奶奶和爸爸，要多关心家人啊，自己早就不是小孩了。</p><blockquote><p>伙伴</p></blockquote><p>不算多，还算好。之后会努力交朋友的，让大家看到我的♥也不是一直很枯燥。以前的现在的各位小伙伴，我们都要努力玩耍，努力学习工作，加油。</p><h3 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h3><blockquote><p>实习</p></blockquote><p>从2019.04-2019.06，大概面了4，5家的算法，都是大厂，被各种蹂躏，我怂了，之后暑假就没再继续了。这是我找工作以来最大的错。</p><p>总结失误：</p><ul><li>不应该上来先面大厂；</li><li>不应该不认真刷题；</li><li>不应该自惭形秽；</li></ul><p>最终2019.09在百姓网实习了4周左右，一边忙着秋招一边实习，真的心都要操碎了。</p><p>谢谢各位。所幸最终的结果是我能接受的。</p><p>因为这样的消极害怕的心理，直接导致自己错过提前批，正式批的笔试都没有通过一家，真的太菜了。还是要努力多学习多刷题，掌握到的编程技能是自己的，会是永远都有用的。</p><blockquote><p>Offer</p></blockquote><p>最后拿到的offer居然是早早就面的第一家，ZTE，这样想想，真的认真面试的只有ZTE和华为两家，华为的依然不是很匹配，被刷掉了。ZTE的说是C++软开，但是偏通信系统协议，依然不能说是理想。而且薪水比较同学差距也有一些。</p><p>谁叫我是菜鸡呢..哭…</p><p>先拿到上海的户口，2020.07入职，2021.04我一定会进入头条或者PDD其中之一的，FLAG立在这了。</p><h2 id="2020，我想要什么？"><a href="#2020，我想要什么？" class="headerlink" title="2020，我想要什么？"></a>2020，我想要什么？</h2><p>2020年鼠年，我刚刚结束了自己的本命年，在实验室做毕设，有几个聊得来的朋友，马上会面临毕业和工作，带着自己并不娴熟的技能，一切好像没那么好，也没那么糟。元旦这天，我想许很多的心愿，希望能在这一年得到回应。</p><p>小孩子都会贪心的。我想要很好的技能，很多的钱，很多的朋友，很充实的时间。</p><p>技能方面：</p><ol><li>C++（工作必须）</li><li>NLP，Python，PyTorch（毕设以及个人发展）</li><li>英语</li></ol><p>生活方面：</p><ol><li>Kindle（想捡起阅读的习惯）</li><li>显示器，桌椅（和我的Surface配合应该算个不错的生产环境）</li><li>羽毛球拍，球鞋</li></ol><p>另外还有，希望顺利毕业，希望工作后也能有生活，希望朋友一直都能毫无芥蒂，希望我喜欢的人也恰好看到了我。</p><p>FLAG:</p><ol><li>新建一个Github账号，把现在的东西慢慢自己写一遍；</li><li>每天commit，少一个commit就减餐一顿；</li><li>27w—&gt;35w，要相信自己努力提高就可以达到。</li></ol><p>总之，2020年，各位一起变得更好吧~ 流年笑掷，未来可期！</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 日记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 年终总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01_PyTorch基础使用</title>
      <link href="/2019/11/07/01-pytorch-ji-chu-shi-yong/"/>
      <url>/2019/11/07/01-pytorch-ji-chu-shi-yong/</url>
      
        <content type="html"><![CDATA[<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torchtorch<span class="token punctuation">.</span>__version__</code></pre><pre><code>'1.3.0'</code></pre><a id="more"></a><h1 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h1><blockquote><p>Tensor的使用：</p></blockquote><ul><li><ol><li>构建</li></ol></li><li><ol start="2"><li>Tensor的基本运算</li></ol></li><li><ol start="3"><li>Tensor与Numpy转换</li></ol></li><li><ol start="4"><li>共享内存的情况</li></ol></li><li><ol start="5"><li>自动微分</li></ol></li></ul><h2 id="1-构建Tensor"><a href="#1-构建Tensor" class="headerlink" title="1. 构建Tensor"></a>1. 构建Tensor</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 构建5×3矩阵，分配空间，不初始化</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 使用特定数据初始化Tensor</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 使用[0, 1]均匀分布随机初始化</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 查看Tensor形状的两种方法</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><pre><code>3</code></pre><h2 id="2-Tensor的基本运算"><a href="#2-Tensor的基本运算" class="headerlink" title="2. Tensor的基本运算"></a>2. Tensor的基本运算</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 加法</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 第一种方式</span>z <span class="token operator">=</span> x <span class="token operator">+</span> y<span class="token comment" spellcheck="true"># 第二种方式</span>torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> out<span class="token operator">=</span>z<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 第三种方式</span>z <span class="token operator">=</span> y<span class="token punctuation">.</span>add<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 第四种方式，修改y值</span>y<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><pre><code>tensor([[1.4718, 0.5690, 1.1329],        [0.3453, 0.8727, 0.7226],        [1.2681, 0.8222, 1.8243],        [1.3840, 0.8803, 1.4788],        [1.5312, 1.0661, 0.9357]])</code></pre><h2 id="3-Tensor与Numpy转换"><a href="#3-Tensor与Numpy转换" class="headerlink" title="3. Tensor与Numpy转换"></a>3. Tensor与Numpy转换</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Tensor的切片与Numpy相似,选出index=1的列</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 新建全为1的Tensor</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Tensor->Numpy</span>b <span class="token operator">=</span> a<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Numpy -> Tensor</span><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np a <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token triple-quoted-string string">'''要注意这里的a和b内存共享，一个改变，另一个会同时跟随改变'''</span><span class="token comment" spellcheck="true"># 获取某个元素值</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 注意：torch.tensor与torch.Tensor有差别，而且，torch.tensor是对原始tensor的拷贝，不再共享同样的内存</span>z_ <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>z <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>z_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>z_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span>z_<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>z_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span></code></pre><pre><code>tensor([5., 3.])tensor([5., 3.])tensor([6., 4.])tensor([5., 3.])/home/xwjia/anaconda3/envs/Torch/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).</code></pre><h2 id="4-共享内存情况"><a href="#4-共享内存情况" class="headerlink" title="4. 共享内存情况"></a>4. 共享内存情况</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 当需要共享内存时，可以：</span>a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 或者</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> a<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 转换为GPU支持的Tensor</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>a <span class="token operator">=</span> a<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>b <span class="token operator">=</span> b<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>c <span class="token operator">=</span> a <span class="token operator">+</span> b</code></pre><h2 id="5-自动微分"><a href="#5-自动微分" class="headerlink" title="5. 自动微分"></a>5. 自动微分</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 为Tensor设定可以求导</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y <span class="token operator">=</span> x<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span>y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>x<span class="token punctuation">.</span>grad<span class="token comment" spellcheck="true"># 第二次求导之前要归零，不然会累加</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>x<span class="token punctuation">.</span>grad</code></pre><pre><code>tensor([[1., 1.],        [1., 1.]])</code></pre><pre class=" language-python"><code class="language-python"></code></pre><h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p>导入<code>torch.nn</code>，封装可以自动求导，只需要在自定义的类继承于<code>nn.Module</code>,类中需要实现<code>__init__</code>和<code>forward</code>方法；</p><p>其中：</p><ul><li><ol><li><code>__init__</code>中存放网络中可以学习的参数；</li></ol></li><li><ol start="2"><li><code>super(Net, self).__init__()</code>等价于父类<code>nn.Module.__init__(self)</code></li></ol></li><li><ol start="3"><li>网络中不学习的参数，比如最大池化或者ReLU，可以放在<code>forward</code>方法中</li></ol></li></ul><h2 id="1-自定义神经网络"><a href="#1-自定义神经网络" class="headerlink" title="1. 自定义神经网络"></a>1. 自定义神经网络</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F <span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">5</span><span class="token operator">*</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#reshape, '-1'表示自适应</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 打印网络可学习的参数</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">)</span>params <span class="token operator">=</span> list<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>len<span class="token punctuation">(</span>params<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> name<span class="token punctuation">,</span> parameters <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> <span class="token string">":"</span><span class="token punctuation">,</span> parameters<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre><code>Net(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))&lt;generator object Module.parameters at 0x7fb59472e7d0&gt;&lt;bound method Module.named_parameters of Net(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))&gt;10conv1.weight : torch.Size([6, 1, 5, 5])conv1.bias : torch.Size([6])conv2.weight : torch.Size([16, 6, 5, 5])conv2.bias : torch.Size([16])fc1.weight : torch.Size([120, 400])fc1.bias : torch.Size([120])fc2.weight : torch.Size([84, 120])fc2.bias : torch.Size([84])fc3.weight : torch.Size([10, 84])fc3.bias : torch.Size([10])</code></pre><pre class=" language-python"><code class="language-python">input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>input<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>out <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span>out<span class="token comment" spellcheck="true"># 所有参数清零</span>net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="需要注意的是，torch-nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用-input-unsqueeze-0-将batch-size设为１。例如-nn-Conv2d-输入必须是4维的，形如"><a href="#需要注意的是，torch-nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用-input-unsqueeze-0-将batch-size设为１。例如-nn-Conv2d-输入必须是4维的，形如" class="headerlink" title="需要注意的是，torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用 input.unsqueeze(0)将batch_size设为１。例如 nn.Conv2d 输入必须是4维的，形如"></a>需要注意的是，torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用 <code>input.unsqueeze(0)</code>将batch_size设为１。例如 <code>nn.Conv2d</code> 输入必须是4维的，形如</h3><p>$$<br>nSamples \times nChannels \times Height \times Width<br>$$</p><h3 id="可将nSample设为1，即"><a href="#可将nSample设为1，即" class="headerlink" title="可将nSample设为1，即"></a>可将nSample设为1，即</h3><p>$$<br>1 \times nChannels \times Height \times Width<br>$$</p><h2 id="2-损失函数"><a href="#2-损失函数" class="headerlink" title="2. 损失函数"></a>2. 损失函数</h2><ul><li><ol><li><code>nn.MSELoss</code>计算均方误差；</li></ol></li><li><ol start="2"><li><code>nn.CrossEntropyLoss</code>计算交叉熵损失；</li></ol></li></ul><pre class=" language-python"><code class="language-python">input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>output <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span>target <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>target<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span></code></pre><pre><code>tensor([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]])tensor(28.4120, grad_fn=&lt;MseLossBackward&gt;)</code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 运行.backward,可以观察调用反向传播之前和之后的grad</span>net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 把net中的所有可学习参数的梯度清零</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"反向传播之前 conv1.bias 的梯度"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>conv1<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"反向传播之后...."</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>conv1<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>grad<span class="token punctuation">)</span></code></pre><pre><code>反向传播之前 conv1.bias 的梯度tensor([0., 0., 0., 0., 0., 0.])反向传播之后....tensor([ 0.0700, -0.0912,  0.0596,  0.0453,  0.0661,  0.0147])</code></pre><h2 id="3-优化器"><a href="#3-优化器" class="headerlink" title="3. 优化器"></a>3. 优化器</h2><p>反向传播计算梯度之后，还需要要优化方法更新网络的权重和参数，比如<code>SGD</code>:</p><p><code>weight = weight - learning_rate * gradient</code></p><p>手动实现如下：</p><pre class=" language-python"><code class="language-python">learning_rate <span class="token operator">=</span> <span class="token number">0.01</span><span class="token keyword">for</span> f <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    f<span class="token punctuation">.</span>data<span class="token punctuation">.</span>sub_<span class="token punctuation">(</span>f<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data <span class="token operator">*</span> learning_rate<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># inplace 减法</span></code></pre><p><code>torch.optim</code>中实现了深度学习中绝大多数的优化方法，例如RMSProp、Adam、SGD等，更便于使用，因此大多数时候并不需要手动写上述代码。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token comment" spellcheck="true"># 新建优化器， 指定需要调整的参数和学习率</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 训练时，梯度先清零：</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 计算loss</span>output <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 反向传播</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 更新参数</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="4-数据加载和预处理"><a href="#4-数据加载和预处理" class="headerlink" title="4. 数据加载和预处理"></a>4. 数据加载和预处理</h2><p><code>torchvision</code>实现了常用的图像数据加载功能，例如Imagenet、CIFAR10、MNIST等，以及常用的数据转换操作，这极大地方便了数据加载，并且代码具有可重用性。</p><h3 id="小试牛刀：CIFAR-10分类"><a href="#小试牛刀：CIFAR-10分类" class="headerlink" title="小试牛刀：CIFAR-10分类"></a>小试牛刀：CIFAR-10分类</h3><p>下面我们来尝试实现对CIFAR-10数据集的分类，步骤如下: </p><ol><li>使用torchvision加载并预处理CIFAR-10数据集</li><li>定义网络</li><li>定义损失函数和优化器</li><li>训练网络并更新网络参数</li><li>测试网络</li></ol><h4 id="CIFAR-10数据加载及预处理"><a href="#CIFAR-10数据加载及预处理" class="headerlink" title="CIFAR-10数据加载及预处理"></a>CIFAR-10数据加载及预处理</h4><p>CIFAR-10<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">^3</a>是一个常用的彩色图片数据集，它有10个类别: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。每张图片都是$3\times32\times32$，也即3-通道彩色图片，分辨率为$32\times32$。</p><h2 id="5-完整CIFAR-10分类练习"><a href="#5-完整CIFAR-10分类练习" class="headerlink" title="5. 完整CIFAR-10分类练习"></a>5. 完整CIFAR-10分类练习</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torchvision  <span class="token keyword">as</span> tv <span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">import</span> ToPILImageshow <span class="token operator">=</span> ToPILImage<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 可以把Tensor转成Image，方便可视化</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#归一化</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 训练集</span>trainset <span class="token operator">=</span> tv<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>    root<span class="token operator">=</span><span class="token string">'/home/xwjia/tmp/data/'</span><span class="token punctuation">,</span>    train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    transform<span class="token operator">=</span>transform<span class="token punctuation">)</span></code></pre><pre><code>0it [00:00, ?it/s]Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/xwjia/tmp/data/cifar-10-python.tar.gz 99%|█████████▉| 169443328/170498071 [00:20&lt;00:00, 11268338.34it/s]Extracting /home/xwjia/tmp/data/cifar-10-python.tar.gz to /home/xwjia/tmp/data/</code></pre><pre class=" language-python"><code class="language-python">trainloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>    trainset<span class="token punctuation">,</span>    batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 测试集</span>testset <span class="token operator">=</span> tv<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>    root<span class="token operator">=</span><span class="token string">'/home/xwjia/tmp/data/'</span><span class="token punctuation">,</span>    train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>testloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>    testset<span class="token punctuation">,</span>    batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>classes <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'plane'</span><span class="token punctuation">,</span> <span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'bird'</span><span class="token punctuation">,</span> <span class="token string">'cat'</span><span class="token punctuation">,</span>           <span class="token string">'deer'</span><span class="token punctuation">,</span> <span class="token string">'dog'</span><span class="token punctuation">,</span> <span class="token string">'frog'</span><span class="token punctuation">,</span> <span class="token string">'horse'</span><span class="token punctuation">,</span> <span class="token string">'ship'</span><span class="token punctuation">,</span> <span class="token string">'truck'</span><span class="token punctuation">)</span></code></pre><pre><code>Files already downloaded and verified</code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 查看某个样本</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token operator">=</span> trainset<span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>classes<span class="token punctuation">[</span>label<span class="token punctuation">]</span><span class="token punctuation">)</span>show<span class="token punctuation">(</span><span class="token punctuation">(</span>data<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre><code>ship</code></pre><p><img src="/2019/11/07/01-pytorch-ji-chu-shi-yong/output_29_1.png" alt="png"></p><p>Dataloader是一个可迭代的对象，它将dataset返回的每一条数据拼接成一个batch，并提供多线程加速优化和数据打乱等操作。当程序对dataset的所有数据遍历完一遍之后，相应的对Dataloader也完成了一次迭代。</p><pre class=" language-python"><code class="language-python">dataiter <span class="token operator">=</span> iter<span class="token punctuation">(</span>trainloader<span class="token punctuation">)</span>images<span class="token punctuation">,</span> labels <span class="token operator">=</span> dataiter<span class="token punctuation">.</span>next<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#返回下一个batch，4张图片和标签</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%11s'</span><span class="token operator">%</span>classes<span class="token punctuation">[</span>labels<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>show<span class="token punctuation">(</span>tv<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span><span class="token punctuation">(</span>images<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">400</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre><code>truck         cat       plane       truck</code></pre><p><img src="/2019/11/07/01-pytorch-ji-chu-shi-yong/output_31_1.png" alt="png"></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 自定义网络结构</span><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>         self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>          self<span class="token punctuation">.</span>fc1   <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">5</span><span class="token operator">*</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>          self<span class="token punctuation">.</span>fc2   <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc3   <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>         x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>         x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>         x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>         x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                <span class="token keyword">return</span> xnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span></code></pre><pre><code>Net(  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))</code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 定义损失函数和优化器</span><span class="token keyword">from</span> torch <span class="token keyword">import</span> optimcriterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 训练网络</span>torch<span class="token punctuation">.</span>set_num_threads<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    running_loss <span class="token operator">=</span> <span class="token number">0.0</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>trainloader<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 输入数据</span>        inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> data        <span class="token comment" spellcheck="true"># 梯度清零</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># forward + backward</span>        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 更新参数</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 打印log信息</span>        <span class="token comment" spellcheck="true"># loss是一个scalar， 需要使用loss.item()获取数值， 不能使用loss[0]</span>        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">2000</span> <span class="token operator">==</span> <span class="token number">1999</span><span class="token punctuation">:</span>   <span class="token comment" spellcheck="true"># 每2000个batch打印一下训练状态</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[%d, %5d] loss: %.3f'</span><span class="token operator">%</span> <span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> running_loss<span class="token operator">/</span><span class="token number">2000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            running_loss <span class="token operator">=</span> <span class="token number">0.0</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Finished Training'</span><span class="token punctuation">)</span></code></pre><pre><code>[1,  2000] loss: 2.163[1,  4000] loss: 1.823[1,  6000] loss: 1.656[1,  8000] loss: 1.586[1, 10000] loss: 1.495[1, 12000] loss: 1.467[2,  2000] loss: 1.405[2,  4000] loss: 1.376[2,  6000] loss: 1.343[2,  8000] loss: 1.322[2, 10000] loss: 1.320[2, 12000] loss: 1.311Finished Training</code></pre><h3 id="接下来看测试集，测试训练的结果"><a href="#接下来看测试集，测试训练的结果" class="headerlink" title="接下来看测试集，测试训练的结果"></a>接下来看测试集，测试训练的结果</h3><pre class=" language-python"><code class="language-python">dataiter <span class="token operator">=</span> iter<span class="token punctuation">(</span>testloader<span class="token punctuation">)</span>images<span class="token punctuation">,</span> labels <span class="token operator">=</span> dataiter<span class="token punctuation">.</span>next<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'实际的label: '</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%08s'</span><span class="token operator">%</span>classes<span class="token punctuation">[</span>labels<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>show<span class="token punctuation">(</span>tv<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>images<span class="token operator">/</span><span class="token number">2</span> <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">400</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 计算图片在每个类别上的分数</span>outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 得分最高的那个类</span>_<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'预测结果： '</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%5s'</span><span class="token operator">%</span>classes<span class="token punctuation">[</span>predicted<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre><code>预测结果：    cat  ship  ship plane</code></pre><pre class=" language-python"><code class="language-python">correct <span class="token operator">=</span> <span class="token number">0</span>   <span class="token comment" spellcheck="true"># 预测正确的图片数</span>total <span class="token operator">=</span> <span class="token number">0</span>     <span class="token comment" spellcheck="true"># 总共的图片数</span><span class="token comment" spellcheck="true"># 由于测试时不需要求导， 所以可以暂时关闭autograd， 提高速度， 节约内存</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> data <span class="token keyword">in</span> testloader<span class="token punctuation">:</span>        images<span class="token punctuation">,</span> labels <span class="token operator">=</span> data        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span>        _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        total <span class="token operator">+=</span> labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        correct <span class="token operator">+=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"10000张测试集中的准确率是： %d %%"</span> <span class="token operator">%</span> <span class="token punctuation">(</span><span class="token number">100</span><span class="token operator">*</span>correct<span class="token operator">/</span>total<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre><code>10000张测试集中的准确率是： 54 %</code></pre><h2 id="在GPU上训练"><a href="#在GPU上训练" class="headerlink" title="在GPU上训练"></a>在GPU上训练</h2><pre class=" language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>images <span class="token operator">=</span> images<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>labels <span class="token operator">=</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>output <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> labels<span class="token punctuation">)</span></code></pre><p>对PyTorch的基础介绍至此结束。总结一下，本节主要包含以下内容。</p><ol><li>Tensor: 类似Numpy数组的数据结构，与Numpy接口类似，可方便地互相转换。</li><li>autograd/: 为tensor提供自动求导功能。</li><li>nn: 专门为神经网络设计的接口，提供了很多有用的功能(神经网络层，损失函数，优化器等)。</li><li>神经网络训练: 以CIFAR-10分类为例演示了神经网络的训练流程，包括数据加载、网络搭建、训练及测试。</li></ol><p>从下一章开始，本书将深入系统地讲解PyTorch的各部分知识。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch框架入门与实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/10/20/hello-world/"/>
      <url>/2019/10/20/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
